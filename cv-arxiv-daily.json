[{"date": "2023.01.24", "papers": [{"id": "2301.09637", "url": "http://arxiv.org/abs/2301.09637v1", "pdf_url": "http://arxiv.org/pdf/2301.09637v1", "title": "InfiniCity: Infinite-Scale City Synthesis", "abs": "Toward infinite-scale 3D city synthesis, we propose a novel framework, InfiniCity, which constructs and renders an unconstrainedly large and 3D-grounded environment from random noises. InfiniCity decomposes the seemingly impractical task into three feasible modules, taking advantage of both 2D and 3D data. First, an infinite-pixel image synthesis module generates arbitrary-scale 2D maps from the bird's-eye view. Next, an octree-based voxel completion module lifts the generated 2D map to 3D octrees. Finally, a voxel-based neural rendering module texturizes the voxels and renders 2D images. InfiniCity can thus synthesize arbitrary-scale and traversable 3D city environments, and allow flexible and interactive editing from users. We quantitatively and qualitatively demonstrate the efficacy of the proposed framework. Project page: https://hubert0527.github.io/infinicity/", "authors": "Chieh Hubert Lin, Hsin-Ying Lee, Willi Menapace, Menglei Chai, Aliaksandr Siarohin, Ming-Hsuan Yang, Sergey Tulyakov", "first_author": "Sergey Tulyakov", "first_end_author": "Chieh Hubert Lin; Sergey Tulyakov", "publish_time": "2023-01-23 18:59:59+00:00", "update_time": "2023-01-23 18:59:59+00:00"}, {"id": "2211.13756", "url": "http://arxiv.org/abs/2211.13756v2", "pdf_url": "http://arxiv.org/pdf/2211.13756v2", "title": "Contrastive pretraining for semantic segmentation is robust to noisy   positive pairs", "abs": "Domain-specific variants of contrastive learning can construct positive pairs from two distinct in-domain images, while traditional methods just augment the same image twice. For example, we can form a positive pair from two satellite images showing the same location at different times. Ideally, this teaches the model to ignore changes caused by seasons, weather conditions or image acquisition artifacts. However, unlike in traditional contrastive methods, this can result in undesired positive pairs, since we form them without human supervision. For example, a positive pair might consist of one image before a disaster and one after. This could teach the model to ignore the differences between intact and damaged buildings, which might be what we want to detect in the downstream task. Similar to false negative pairs, this could impede model performance. Crucially, in this setting only parts of the images differ in relevant ways, while other parts remain similar. Surprisingly, we find that downstream semantic segmentation is either robust to such badly matched pairs or even benefits from them. The experiments are conducted on the remote sensing dataset xBD, and a synthetic segmentation dataset for which we have full control over the pairing conditions. As a result, practitioners can use these domain-specific contrastive methods without having to filter their positive pairs beforehand, or might even be encouraged to purposefully include such pairs in their pretraining dataset.", "authors": "Sebastian Gerard, Josephine Sullivan", "first_author": "Josephine Sullivan", "first_end_author": "Sebastian Gerard; Josephine Sullivan", "publish_time": "2022-11-24 18:59:01+00:00", "update_time": "2023-01-23 18:59:54+00:00"}, {"id": "2301.09632", "url": "http://arxiv.org/abs/2301.09632v1", "pdf_url": "http://arxiv.org/pdf/2301.09632v1", "title": "HexPlane: A Fast Representation for Dynamic Scenes", "abs": "Modeling and re-rendering dynamic 3D scenes is a challenging task in 3D vision. Prior approaches build on NeRF and rely on implicit representations. This is slow since it requires many MLP evaluations, constraining real-world applications. We show that dynamic 3D scenes can be explicitly represented by six planes of learned features, leading to an elegant solution we call HexPlane. A HexPlane computes features for points in spacetime by fusing vectors extracted from each plane, which is highly efficient. Pairing a HexPlane with a tiny MLP to regress output colors and training via volume rendering gives impressive results for novel view synthesis on dynamic scenes, matching the image quality of prior work but reducing training time by more than $100\\times$. Extensive ablations confirm our HexPlane design and show that it is robust to different feature fusion mechanisms, coordinate systems, and decoding mechanisms. HexPlanes are a simple and effective solution for representing 4D volumes, and we hope they can broadly contribute to modeling spacetime for dynamic 3D scenes.", "authors": "Ang Cao, Justin Johnson", "first_author": "Justin Johnson", "first_end_author": "Ang Cao; Justin Johnson", "publish_time": "2023-01-23 18:59:25+00:00", "update_time": "2023-01-23 18:59:25+00:00"}, {"id": "2301.09629", "url": "http://arxiv.org/abs/2301.09629v1", "pdf_url": "http://arxiv.org/pdf/2301.09629v1", "title": "LEGO-Net: Learning Regular Rearrangements of Objects in Rooms", "abs": "Humans universally dislike the task of cleaning up a messy room. If machines were to help us with this task, they must understand human criteria for regular arrangements, such as several types of symmetry, co-linearity or co-circularity, spacing uniformity in linear or circular patterns, and further inter-object relationships that relate to style and functionality. Previous approaches for this task relied on human input to explicitly specify goal state, or synthesized scenes from scratch -- but such methods do not address the rearrangement of existing messy scenes without providing a goal state. In this paper, we present LEGO-Net, a data-driven transformer-based iterative method for learning regular rearrangement of objects in messy rooms. LEGO-Net is partly inspired by diffusion models -- it starts with an initial messy state and iteratively \"de-noises'' the position and orientation of objects to a regular state while reducing the distance traveled. Given randomly perturbed object positions and orientations in an existing dataset of professionally-arranged scenes, our method is trained to recover a regular re-arrangement. Results demonstrate that our method is able to reliably rearrange room scenes and outperform other methods. We additionally propose a metric for evaluating regularity in room arrangements using number-theoretic machinery.", "authors": "Qiuhong Anna Wei, Sijie Ding, Jeong Joon Park, Rahul Sajnani, Adrien Poulenard, Srinath Sridhar, Leonidas Guibas", "first_author": "Leonidas Guibas", "first_end_author": "Qiuhong Anna Wei; Leonidas Guibas", "publish_time": "2023-01-23 18:58:02+00:00", "update_time": "2023-01-23 18:58:02+00:00"}, {"id": "2206.08312", "url": "http://arxiv.org/abs/2206.08312v2", "pdf_url": "http://arxiv.org/pdf/2206.08312v2", "title": "SoundSpaces 2.0: A Simulation Platform for Visual-Acoustic Learning", "abs": "We introduce SoundSpaces 2.0, a platform for on-the-fly geometry-based audio rendering for 3D environments. Given a 3D mesh of a real-world environment, SoundSpaces can generate highly realistic acoustics for arbitrary sounds captured from arbitrary microphone locations. Together with existing 3D visual assets, it supports an array of audio-visual research tasks, such as audio-visual navigation, mapping, source localization and separation, and acoustic matching. Compared to existing resources, SoundSpaces 2.0 has the advantages of allowing continuous spatial sampling, generalization to novel environments, and configurable microphone and material properties. To our knowledge, this is the first geometry-based acoustic simulation that offers high fidelity and realism while also being fast enough to use for embodied learning. We showcase the simulator's properties and benchmark its performance against real-world audio measurements. In addition, we demonstrate two downstream tasks -- embodied navigation and far-field automatic speech recognition -- and highlight sim2real performance for the latter. SoundSpaces 2.0 is publicly available to facilitate wider research for perceptual systems that can both see and hear.", "authors": "Changan Chen, Carl Schissler, Sanchit Garg, Philip Kobernik, Alexander Clegg, Paul Calamia, Dhruv Batra, Philip W Robinson, Kristen Grauman", "first_author": "Kristen Grauman", "first_end_author": "Changan Chen; Kristen Grauman", "publish_time": "2022-06-16 17:17:44+00:00", "update_time": "2023-01-23 18:49:47+00:00"}, {"id": "2301.09624", "url": "http://arxiv.org/abs/2301.09624v1", "pdf_url": "http://arxiv.org/pdf/2301.09624v1", "title": "Maximum Mean Discrepancy Kernels for Predictive and Prognostic Modeling   of Whole Slide Images", "abs": "How similar are two images? In computational pathology, where Whole Slide Images (WSIs) of digitally scanned tissue samples from patients can be multi-gigapixels in size, determination of degree of similarity between two WSIs is a challenging task with a number of practical applications. In this work, we explore a novel strategy based on kernelized Maximum Mean Discrepancy (MMD) analysis for determination of pairwise similarity between WSIs. The proposed approach works by calculating MMD between two WSIs using kernels over deep features of image patches. This allows representation of an entire dataset of WSIs as a kernel matrix for WSI level clustering, weakly-supervised prediction of TP-53 mutation status in breast cancer patients from their routine WSIs as well as survival analysis with state of the art prediction performance. We believe that this work will open up further avenues for application of WSI-level kernels for predictive and prognostic tasks in computational pathology.", "authors": "Piotr Keller, Muhammad Dawood, Fayyaz ul Amir Afsar Minhas", "first_author": "Fayyaz ul Amir Afsar Minhas", "first_end_author": "Piotr Keller; Fayyaz ul Amir Afsar Minhas", "publish_time": "2023-01-23 18:47:41+00:00", "update_time": "2023-01-23 18:47:41+00:00"}, {"id": "2301.09620", "url": "http://arxiv.org/abs/2301.09620v1", "pdf_url": "http://arxiv.org/pdf/2301.09620v1", "title": "Tracking the industrial growth of modern China with high-resolution   panchromatic imagery: A sequential convolutional approach", "abs": "Due to insufficient or difficult to obtain data on development in inaccessible regions, remote sensing data is an important tool for interested stakeholders to collect information on economic growth. To date, no studies have utilized deep learning to estimate industrial growth at the level of individual sites. In this study, we harness high-resolution panchromatic imagery to estimate development over time at 419 industrial sites in the People's Republic of China using a multi-tier computer vision framework. We present two methods for approximating development: (1) structural area coverage estimated through a Mask R-CNN segmentation algorithm, and (2) imputing development directly with visible & infrared radiance from the Visible Infrared Imaging Radiometer Suite (VIIRS). Labels generated from these methods are comparatively evaluated and tested. On a dataset of 2,078 50 cm resolution images spanning 19 years, the results indicate that two dimensions of industrial development can be estimated using high-resolution daytime imagery, including (a) the total square meters of industrial development (average error of 0.021 $\\textrm{km}^2$), and (b) the radiance of lights (average error of 9.8 $\\mathrm{\\frac{nW}{cm^{2}sr}}$). Trend analysis of the techniques reveal estimates from a Mask R-CNN-labeled CNN-LSTM track ground truth measurements most closely. The Mask R-CNN estimates positive growth at every site from the oldest image to the most recent, with an average change of 4,084 $\\textrm{m}^2$.", "authors": "Ethan Brewer, Zhonghui Lv, Dan Runfola", "first_author": "Dan Runfola", "first_end_author": "Ethan Brewer; Dan Runfola", "publish_time": "2023-01-23 18:40:21+00:00", "update_time": "2023-01-23 18:40:21+00:00"}, {"id": "2301.09617", "url": "http://arxiv.org/abs/2301.09617v1", "pdf_url": "http://arxiv.org/pdf/2301.09617v1", "title": "Fully transformer-based biomarker prediction from colorectal cancer   histology: a large-scale multicentric study", "abs": "Background: Deep learning (DL) can extract predictive and prognostic biomarkers from routine pathology slides in colorectal cancer. For example, a DL test for the diagnosis of microsatellite instability (MSI) in CRC has been approved in 2022. Current approaches rely on convolutional neural networks (CNNs). Transformer networks are outperforming CNNs and are replacing them in many applications, but have not been used for biomarker prediction in cancer at a large scale. In addition, most DL approaches have been trained on small patient cohorts, which limits their clinical utility. Methods: In this study, we developed a new fully transformer-based pipeline for end-to-end biomarker prediction from pathology slides. We combine a pre-trained transformer encoder and a transformer network for patch aggregation, capable of yielding single and multi-target prediction at patient level. We train our pipeline on over 9,000 patients from 10 colorectal cancer cohorts. Results: A fully transformer-based approach massively improves the performance, generalizability, data efficiency, and interpretability as compared with current state-of-the-art algorithms. After training on a large multicenter cohort, we achieve a sensitivity of 0.97 with a negative predictive value of 0.99 for MSI prediction on surgical resection specimens. We demonstrate for the first time that resection specimen-only training reaches clinical-grade performance on endoscopic biopsy tissue, solving a long-standing diagnostic problem. Interpretation: A fully transformer-based end-to-end pipeline trained on thousands of pathology slides yields clinical-grade performance for biomarker prediction on surgical resections and biopsies. Our new methods are freely available under an open source license.", "authors": "Sophia J. Wagner, Daniel Reisenb\u00fcchler, Nicholas P. West, Jan Moritz Niehues, Gregory Patrick Veldhuizen, Philip Quirke, Heike I. Grabsch, Piet A. van den Brandt, Gordon G. A. Hutchins, Susan D. Richman, Tanwei Yuan, Rupert Langer, Josien Christina Anna Jenniskens, Kelly Offermans, Wolfram Mueller, Richard Gray, Stephen B. Gruber, Joel K. Greenson, Gad Rennert, Joseph D. Bonner, Daniel Schmolze, Jacqueline A. James, Maurice B. Loughrey, Manuel Salto-Tellez, Hermann Brenner, Michael Hoffmeister, Daniel Truhn, Julia A. Schnabel, Melanie Boxberg, Tingying Peng, Jakob Nikolas Kather", "first_author": "Jakob Nikolas Kather", "first_end_author": "Sophia J. Wagner; Jakob Nikolas Kather", "publish_time": "2023-01-23 18:33:38+00:00", "update_time": "2023-01-23 18:33:38+00:00"}, {"id": "2301.09602", "url": "http://arxiv.org/abs/2301.09602v1", "pdf_url": "http://arxiv.org/pdf/2301.09602v1", "title": "Adapting the Hypersphere Loss Function from Anomaly Detection to Anomaly   Segmentation", "abs": "We propose an incremental improvement to Fully Convolutional Data Description (FCDD), an adaptation of the one-class classification approach from anomaly detection to image anomaly segmentation (a.k.a. anomaly localization). We analyze its original loss function and propose a substitute that better resembles its predecessor, the Hypersphere Classifier (HSC). Both are compared on the MVTec Anomaly Detection Dataset (MVTec-AD) -- training images are flawless objects/textures and the goal is to segment unseen defects -- showing that consistent improvement is achieved by better designing the pixel-wise supervision.", "authors": "Joao P. C. Bertoldo, Santiago Velasco-Forero, Jesus Angulo, Etienne Decenci\u00e8re", "first_author": "Etienne Decenci\u00e8re", "first_end_author": "Joao P. C. Bertoldo; Etienne Decenci\u00e8re", "publish_time": "2023-01-23 18:06:35+00:00", "update_time": "2023-01-23 18:06:35+00:00"}, {"id": "2301.09595", "url": "http://arxiv.org/abs/2301.09595v1", "pdf_url": "http://arxiv.org/pdf/2301.09595v1", "title": "Zorro: the masked multimodal transformer", "abs": "Attention-based models are appealing for multimodal processing because inputs from multiple modalities can be concatenated and fed to a single backbone network - thus requiring very little fusion engineering. The resulting representations are however fully entangled throughout the network, which may not always be desirable: in learning, contrastive audio-visual self-supervised learning requires independent audio and visual features to operate, otherwise learning collapses; in inference, evaluation of audio-visual models should be possible on benchmarks having just audio or just video. In this paper, we introduce Zorro, a technique that uses masks to control how inputs from each modality are routed inside Transformers, keeping some parts of the representation modality-pure. We apply this technique to three popular transformer-based architectures (ViT, Swin and HiP) and show that with contrastive pre-training Zorro achieves state-of-the-art results on most relevant benchmarks for multimodal tasks (AudioSet and VGGSound). Furthermore, the resulting models are able to perform unimodal inference on both video and audio benchmarks such as Kinetics-400 or ESC-50.", "authors": "Adri\u00e0 Recasens, Jason Lin, Jo\u0101o Carreira, Drew Jaegle, Luyu Wang, Jean-baptiste Alayrac, Pauline Luc, Antoine Miech, Lucas Smaira, Ross Hemsley, Andrew Zisserman", "first_author": "Andrew Zisserman", "first_end_author": "Adri\u00e0 Recasens; Andrew Zisserman", "publish_time": "2023-01-23 17:51:39+00:00", "update_time": "2023-01-23 17:51:39+00:00"}, {"id": "2301.08730", "url": "http://arxiv.org/abs/2301.08730v2", "pdf_url": "http://arxiv.org/pdf/2301.08730v2", "title": "Novel-View Acoustic Synthesis", "abs": "We introduce the novel-view acoustic synthesis (NVAS) task: given the sight and sound observed at a source viewpoint, can we synthesize the sound of that scene from an unseen target viewpoint? We propose a neural rendering approach: Visually-Guided Acoustic Synthesis (ViGAS) network that learns to synthesize the sound of an arbitrary point in space by analyzing the input audio-visual cues. To benchmark this task, we collect two first-of-their-kind large-scale multi-view audio-visual datasets, one synthetic and one real. We show that our model successfully reasons about the spatial cues and synthesizes faithful audio on both datasets. To our knowledge, this work represents the very first formulation, dataset, and approach to solve the novel-view acoustic synthesis task, which has exciting potential applications ranging from AR/VR to art and design. Unlocked by this work, we believe that the future of novel-view synthesis is in multi-modal learning from videos.", "authors": "Changan Chen, Alexander Richard, Roman Shapovalov, Vamsi Krishna Ithapu, Natalia Neverova, Kristen Grauman, Andrea Vedaldi", "first_author": "Andrea Vedaldi", "first_end_author": "Changan Chen; Andrea Vedaldi", "publish_time": "2023-01-20 18:49:58+00:00", "update_time": "2023-01-23 17:11:30+00:00"}, {"id": "2301.09544", "url": "http://arxiv.org/abs/2301.09544v1", "pdf_url": "http://arxiv.org/pdf/2301.09544v1", "title": "Learning to View: Decision Transformers for Active Object Detection", "abs": "Active perception describes a broad class of techniques that couple planning and perception systems to move the robot in a way to give the robot more information about the environment. In most robotic systems, perception is typically independent of motion planning. For example, traditional object detection is passive: it operates only on the images it receives. However, we have a chance to improve the results if we allow planning to consume detection signals and move the robot to collect views that maximize the quality of the results. In this paper, we use reinforcement learning (RL) methods to control the robot in order to obtain images that maximize the detection quality. Specifically, we propose using a Decision Transformer with online fine-tuning, which first optimizes the policy with a pre-collected expert dataset and then improves the learned policy by exploring better solutions in the environment. We evaluate the performance of proposed method on an interactive dataset collected from an indoor scenario simulator. Experimental results demonstrate that our method outperforms all baselines, including expert policy and pure offline RL methods. We also provide exhaustive analyses of the reward distribution and observation space.", "authors": "Wenhao Ding, Nathalie Majcherczyk, Mohit Deshpande, Xuewei Qi, Ding Zhao, Rajasimman Madhivanan, Arnie Sen", "first_author": "Arnie Sen", "first_end_author": "Wenhao Ding; Arnie Sen", "publish_time": "2023-01-23 17:00:48+00:00", "update_time": "2023-01-23 17:00:48+00:00"}, {"id": "2301.09542", "url": "http://arxiv.org/abs/2301.09542v1", "pdf_url": "http://arxiv.org/pdf/2301.09542v1", "title": "Improving Presentation Attack Detection for ID Cards on Remote   Verification Systems", "abs": "In this paper, an updated two-stage, end-to-end Presentation Attack Detection method for remote biometric verification systems of ID cards, based on MobileNetV2, is presented. Several presentation attack species such as printed, display, composite (based on cropped and spliced areas), plastic (PVC), and synthetic ID card images using different capture sources are used. This proposal was developed using a database consisting of 190.000 real case Chilean ID card images with the support of a third-party company. Also, a new framework called PyPAD, used to estimate multi-class metrics compliant with the ISO/IEC 30107-3 standard was developed, and will be made available for research purposes. Our method is trained on two convolutional neural networks separately, reaching BPCER\\textsubscript{100} scores on ID cards attacks of 1.69\\% and 2.36\\% respectively. The two-stage method using both models together can reach a BPCER\\textsubscript{100} score of 0.92\\%.", "authors": "Sebastian Gonzalez, Juan Tapia", "first_author": "Juan Tapia", "first_end_author": "Sebastian Gonzalez; Juan Tapia", "publish_time": "2023-01-23 16:59:26+00:00", "update_time": "2023-01-23 16:59:26+00:00"}, {"id": "2301.09525", "url": "http://arxiv.org/abs/2301.09525v1", "pdf_url": "http://arxiv.org/pdf/2301.09525v1", "title": "DeepFEL: Deep Fastfood Ensemble Learning for Histopathology Image   Analysis", "abs": "Computational pathology tasks have some unique characterises such as multi-gigapixel images, tedious and frequently uncertain annotations, and unavailability of large number of cases [13]. To address some of these issues, we present Deep Fastfood Ensembles - a simple, fast and yet effective method for combining deep features pooled from popular CNN models pre-trained on totally different source domains (e.g., natural image objects) and projected onto diverse dimensions using random projections, the so-called Fastfood [11]. The final ensemble output is obtained by a consensus of simple individual classifiers, each of which is trained on a different collection of random basis vectors. This offers extremely fast and yet effective solution, especially when training times and domain labels are of the essence. We demonstrate the effectiveness of the proposed deep fastfood ensemble learning as compared to the state-of-the-art methods for three different tasks in histopathology image analysis.", "authors": "Nima Hatami", "first_author": "Nima Hatami", "first_end_author": "Nima Hatami; Nima Hatami", "publish_time": "2023-01-23 16:16:24+00:00", "update_time": "2023-01-23 16:16:24+00:00"}, {"id": "2301.09522", "url": "http://arxiv.org/abs/2301.09522v1", "pdf_url": "http://arxiv.org/pdf/2301.09522v1", "title": "Optimising Event-Driven Spiking Neural Network with Regularisation and   Cutoff", "abs": "Spiking neural networks (SNNs), a variant of artificial neural networks (ANNs) with the benefit of energy efficiency, have achieved the accuracy close to its ANN counterparts, on benchmark datasets such as CIFAR10/100 and ImageNet. However, comparing with frame-based input (e.g., images), event-based inputs from e.g., Dynamic Vision Sensor (DVS) can make a better use of SNNs thanks to the SNNs' asynchronous working mechanism. In this paper, we strengthen the marriage between SNNs and event-based inputs with a proposal to consider anytime optimal inference SNNs, or AOI-SNNs, which can terminate anytime during the inference to achieve optimal inference result. Two novel optimisation techniques are presented to achieve AOI-SNNs: a regularisation and a cutoff. The regularisation enables the training and construction of SNNs with optimised performance, and the cutoff technique optimises the inference of SNNs on event-driven inputs. We conduct an extensive set of experiments on multiple benchmark event-based datasets, including CIFAR10-DVS, N-Caltech101 and DVS128 Gesture. The experimental results demonstrate that our techniques are superior to the state-of-the-art with respect to the accuracy and latency.", "authors": "Dengyu Wu, Gaojie Jin, Han Yu, Xinping Yi, Xiaowei Huang", "first_author": "Xiaowei Huang", "first_end_author": "Dengyu Wu; Xiaowei Huang", "publish_time": "2023-01-23 16:14:09+00:00", "update_time": "2023-01-23 16:14:09+00:00"}, {"id": "2301.09515", "url": "http://arxiv.org/abs/2301.09515v1", "pdf_url": "http://arxiv.org/pdf/2301.09515v1", "title": "StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale   Text-to-Image Synthesis", "abs": "Text-to-image synthesis has recently seen significant progress thanks to large pretrained language models, large-scale training data, and the introduction of scalable model families such as diffusion and autoregressive models. However, the best-performing models require iterative evaluation to generate a single sample. In contrast, generative adversarial networks (GANs) only need a single forward pass. They are thus much faster, but they currently remain far behind the state-of-the-art in large-scale text-to-image synthesis. This paper aims to identify the necessary steps to regain competitiveness. Our proposed model, StyleGAN-T, addresses the specific requirements of large-scale text-to-image synthesis, such as large capacity, stable training on diverse datasets, strong text alignment, and controllable variation vs. text alignment tradeoff. StyleGAN-T significantly improves over previous GANs and outperforms distilled diffusion models - the previous state-of-the-art in fast text-to-image synthesis - in terms of sample quality and speed.", "authors": "Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, Timo Aila", "first_author": "Timo Aila", "first_end_author": "Axel Sauer; Timo Aila", "publish_time": "2023-01-23 16:05:45+00:00", "update_time": "2023-01-23 16:05:45+00:00"}, {"id": "2202.06924", "url": "http://arxiv.org/abs/2202.06924v2", "pdf_url": "http://arxiv.org/pdf/2202.06924v2", "title": "Do Gradient Inversion Attacks Make Federated Learning Unsafe?", "abs": "Federated learning (FL) allows the collaborative training of AI models without needing to share raw data. This capability makes it especially interesting for healthcare applications where patient and data privacy is of utmost concern. However, recent works on the inversion of deep neural networks from model gradients raised concerns about the security of FL in preventing the leakage of training data. In this work, we show that these attacks presented in the literature are impractical in FL use-cases where the clients' training involves updating the Batch Normalization (BN) statistics and provide a new baseline attack that works for such scenarios. Furthermore, we present new ways to measure and visualize potential data leakage in FL. Our work is a step towards establishing reproducible methods of measuring data leakage in FL and could help determine the optimal tradeoffs between privacy-preserving techniques, such as differential privacy, and model accuracy based on quantifiable metrics.   Code is available at https://nvidia.github.io/NVFlare/research/quantifying-data-leakage.", "authors": "Ali Hatamizadeh, Hongxu Yin, Pavlo Molchanov, Andriy Myronenko, Wenqi Li, Prerna Dogra, Andrew Feng, Mona G. Flores, Jan Kautz, Daguang Xu, Holger R. Roth", "first_author": "Holger R. Roth", "first_end_author": "Ali Hatamizadeh; Holger R. Roth", "publish_time": "2022-02-14 18:33:12+00:00", "update_time": "2023-01-23 16:03:10+00:00"}, {"id": "2301.09506", "url": "http://arxiv.org/abs/2301.09506v1", "pdf_url": "http://arxiv.org/pdf/2301.09506v1", "title": "OvarNet: Towards Open-vocabulary Object Attribute Recognition", "abs": "In this paper, we consider the problem of simultaneously detecting objects and inferring their visual attributes in an image, even for those with no manual annotations provided at the training stage, resembling an open-vocabulary scenario. To achieve this goal, we make the following contributions: (i) we start with a naive two-stage approach for open-vocabulary object detection and attribute classification, termed CLIP-Attr. The candidate objects are first proposed with an offline RPN and later classified for semantic category and attributes; (ii) we combine all available datasets and train with a federated strategy to finetune the CLIP model, aligning the visual representation with attributes, additionally, we investigate the efficacy of leveraging freely available online image-caption pairs under weakly supervised learning; (iii) in pursuit of efficiency, we train a Faster-RCNN type model end-to-end with knowledge distillation, that performs class-agnostic object proposals and classification on semantic categories and attributes with classifiers generated from a text encoder; Finally, (iv) we conduct extensive experiments on VAW, MS-COCO, LSA, and OVAD datasets, and show that recognition of semantic category and attributes is complementary for visual scene understanding, i.e., jointly training object detection and attributes prediction largely outperform existing approaches that treat the two tasks independently, demonstrating strong generalization ability to novel attributes and categories.", "authors": "Keyan Chen, Xiaolong Jiang, Yao Hu, Xu Tang, Yan Gao, Jianqi Chen, Weidi Xie", "first_author": "Weidi Xie", "first_end_author": "Keyan Chen; Weidi Xie", "publish_time": "2023-01-23 15:59:29+00:00", "update_time": "2023-01-23 15:59:29+00:00"}, {"id": "2209.04699", "url": "http://arxiv.org/abs/2209.04699v2", "pdf_url": "http://arxiv.org/pdf/2209.04699v2", "title": "Explainable Image Quality Assessments in Teledermatological Photography", "abs": "Image quality is a crucial factor in the effectiveness and efficiency of teledermatological consultations. However, up to 50% of images sent by patients have quality issues, thus increasing the time to diagnosis and treatment. An automated, easily deployable, explainable method for assessing image quality is necessary to improve the current teledermatological consultation flow. We introduce ImageQX, a convolutional neural network for image quality assessment with a learning mechanism for identifying the most common poor image quality explanations: bad framing, bad lighting, blur, low resolution, and distance issues. ImageQX was trained on 26,635 photographs and validated on 9,874 photographs, each annotated with image quality labels and poor image quality explanations by up to 12 board-certified dermatologists. The photographic images were taken between 2017 and 2019 using a mobile skin disease tracking application accessible worldwide. Our method achieves expert-level performance for both image quality assessment and poor image quality explanation. For image quality assessment, ImageQX obtains a macro F1-score of 0.73 +- 0.01, which places it within standard deviation of the pairwise inter-rater F1-score of 0.77 +- 0.07. For poor image quality explanations, our method obtains F1-scores of between 0.37 +- 0.01 and 0.70 +- 0.01, similar to the inter-rater pairwise F1-score of between 0.24 +- 0.15 and 0.83 +- 0.06. Moreover, with a size of only 15 MB, ImageQX is easily deployable on mobile devices. With an image quality detection performance similar to that of dermatologists, incorporating ImageQX into the teledermatology flow can enable a better, faster flow for remote consultations.", "authors": "Raluca Jalaboi, Ole Winther, Alfiia Galimzianova", "first_author": "Alfiia Galimzianova", "first_end_author": "Raluca Jalaboi; Alfiia Galimzianova", "publish_time": "2022-09-10 15:48:28+00:00", "update_time": "2023-01-23 15:59:20+00:00"}, {"id": "2301.09498", "url": "http://arxiv.org/abs/2301.09498v1", "pdf_url": "http://arxiv.org/pdf/2301.09498v1", "title": "Triplet Contrastive Learning for Unsupervised Vehicle Re-identification", "abs": "Part feature learning is a critical technology for finegrained semantic understanding in vehicle re-identification. However, recent unsupervised re-identification works exhibit serious gradient collapse issues when directly modeling the part features and global features. To address this problem, in this paper, we propose a novel Triplet Contrastive Learning framework (TCL) which leverages cluster features to bridge the part features and global features. Specifically, TCL devises three memory banks to store the features according to their attributes and proposes a proxy contrastive loss (PCL) to make contrastive learning between adjacent memory banks, thus presenting the associations between the part and global features as a transition of the partcluster and cluster-global associations. Since the cluster memory bank deals with all the instance features, it can summarize them into a discriminative feature representation. To deeply exploit the instance information, TCL proposes two additional loss functions. For the inter-class instance, a hybrid contrastive loss (HCL) re-defines the sample correlations by approaching the positive cluster features and leaving the all negative instance features. For the intra-class instances, a weighted regularization cluster contrastive loss (WRCCL) refines the pseudo labels by penalizing the mislabeled images according to the instance similarity. Extensive experiments show that TCL outperforms many state-of-the-art unsupervised vehicle re-identification approaches. The code will be available at https://github.com/muzishen/TCL.", "authors": "Fei Shen, Xiaoyu Du, Liyan Zhang, Jinhui Tang", "first_author": "Jinhui Tang", "first_end_author": "Fei Shen; Jinhui Tang", "publish_time": "2023-01-23 15:52:12+00:00", "update_time": "2023-01-23 15:52:12+00:00"}, {"id": "2210.07903", "url": "http://arxiv.org/abs/2210.07903v2", "pdf_url": "http://arxiv.org/pdf/2210.07903v2", "title": "Text Detection Forgot About Document OCR", "abs": "Detection and recognition of text from scans and other images, commonly denoted as Optical Character Recognition (OCR), is a widely used form of automated document processing with a number of methods available. Yet OCR systems still do not achieve 100% accuracy, requiring human corrections in applications where correct readout is essential. Advances in machine learning enabled even more challenging scenarios of text detection and recognition \"in-the-wild\" - such as detecting text on objects from photographs of complex scenes. While the state-of-the-art methods for in-the-wild text recognition are typically evaluated on complex scenes, their performance in the domain of documents is typically not published, and a comprehensive comparison with methods for document OCR is missing. This paper compares several methods designed for in-the-wild text recognition and for document text recognition, and provides their evaluation on the domain of structured documents. The results suggest that state-of-the-art methods originally proposed for in-the-wild text detection also achieve competitive results on document text detection, outperforming available OCR methods. We argue that the application of document OCR should not be omitted in evaluation of text detection and recognition methods.", "authors": "Krzysztof Olejniczak, Milan \u0160ulc", "first_author": "Milan \u0160ulc", "first_end_author": "Krzysztof Olejniczak; Milan \u0160ulc", "publish_time": "2022-10-14 15:37:54+00:00", "update_time": "2023-01-23 15:34:33+00:00"}, {"id": "2301.09489", "url": "http://arxiv.org/abs/2301.09489v1", "pdf_url": "http://arxiv.org/pdf/2301.09489v1", "title": "Contracting Skeletal Kinematic Embeddings for Anomaly Detection", "abs": "Detecting the anomaly of human behavior is paramount to timely recognizing endangering situations, such as street fights or elderly falls. However, anomaly detection is complex, since anomalous events are rare and because it is an open set recognition task, i.e., what is anomalous at inference has not been observed at training. We propose COSKAD, a novel model which encodes skeletal human motion by an efficient graph convolutional network and learns to COntract SKeletal kinematic embeddings onto a latent hypersphere of minimum volume for Anomaly Detection. We propose and analyze three latent space designs for COSKAD: the commonly-adopted Euclidean, and the new spherical-radial and hyperbolic volumes. All three variants outperform the state-of-the-art, including video-based techniques, on the ShangaiTechCampus, the Avenue, and on the most recent UBnormal dataset, for which we contribute novel skeleton annotations and the selection of human-related videos. The source code and dataset will be released upon acceptance.", "authors": "Alessandro Flaborea, Guido Maria D'Amely di Melendugno, Stefano D'arrigo, Marco Aurelio Sterpa, Alessio Sampieri, Fabio Galasso", "first_author": "Fabio Galasso", "first_end_author": "Alessandro Flaborea; Fabio Galasso", "publish_time": "2023-01-23 15:32:27+00:00", "update_time": "2023-01-23 15:32:27+00:00"}, {"id": "2202.06198", "url": "http://arxiv.org/abs/2202.06198v2", "pdf_url": "http://arxiv.org/pdf/2202.06198v2", "title": "Data standardization for robust lip sync", "abs": "Lip sync is a fundamental audio-visual task. However, existing lip sync methods fall short of being robust to the incredible diversity of videos taken in the wild, and the majority of the diversity is caused by compound distracting factors that could degrade existing lip sync methods. To address these issues, this paper proposes a data standardization pipeline that can produce standardized expressive images while preserving lip motion information from the input and reducing the effects of compound distracting factors. Based on recent advances in 3D face reconstruction, we first create a model that can consistently disentangle expressions, with lip motion information embedded. Then, to reduce the effects of compound distracting factors on synthesized images, we synthesize images with only expressions from the input, intentionally setting all other attributes at predefined values independent of the input. Using synthesized images, existing lip sync methods improve their data efficiency and robustness, and they achieve competitive performance for the active speaker detection task.", "authors": "Chun Wang", "first_author": "Chun Wang", "first_end_author": "Chun Wang; Chun Wang", "publish_time": "2022-02-13 04:09:21+00:00", "update_time": "2023-01-23 14:50:26+00:00"}, {"id": "2301.09461", "url": "http://arxiv.org/abs/2301.09461v1", "pdf_url": "http://arxiv.org/pdf/2301.09461v1", "title": "Study on the identification limits of craniofacial superimposition", "abs": "Craniofacial Superimposition involves the superimposition of an image of a skull with a number of ante-mortem face images of an individual and the analysis of their morphological correspondence. Despite being used for one century, it is not yet a mature and fully accepted technique due to the absence of solid scientific approaches, significant reliability studies, and international standards. In this paper we present a comprehensive experimentation on the limitations of Craniofacial Superimposition as a forensic identification technique. The study involves different experiments over more than 1 Million comparisons performed by a landmark-based automatic 3D/2D superimposition method. The total sample analyzed consists of 320 subjects and 29 craniofacial landmarks.", "authors": "\u00d3scar Ib\u00e1\u00f1ez, Enrique Bermejo, Andrea Valsecchi", "first_author": "Andrea Valsecchi", "first_end_author": "\u00d3scar Ib\u00e1\u00f1ez; Andrea Valsecchi", "publish_time": "2023-01-23 14:46:43+00:00", "update_time": "2023-01-23 14:46:43+00:00"}, {"id": "2301.09460", "url": "http://arxiv.org/abs/2301.09460v1", "pdf_url": "http://arxiv.org/pdf/2301.09460v1", "title": "HRVQA: A Visual Question Answering Benchmark for High-Resolution Aerial   Images", "abs": "Visual question answering (VQA) is an important and challenging multimodal task in computer vision. Recently, a few efforts have been made to bring VQA task to aerial images, due to its potential real-world applications in disaster monitoring, urban planning, and digital earth product generation. However, not only the huge variation in the appearance, scale and orientation of the concepts in aerial images, but also the scarcity of the well-annotated datasets restricts the development of VQA in this domain. In this paper, we introduce a new dataset, HRVQA, which provides collected 53512 aerial images of 1024*1024 pixels and semi-automatically generated 1070240 QA pairs. To benchmark the understanding capability of VQA models for aerial images, we evaluate the relevant methods on HRVQA. Moreover, we propose a novel model, GFTransformer, with gated attention modules and a mutual fusion module. The experiments show that the proposed dataset is quite challenging, especially the specific attribute related questions. Our method achieves superior performance in comparison to the previous state-of-the-art approaches. The dataset and the source code will be released at https://hrvqa.nl/.", "authors": "Kun Li, George Vosselman, Michael Ying Yang", "first_author": "Michael Ying Yang", "first_end_author": "Kun Li; Michael Ying Yang", "publish_time": "2023-01-23 14:36:38+00:00", "update_time": "2023-01-23 14:36:38+00:00"}, {"id": "2205.15769", "url": "http://arxiv.org/abs/2205.15769v2", "pdf_url": "http://arxiv.org/pdf/2205.15769v2", "title": "Concept-level Debugging of Part-Prototype Networks", "abs": "Part-prototype Networks (ProtoPNets) are concept-based classifiers designed to achieve the same performance as black-box models without compromising transparency. ProtoPNets compute predictions based on similarity to class-specific part-prototypes learned to recognize parts of training examples, making it easy to faithfully determine what examples are responsible for any target prediction and why. However, like other models, they are prone to picking up confounders and shortcuts from the data, thus suffering from compromised prediction accuracy and limited generalization. We propose ProtoPDebug, an effective concept-level debugger for ProtoPNets in which a human supervisor, guided by the model's explanations, supplies feedback in the form of what part-prototypes must be forgotten or kept, and the model is fine-tuned to align with this supervision. Our experimental evaluation shows that ProtoPDebug outperforms state-of-the-art debuggers for a fraction of the annotation cost. An online experiment with laypeople confirms the simplicity of the feedback requested to the users and the effectiveness of the collected feedback for learning confounder-free part-prototypes. ProtoPDebug is a promising tool for trustworthy interactive learning in critical applications, as suggested by a preliminary evaluation on a medical decision making task.", "authors": "Andrea Bontempelli, Stefano Teso, Katya Tentori, Fausto Giunchiglia, Andrea Passerini", "first_author": "Andrea Passerini", "first_end_author": "Andrea Bontempelli; Andrea Passerini", "publish_time": "2022-05-31 13:18:51+00:00", "update_time": "2023-01-23 14:35:33+00:00"}, {"id": "2301.07502", "url": "http://arxiv.org/abs/2301.07502v2", "pdf_url": "http://arxiv.org/pdf/2301.07502v2", "title": "Multimodal Side-Tuning for Document Classification", "abs": "In this paper, we propose to exploit the side-tuning framework for multimodal document classification. Side-tuning is a methodology for network adaptation recently introduced to solve some of the problems related to previous approaches. Thanks to this technique it is actually possible to overcome model rigidity and catastrophic forgetting of transfer learning by fine-tuning. The proposed solution uses off-the-shelf deep learning architectures leveraging the side-tuning framework to combine a base model with a tandem of two side networks. We show that side-tuning can be successfully employed also when different data sources are considered, e.g. text and images in document classification. The experimental results show that this approach pushes further the limit for document classification accuracy with respect to the state of the art.", "authors": "Stefano Pio Zingaro, Giuseppe Lisanti, Maurizio Gabbrielli", "first_author": "Maurizio Gabbrielli", "first_end_author": "Stefano Pio Zingaro; Maurizio Gabbrielli", "publish_time": "2023-01-16 11:08:03+00:00", "update_time": "2023-01-23 14:28:15+00:00"}, {"id": "2301.09451", "url": "http://arxiv.org/abs/2301.09451v1", "pdf_url": "http://arxiv.org/pdf/2301.09451v1", "title": "A Simple Recipe for Competitive Low-compute Self supervised Vision   Models", "abs": "Self-supervised methods in vision have been mostly focused on large architectures as they seem to suffer from a significant performance drop for smaller architectures. In this paper, we propose a simple self-supervised distillation technique that can train high performance low-compute neural networks. Our main insight is that existing joint-embedding based SSL methods can be repurposed for knowledge distillation from a large self-supervised teacher to a small student model. Thus, we call our method Replace one Branch (RoB) as it simply replaces one branch of the joint-embedding training with a large teacher model. RoB is widely applicable to a number of architectures such as small ResNets, MobileNets and ViT, and pretrained models such as DINO, SwAV or iBOT. When pretraining on the ImageNet dataset, RoB yields models that compete with supervised knowledge distillation. When applied to MSN, RoB produces students with strong semi-supervised capabilities. Finally, our best ViT-Tiny models improve over prior SSL state-of-the-art on ImageNet by $2.3\\%$ and are on par or better than a supervised distilled DeiT on five downstream transfer tasks (iNaturalist, CIFAR, Clevr/Count, Clevr/Dist and Places). We hope RoB enables practical self-supervision at smaller scale.", "authors": "Quentin Duval, Ishan Misra, Nicolas Ballas", "first_author": "Nicolas Ballas", "first_end_author": "Quentin Duval; Nicolas Ballas", "publish_time": "2023-01-23 14:20:01+00:00", "update_time": "2023-01-23 14:20:01+00:00"}, {"id": "2301.09452", "url": "http://arxiv.org/abs/2301.09452v1", "pdf_url": "http://arxiv.org/pdf/2301.09452v1", "title": "Fast and robust single particle reconstruction in 3D fluorescence   microscopy", "abs": "Single particle reconstruction has recently emerged in 3D fluorescence microscopy as a powerful technique to improve the axial resolution and the degree of fluorescent labeling. It is based on the reconstruction of an average volume of a biological particle from the acquisition multiple views with unknown poses. Current methods are limited either by template bias, restriction to 2D data, high computational cost or a lack of robustness to low fluorescent labeling. In this work, we propose a single particle reconstruction method dedicated to convolutional models in 3D fluorescence microscopy that overcome these issues. We address the joint reconstruction and estimation of the poses of the particles, which translates into a challenging non-convex optimization problem. Our approach is based on a multilevel reformulation of this problem, and the development of efficient optimization techniques at each level. We demonstrate on synthetic data that our method outperforms the standard approaches in terms of resolution and reconstruction error, while achieving a low computational cost. We also perform successful reconstruction on real datasets of centrioles to show the potential of our method in concrete applications.", "authors": "Thibaut Eloy, Etienne Baudrier, Marine Laporte, Virginie Hamel, Paul Guichard, Denis Fortun", "first_author": "Denis Fortun", "first_end_author": "Thibaut Eloy; Denis Fortun", "publish_time": "2023-01-23 14:20:01+00:00", "update_time": "2023-01-23 14:20:01+00:00"}, {"id": "2301.09431", "url": "http://arxiv.org/abs/2301.09431v1", "pdf_url": "http://arxiv.org/pdf/2301.09431v1", "title": "Multi-domain stain normalization for digital pathology: A   cycle-consistent adversarial network for whole slide images", "abs": "The variation in histologic staining between different medical centers is one of the most profound challenges in the field of computer-aided diagnosis. The appearance disparity of pathological whole slide images causes algorithms to become less reliable, which in turn impedes the wide-spread applicability of downstream tasks like cancer diagnosis. Furthermore, different stainings lead to biases in the training which in case of domain shifts negatively affect the test performance. Therefore, in this paper we propose MultiStain-CycleGAN, a multi-domain approach to stain normalization based on CycleGAN. Our modifications to CycleGAN allow us to normalize images of different origins without retraining or using different models. We perform an extensive evaluation of our method using various metrics and compare it to commonly used methods that are multi-domain capable. First, we evaluate how well our method fools a domain classifier that tries to assign a medical center to an image. Then, we test our normalization on the tumor classification performance of a downstream classifier. Furthermore, we evaluate the image quality of the normalized images using the Structural similarity index and the ability to reduce the domain shift using the Fr\\'echet inception distance. We show that our method proves to be multi-domain capable, provides the highest image quality among the compared methods, and can most reliably fool the domain classifier while keeping the tumor classifier performance high. By reducing the domain influence, biases in the data can be removed on the one hand and the origin of the whole slide image can be disguised on the other, thus enhancing patient data privacy.", "authors": "Martin J. Hetz, Tabea-Clara Bucher, Titus J. Brinker", "first_author": "Titus J. Brinker", "first_end_author": "Martin J. Hetz; Titus J. Brinker", "publish_time": "2023-01-23 13:34:49+00:00", "update_time": "2023-01-23 13:34:49+00:00"}, {"id": "2301.09430", "url": "http://arxiv.org/abs/2301.09430v1", "pdf_url": "http://arxiv.org/pdf/2301.09430v1", "title": "RainDiffusion:When Unsupervised Learning Meets Diffusion Models for   Real-world Image Deraining", "abs": "What will happen when unsupervised learning meets diffusion models for real-world image deraining? To answer it, we propose RainDiffusion, the first unsupervised image deraining paradigm based on diffusion models. Beyond the traditional unsupervised wisdom of image deraining, RainDiffusion introduces stable training of unpaired real-world data instead of weakly adversarial training. RainDiffusion consists of two cooperative branches: Non-diffusive Translation Branch (NTB) and Diffusive Translation Branch (DTB). NTB exploits a cycle-consistent architecture to bypass the difficulty in unpaired training of standard diffusion models by generating initial clean/rainy image pairs. DTB leverages two conditional diffusion modules to progressively refine the desired output with initial image pairs and diffusive generative prior, to obtain a better generalization ability of deraining and rain generation. Rain-Diffusion is a non adversarial training paradigm, serving as a new standard bar for real-world image deraining. Extensive experiments confirm the superiority of our RainDiffusion over un/semi-supervised methods and show its competitive advantages over fully-supervised ones.", "authors": "Mingqiang Wei, Yiyang Shen, Yongzhen Wang, Haoran Xie, Fu Lee Wang", "first_author": "Fu Lee Wang", "first_end_author": "Mingqiang Wei; Fu Lee Wang", "publish_time": "2023-01-23 13:34:01+00:00", "update_time": "2023-01-23 13:34:01+00:00"}, {"id": "2204.07183", "url": "http://arxiv.org/abs/2204.07183v2", "pdf_url": "http://arxiv.org/pdf/2204.07183v2", "title": "Interactive Object Segmentation in 3D Point Clouds", "abs": "We propose an interactive approach for 3D instance segmentation, where users can iteratively collaborate with a deep learning model to segment objects in a 3D point cloud directly. Current methods for 3D instance segmentation are generally trained in a fully-supervised fashion, which requires large amounts of costly training labels, and does not generalize well to classes unseen during training. Few works have attempted to obtain 3D segmentation masks using human interactions. Existing methods rely on user feedback in the 2D image domain. As a consequence, users are required to constantly switch between 2D images and 3D representations, and custom architectures are employed to combine multiple input modalities. Therefore, integration with existing standard 3D models is not straightforward. The core idea of this work is to enable users to interact directly with 3D point clouds by clicking on desired 3D objects of interest~(or their background) to interactively segment the scene in an open-world setting. Specifically, our method does not require training data from any target domain, and can adapt to new environments where no appropriate training sets are available. Our system continuously adjusts the object segmentation based on the user feedback and achieves accurate dense 3D segmentation masks with minimal human effort (few clicks per object). Besides its potential for efficient labeling of large-scale and varied 3D datasets, our approach, where the user directly interacts with the 3D environment, enables new applications in AR/VR and human-robot interaction.", "authors": "Theodora Kontogianni, Ekin Celikkan, Siyu Tang, Konrad Schindler", "first_author": "Konrad Schindler", "first_end_author": "Theodora Kontogianni; Konrad Schindler", "publish_time": "2022-04-14 18:31:59+00:00", "update_time": "2023-01-23 12:50:38+00:00"}, {"id": "2301.09376", "url": "http://arxiv.org/abs/2301.09376v1", "pdf_url": "http://arxiv.org/pdf/2301.09376v1", "title": "Crowd3D: Towards Hundreds of People Reconstruction from a Single Image", "abs": "Image-based multi-person reconstruction in wide-field large scenes is critical for crowd analysis and security alert. However, existing methods cannot deal with large scenes containing hundreds of people, which encounter the challenges of large number of people, large variations in human scale, and complex spatial distribution. In this paper, we propose Crowd3D, the first framework to reconstruct the 3D poses, shapes and locations of hundreds of people with global consistency from a single large-scene image. The core of our approach is to convert the problem of complex crowd localization into pixel localization with the help of our newly defined concept, Human-scene Virtual Interaction Point (HVIP). To reconstruct the crowd with global consistency, we propose a progressive reconstruction network based on HVIP by pre-estimating a scene-level camera and a ground plane. To deal with a large number of persons and various human sizes, we also design an adaptive human-centric cropping scheme. Besides, we contribute a benchmark dataset, LargeCrowd, for crowd reconstruction in a large scene. Experimental results demonstrate the effectiveness of the proposed method. The code and datasets will be made public.", "authors": "Hao Wen, Jing Huang, Huili Cui, Haozhe Lin, YuKun Lai, Lu Fang, Kun Li", "first_author": "Kun Li", "first_end_author": "Hao Wen; Kun Li", "publish_time": "2023-01-23 11:45:27+00:00", "update_time": "2023-01-23 11:45:27+00:00"}, {"id": "2103.14675", "url": "http://arxiv.org/abs/2103.14675v6", "pdf_url": "http://arxiv.org/pdf/2103.14675v6", "title": "Synthesis of Compositional Animations from Textual Descriptions", "abs": "\"How can we animate 3D-characters from a movie script or move robots by simply telling them what we would like them to do?\" \"How unstructured and complex can we make a sentence and still generate plausible movements from it?\" These are questions that need to be answered in the long-run, as the field is still in its infancy. Inspired by these problems, we present a new technique for generating compositional actions, which handles complex input sentences. Our output is a 3D pose sequence depicting the actions in the input sentence. We propose a hierarchical two-stream sequential model to explore a finer joint-level mapping between natural language sentences and 3D pose sequences corresponding to the given motion. We learn two manifold representations of the motion -- one each for the upper body and the lower body movements. Our model can generate plausible pose sequences for short sentences describing single actions as well as long compositional sentences describing multiple sequential and superimposed actions. We evaluate our proposed model on the publicly available KIT Motion-Language Dataset containing 3D pose data with human-annotated sentences. Experimental results show that our model advances the state-of-the-art on text-based motion synthesis in objective evaluations by a margin of 50%. Qualitative evaluations based on a user study indicate that our synthesized motions are perceived to be the closest to the ground-truth motion captures for both short and compositional sentences.", "authors": "Anindita Ghosh, Noshaba Cheema, Cennet Oguz, Christian Theobalt, Philipp Slusallek", "first_author": "Philipp Slusallek", "first_end_author": "Anindita Ghosh; Philipp Slusallek", "publish_time": "2021-03-26 18:23:29+00:00", "update_time": "2023-01-23 11:17:09+00:00"}, {"id": "2206.09372", "url": "http://arxiv.org/abs/2206.09372v2", "pdf_url": "http://arxiv.org/pdf/2206.09372v2", "title": "mvHOTA: A multi-view higher order tracking accuracy metric to measure   spatial and temporal associations in multi-point detection", "abs": "Multi-point tracking is a challenging task that involves detecting points in the scene and tracking them across a sequence of frames. Computing detection-based measures like the F-measure on a frame-by-frame basis is not sufficient to assess the overall performance, as it does not interpret performance in the temporal domain. The main evaluation metric available comes from Multi-object tracking (MOT) methods to benchmark performance on datasets such as KITTI with the recently proposed higher order tracking accuracy (HOTA) metric, which is capable of providing a better description of the performance over metrics such as MOTA, DetA, and IDF1. While the HOTA metric takes into account temporal associations, it does not provide a tailored means to analyse the spatial associations of a dataset in a multi-camera setup. Moreover, there are differences in evaluating the detection task for points when compared to objects (point distances vs. bounding box overlap). Therefore in this work, we propose a multi-view higher order tracking metric (mvHOTA) to determine the accuracy of multi-point (multi-instance and multi-class) tracking methods, while taking into account temporal and spatial associations.mvHOTA can be interpreted as the geometric mean of detection, temporal, and spatial associations, thereby providing equal weighting to each of the factors. We demonstrate the use of this metric to evaluate the tracking performance on an endoscopic point detection dataset from a previously organised surgical data science challenge. Furthermore, we compare with other adjusted MOT metrics for this use-case, discuss the properties of mvHOTA, and show how the proposed multi-view Association and the Occlusion index (OI) facilitate analysis of methods with respect to handling of occlusions. The code is available at https://github.com/Cardio-AI/mvhota.", "authors": "Lalith Sharan, Halvar Kelm, Gabriele Romano, Matthias Karck, Raffaele De Simone, Sandy Engelhardt", "first_author": "Sandy Engelhardt", "first_end_author": "Lalith Sharan; Sandy Engelhardt", "publish_time": "2022-06-19 10:31:53+00:00", "update_time": "2023-01-23 10:44:12+00:00"}, {"id": "2301.05489", "url": "http://arxiv.org/abs/2301.05489v2", "pdf_url": "http://arxiv.org/pdf/2301.05489v2", "title": "Neural Image Compression with a Diffusion-Based Decoder", "abs": "Diffusion probabilistic models have recently achieved remarkable success in generating high quality image and video data. In this work, we build on this class of generative models and introduce a method for lossy compression of high resolution images. The resulting codec, which we call DIffuson-based Residual Augmentation Codec (DIRAC),is the first neural codec to allow smooth traversal of the rate-distortion-perception tradeoff at test time, while obtaining competitive performance with GAN-based methods in perceptual quality. Furthermore, while sampling from diffusion probabilistic models is notoriously expensive, we show that in the compression setting the number of steps can be drastically reduced.", "authors": "Noor Fathima Ghouse, Jens Petersen, Auke Wiggers, Tianlin Xu, Guillaume Sauti\u00e8re", "first_author": "Guillaume Sauti\u00e8re", "first_end_author": "Noor Fathima Ghouse; Guillaume Sauti\u00e8re", "publish_time": "2023-01-13 11:27:26+00:00", "update_time": "2023-01-23 10:43:28+00:00"}, {"id": "2203.04908", "url": "http://arxiv.org/abs/2203.04908v2", "pdf_url": "http://arxiv.org/pdf/2203.04908v2", "title": "Rethinking data-driven point spread function modeling with a   differentiable optical model", "abs": "In astronomy, upcoming space telescopes with wide-field optical instruments have a spatially varying point spread function (PSF). Specific scientific goals require a high-fidelity estimation of the PSF at target positions where no direct measurement of the PSF is provided. Even though observations of the PSF are available at some positions of the field of view (FOV), they are undersampled, noisy, and integrated into wavelength in the instrument's passband. PSF modeling represents a challenging ill-posed problem, as it requires building a model from degraded observations that can infer a super-resolved PSF at any wavelength and position in the FOV. Our model, coined WaveDiff, proposes a paradigm shift in the data-driven modeling of the point spread function field of telescopes. We change the data-driven modeling space from the pixels to the wavefront by adding a differentiable optical forward model into the modeling framework. This change allows the transfer of complexity from the instrumental response into the forward model. The proposed model relies on stochastic gradient descent to estimate its parameters. Our framework paves the way to building powerful, physically motivated models that do not require special calibration data. This paper demonstrates the WaveDiff model in a simplified setting of a space telescope. The proposed framework represents a performance breakthrough with respect to the existing state-of-the-art data-driven approach. The pixel reconstruction errors decrease 6-fold at observation resolution and 44-fold for a 3x super-resolution. The ellipticity errors are reduced at least 20 times, and the size error is reduced more than 250 times. By only using noisy broad-band in-focus observations, we successfully capture the PSF chromatic variations due to diffraction. Code available at https://github.com/tobias-liaudat/wf-psf.", "authors": "Tobias Liaudat, Jean-Luc Starck, Martin Kilbinger, Pierre-Antoine Frugier", "first_author": "Pierre-Antoine Frugier", "first_end_author": "Tobias Liaudat; Pierre-Antoine Frugier", "publish_time": "2022-03-09 17:39:18+00:00", "update_time": "2023-01-23 10:07:36+00:00"}, {"id": "2301.09339", "url": "http://arxiv.org/abs/2301.09339v1", "pdf_url": "http://arxiv.org/pdf/2301.09339v1", "title": "Computer Vision for a Camel-Vehicle Collision Mitigation System", "abs": "As the population grows and more land is being used for urbanization, ecosystems are disrupted by our roads and cars. This expansion of infrastructure cuts through wildlife territories, leading to many instances of Wildlife-Vehicle Collision (WVC). These instances of WVC are a global issue that is having a global socio-economic impact, resulting in billions of dollars in property damage and, at times, fatalities for vehicle occupants. In Saudi Arabia, this issue is similar, with instances of Camel-Vehicle Collision (CVC) being particularly deadly due to the large size of camels, which results in a 25% fatality rate [4]. The focus of this work is to test different object detection models on the task of detecting camels on the road. The Deep Learning (DL) object detection models used in the experiments are: CenterNet, EfficientDet, Faster R-CNN, and SSD. Results of the experiments show that CenterNet performed the best in terms of accuracy and was the most efficient in training. In the future, the plan is to expand on this work by developing a system to make countryside roads safer.", "authors": "Khalid Alnujaidi, Ghadah Alhabib", "first_author": "Ghadah Alhabib", "first_end_author": "Khalid Alnujaidi; Ghadah Alhabib", "publish_time": "2023-01-23 09:45:31+00:00", "update_time": "2023-01-23 09:45:31+00:00"}, {"id": "2301.09338", "url": "http://arxiv.org/abs/2301.09338v1", "pdf_url": "http://arxiv.org/pdf/2301.09338v1", "title": "Employing similarity to highlight differences: On the impact of   anatomical assumptions in chest X-ray registration methods", "abs": "To facilitate both the detection and the interpretation of findings in chest X-rays, comparison with a previous image of the same patient is very valuable to radiologists. Today, the most common approach for deep learning methods to automatically inspect chest X-rays disregards the patient history and classifies only single images as normal or abnormal. Nevertheless, several methods for assisting in the task of comparison through image registration have been proposed in the past. However, as we illustrate, they tend to miss specific types of pathological changes like cardiomegaly and effusion. Due to assumptions on fixed anatomical structures or their measurements of registration quality they tend to produce unnaturally deformed warp fields impacting visualization of the difference image between moving and fixed images. To overcome these limitations, we are the first to use a new paradigm based on individual rib pair segmentation for anatomy penalized registration, which proves a natural way to limit folding of the warp field, especially beneficial for image pairs with large pathological changes. We show that it is possible to develop a deep learning powered solution that can visualize what other methods overlook on a large data set of paired public images, starting from less than 25 fully labeled and 50 partly labeled training images, employing sequential instance memory segmentation with hole dropout, weak labeling, coarse-to-fine refinement and Gaussian mixture model histogram matching. We statistically evaluate the benefits of our method over the SOTA and highlight the limits of currently used metrics for registration of chest X-rays.", "authors": "Astrid Berg, Eva Vandersmissen, Maria Wimmer, David Major, Theresa Neubauer, Dimitrios Lenis, Jeroen Cant, Annemiek Snoeckx, Katja B\u00fchler", "first_author": "Katja B\u00fchler", "first_end_author": "Astrid Berg; Katja B\u00fchler", "publish_time": "2023-01-23 09:42:49+00:00", "update_time": "2023-01-23 09:42:49+00:00"}, {"id": "2301.09322", "url": "http://arxiv.org/abs/2301.09322v1", "pdf_url": "http://arxiv.org/pdf/2301.09322v1", "title": "Deep Learning-Based Assessment of Cerebral Microbleeds in COVID-19", "abs": "Cerebral Microbleeds (CMBs), typically captured as hypointensities from susceptibility-weighted imaging (SWI), are particularly important for the study of dementia, cerebrovascular disease, and normal aging. Recent studies on COVID-19 have shown an increase in CMBs of coronavirus cases. Automatic detection of CMBs is challenging due to the small size and amount of CMBs making the classes highly imbalanced, lack of publicly available annotated data, and similarity with CMB mimics such as calcifications, irons, and veins. Hence, the existing deep learning methods are mostly trained on very limited research data and fail to generalize to unseen data with high variability and cannot be used in clinical setups. To this end, we propose an efficient 3D deep learning framework that is actively trained on multi-domain data. Two public datasets assigned for normal aging, stroke, and Alzheimer's disease analysis as well as an in-house dataset for COVID-19 assessment are used to train and evaluate the models. The obtained results show that the proposed method is robust to low-resolution images and achieves 78% recall and 80% precision on the entire test set with an average false positive of 1.6 per scan.", "authors": "Neus Rodeja Ferrer, Malini Vendela Sagar, Kiril Vadimovic Klein, Christina Kruuse, Mads Nielsen, Mostafa Mehdipour Ghazi", "first_author": "Mostafa Mehdipour Ghazi", "first_end_author": "Neus Rodeja Ferrer; Mostafa Mehdipour Ghazi", "publish_time": "2023-01-23 08:46:17+00:00", "update_time": "2023-01-23 08:46:17+00:00"}, {"id": "2301.09318", "url": "http://arxiv.org/abs/2301.09318v1", "pdf_url": "http://arxiv.org/pdf/2301.09318v1", "title": "Toward Foundation Models for Earth Monitoring: Generalizable Deep   Learning Models for Natural Hazard Segmentation", "abs": "Climate change results in an increased probability of extreme weather events that put societies and businesses at risk on a global scale. Therefore, near real-time mapping of natural hazards is an emerging priority for the support of natural disaster relief, risk management, and informing governmental policy decisions. Recent methods to achieve near real-time mapping increasingly leverage deep learning (DL). However, DL-based approaches are designed for one specific task in a single geographic region based on specific frequency bands of satellite data. Therefore, DL models used to map specific natural hazards struggle with their generalization to other types of natural hazards in unseen regions. In this work, we propose a methodology to significantly improve the generalizability of DL natural hazards mappers based on pre-training on a suitable pre-task. Without access to any data from the target domain, we demonstrate this improved generalizability across four U-Net architectures for the segmentation of unseen natural hazards. Importantly, our method is invariant to geographic differences and differences in the type of frequency bands of satellite data. By leveraging characteristics of unlabeled images from the target domain that are publicly available, our approach is able to further improve the generalization behavior without fine-tuning. Thereby, our approach supports the development of foundation models for earth monitoring with the objective of directly segmenting unseen natural hazards across novel geographic regions given different sources of satellite imagery.", "authors": "Johannes Jakubik, Michal Muszynski, Michael V\u00f6ssing, Niklas K\u00fchl, Thomas Brunschwiler", "first_author": "Thomas Brunschwiler", "first_end_author": "Johannes Jakubik; Thomas Brunschwiler", "publish_time": "2023-01-23 08:35:00+00:00", "update_time": "2023-01-23 08:35:00+00:00"}, {"id": "2301.09315", "url": "http://arxiv.org/abs/2301.09315v1", "pdf_url": "http://arxiv.org/pdf/2301.09315v1", "title": "AI-Based Framework for Understanding Car Following Behaviors of Drivers   in A Naturalistic Driving Environment", "abs": "The most common type of accident on the road is a rear-end crash. These crashes have a significant negative impact on traffic flow and are frequently fatal. To gain a more practical understanding of these scenarios, it is necessary to accurately model car following behaviors that result in rear-end crashes. Numerous studies have been carried out to model drivers' car-following behaviors; however, the majority of these studies have relied on simulated data, which may not accurately represent real-world incidents. Furthermore, most studies are restricted to modeling the ego vehicle's acceleration, which is insufficient to explain the behavior of the ego vehicle. As a result, the current study attempts to address these issues by developing an artificial intelligence framework for extracting features relevant to understanding driver behavior in a naturalistic environment. Furthermore, the study modeled the acceleration of both the ego vehicle and the leading vehicle using extracted information from NDS videos. According to the study's findings, young people are more likely to be aggressive drivers than elderly people. In addition, when modeling the ego vehicle's acceleration, it was discovered that the relative velocity between the ego vehicle and the leading vehicle was more important than the distance between the two vehicles.", "authors": "Armstrong Aboah, Abdul Rashid Mussah, Yaw Adu-Gyamfi", "first_author": "Yaw Adu-Gyamfi", "first_end_author": "Armstrong Aboah; Yaw Adu-Gyamfi", "publish_time": "2023-01-23 08:24:33+00:00", "update_time": "2023-01-23 08:24:33+00:00"}, {"id": "2301.09299", "url": "http://arxiv.org/abs/2301.09299v1", "pdf_url": "http://arxiv.org/pdf/2301.09299v1", "title": "Self-Supervised Image Representation Learning: Transcending Masking with   Paired Image Overlay", "abs": "Self-supervised learning has become a popular approach in recent years for its ability to learn meaningful representations without the need for data annotation. This paper proposes a novel image augmentation technique, overlaying images, which has not been widely applied in self-supervised learning. This method is designed to provide better guidance for the model to understand underlying information, resulting in more useful representations. The proposed method is evaluated using contrastive learning, a widely used self-supervised learning method that has shown solid performance in downstream tasks. The results demonstrate the effectiveness of the proposed augmentation technique in improving the performance of self-supervised models.", "authors": "Yinheng Li, Han Ding, Shaofei Wang", "first_author": "Shaofei Wang", "first_end_author": "Yinheng Li; Shaofei Wang", "publish_time": "2023-01-23 07:00:04+00:00", "update_time": "2023-01-23 07:00:04+00:00"}, {"id": "2212.11541", "url": "http://arxiv.org/abs/2212.11541v2", "pdf_url": "http://arxiv.org/pdf/2212.11541v2", "title": "Generative Colorization of Structured Mobile Web Pages", "abs": "Color is a critical design factor for web pages, affecting important factors such as viewer emotions and the overall trust and satisfaction of a website. Effective coloring requires design knowledge and expertise, but if this process could be automated through data-driven modeling, efficient exploration and alternative workflows would be possible. However, this direction remains underexplored due to the lack of a formalization of the web page colorization problem, datasets, and evaluation protocols. In this work, we propose a new dataset consisting of e-commerce mobile web pages in a tractable format, which are created by simplifying the pages and extracting canonical color styles with a common web browser. The web page colorization problem is then formalized as a task of estimating plausible color styles for a given web page content with a given hierarchical structure of the elements. We present several Transformer-based methods that are adapted to this task by prepending structural message passing to capture hierarchical relationships between elements. Experimental results, including a quantitative evaluation designed for this task, demonstrate the advantages of our methods over statistical and image colorization methods. The code is available at https://github.com/CyberAgentAILab/webcolor.", "authors": "Kotaro Kikuchi, Naoto Inoue, Mayu Otani, Edgar Simo-Serra, Kota Yamaguchi", "first_author": "Kota Yamaguchi", "first_end_author": "Kotaro Kikuchi; Kota Yamaguchi", "publish_time": "2022-12-22 08:36:55+00:00", "update_time": "2023-01-23 06:58:44+00:00"}, {"id": "2301.09282", "url": "http://arxiv.org/abs/2301.09282v1", "pdf_url": "http://arxiv.org/pdf/2301.09282v1", "title": "Classification of Luminal Subtypes in Full Mammogram Images Using   Transfer Learning", "abs": "Automatic identification of patients with luminal and non-luminal subtypes during a routine mammography screening can support clinicians in streamlining breast cancer therapy planning. Recent machine learning techniques have shown promising results in molecular subtype classification in mammography; however, they are highly dependent on pixel-level annotations, handcrafted, and radiomic features. In this work, we provide initial insights into the luminal subtype classification in full mammogram images trained using only image-level labels. Transfer learning is applied from a breast abnormality classification task, to finetune a ResNet-18-based luminal versus non-luminal subtype classification task. We present and compare our results on the publicly available CMMD dataset and show that our approach significantly outperforms the baseline classifier by achieving a mean AUC score of 0.6688 and a mean F1 score of 0.6693 on the test dataset. The improvement over baseline is statistically significant, with a p-value of p<0.0001.", "authors": "Adarsh Bhandary Panambur, Prathmesh Madhu, Andreas Maier", "first_author": "Andreas Maier", "first_end_author": "Adarsh Bhandary Panambur; Andreas Maier", "publish_time": "2023-01-23 05:58:26+00:00", "update_time": "2023-01-23 05:58:26+00:00"}, {"id": "2110.04070", "url": "http://arxiv.org/abs/2110.04070v3", "pdf_url": "http://arxiv.org/pdf/2110.04070v3", "title": "Dataset Structural Index: Leveraging a machine's perspective towards   visual data", "abs": "With advances in vision and perception architectures, we have realized that working with data is equally crucial, if not more, than the algorithms. Till today, we have trained machines based on our knowledge and perspective of the world. The entire concept of Dataset Structural Index(DSI) revolves around understanding a machine`s perspective of the dataset. With DSI, I show two meta values with which we can get more information over a visual dataset and use it to optimize data, create better architectures, and have an ability to guess which model would work best. These two values are the Variety contribution ratio and Similarity matrix. In the paper, I show many applications of DSI, one of which is how the same level of accuracy can be achieved with the same model architectures trained over less amount of data.", "authors": "Dishant Parikh", "first_author": "Dishant Parikh", "first_end_author": "Dishant Parikh; Dishant Parikh", "publish_time": "2021-10-05 06:40:16+00:00", "update_time": "2023-01-23 05:33:48+00:00"}, {"id": "2301.09268", "url": "http://arxiv.org/abs/2301.09268v1", "pdf_url": "http://arxiv.org/pdf/2301.09268v1", "title": "PCBDet: An Efficient Deep Neural Network Object Detection Architecture   for Automatic PCB Component Detection on the Edge", "abs": "There can be numerous electronic components on a given PCB, making the task of visual inspection to detect defects very time-consuming and prone to error, especially at scale. There has thus been significant interest in automatic PCB component detection, particularly leveraging deep learning. However, deep neural networks typically require high computational resources, possibly limiting their feasibility in real-world use cases in manufacturing, which often involve high-volume and high-throughput detection with constrained edge computing resource availability. As a result of an exploration of efficient deep neural network architectures for this use case, we introduce PCBDet, an attention condenser network design that provides state-of-the-art inference throughput while achieving superior PCB component detection performance compared to other state-of-the-art efficient architecture designs. Experimental results show that PCBDet can achieve up to 2$\\times$ inference speed-up on an ARM Cortex A72 processor when compared to an EfficientNet-based design while achieving $\\sim$2-4\\% higher mAP on the FICS-PCB benchmark dataset.", "authors": "Brian Li, Steven Palayew, Francis Li, Saad Abbasi, Saeejith Nair, Alexander Wong", "first_author": "Alexander Wong", "first_end_author": "Brian Li; Alexander Wong", "publish_time": "2023-01-23 04:34:25+00:00", "update_time": "2023-01-23 04:34:25+00:00"}, {"id": "2301.09266", "url": "http://arxiv.org/abs/2301.09266v1", "pdf_url": "http://arxiv.org/pdf/2301.09266v1", "title": "FInC Flow: Fast and Invertible $k \\times k$ Convolutions for Normalizing   Flows", "abs": "Invertible convolutions have been an essential element for building expressive normalizing flow-based generative models since their introduction in Glow. Several attempts have been made to design invertible $k \\times k$ convolutions that are efficient in training and sampling passes. Though these attempts have improved the expressivity and sampling efficiency, they severely lagged behind Glow which used only $1 \\times 1$ convolutions in terms of sampling time. Also, many of the approaches mask a large number of parameters of the underlying convolution, resulting in lower expressivity on a fixed run-time budget. We propose a $k \\times k$ convolutional layer and Deep Normalizing Flow architecture which i.) has a fast parallel inversion algorithm with running time O$(n k^2)$ ($n$ is height and width of the input image and k is kernel size), ii.) masks the minimal amount of learnable parameters in a layer. iii.) gives better forward pass and sampling times comparable to other $k \\times k$ convolution-based models on real-world benchmarks. We provide an implementation of the proposed parallel algorithm for sampling using our invertible convolutions on GPUs. Benchmarks on CIFAR-10, ImageNet, and CelebA datasets show comparable performance to previous works regarding bits per dimension while significantly improving the sampling time.", "authors": "Aditya Kallappa, Sandeep Nagar, Girish Varma", "first_author": "Girish Varma", "first_end_author": "Aditya Kallappa; Girish Varma", "publish_time": "2023-01-23 04:31:03+00:00", "update_time": "2023-01-23 04:31:03+00:00"}, {"id": "2301.09264", "url": "http://arxiv.org/abs/2301.09264v1", "pdf_url": "http://arxiv.org/pdf/2301.09264v1", "title": "Efficient Training Under Limited Resources", "abs": "Training time budget and size of the dataset are among the factors affecting the performance of a Deep Neural Network (DNN). This paper shows that Neural Architecture Search (NAS), Hyper Parameters Optimization (HPO), and Data Augmentation help DNNs perform much better while these two factors are limited. However, searching for an optimal architecture and the best hyperparameter values besides a good combination of data augmentation techniques under low resources requires many experiments. We present our approach to achieving such a goal in three steps: reducing training epoch time by compressing the model while maintaining the performance compared to the original model, preventing model overfitting when the dataset is small, and performing the hyperparameter tuning. We used NOMAD, which is a blackbox optimization software based on a derivative-free algorithm to do NAS and HPO. Our work achieved an accuracy of 86.0 % on a tiny subset of Mini-ImageNet at the ICLR 2021 Hardware Aware Efficient Training (HAET) Challenge and won second place in the competition. The competition results can be found at haet2021.github.io/challenge and our source code can be found at github.com/DouniaLakhmiri/ICLR\\_HAET2021.", "authors": "Mahdi Zolnouri, Dounia Lakhmiri, Christophe Tribes, Eyy\u00fcb Sari, S\u00e9bastien Le Digabel", "first_author": "S\u00e9bastien Le Digabel", "first_end_author": "Mahdi Zolnouri; S\u00e9bastien Le Digabel", "publish_time": "2023-01-23 04:26:20+00:00", "update_time": "2023-01-23 04:26:20+00:00"}, {"id": "2301.09257", "url": "http://arxiv.org/abs/2301.09257v1", "pdf_url": "http://arxiv.org/pdf/2301.09257v1", "title": "Real-Time Simultaneous Localization and Mapping with LiDAR intensity", "abs": "We propose a novel real-time LiDAR intensity image-based simultaneous localization and mapping method , which addresses the geometry degeneracy problem in unstructured environments. Traditional LiDAR-based front-end odometry mostly relies on geometric features such as points, lines and planes. A lack of these features in the environment can lead to the failure of the entire odometry system. To avoid this problem, we extract feature points from the LiDAR-generated point cloud that match features identified in LiDAR intensity images. We then use the extracted feature points to perform scan registration and estimate the robot ego-movement. For the back-end, we jointly optimize the distance between the corresponding feature points, and the point to plane distance for planes identified in the map. In addition, we use the features extracted from intensity images to detect loop closure candidates from previous scans and perform pose graph optimization. Our experiments show that our method can run in real time with high accuracy and works well with illumination changes, low-texture, and unstructured environments.", "authors": "Wenqiang Du, Giovanni Beltrame", "first_author": "Giovanni Beltrame", "first_end_author": "Wenqiang Du; Giovanni Beltrame", "publish_time": "2023-01-23 03:59:48+00:00", "update_time": "2023-01-23 03:59:48+00:00"}, {"id": "2301.09255", "url": "http://arxiv.org/abs/2301.09255v1", "pdf_url": "http://arxiv.org/pdf/2301.09255v1", "title": "Combined Use of Federated Learning and Image Encryption for   Privacy-Preserving Image Classification with Vision Transformer", "abs": "In recent years, privacy-preserving methods for deep learning have become an urgent problem. Accordingly, we propose the combined use of federated learning (FL) and encrypted images for privacy-preserving image classification under the use of the vision transformer (ViT). The proposed method allows us not only to train models over multiple participants without directly sharing their raw data but to also protect the privacy of test (query) images for the first time. In addition, it can also maintain the same accuracy as normally trained models. In an experiment, the proposed method was demonstrated to well work without any performance degradation on the CIFAR-10 and CIFAR-100 datasets.", "authors": "Teru Nagamori, Hitoshi Kiya", "first_author": "Hitoshi Kiya", "first_end_author": "Teru Nagamori; Hitoshi Kiya", "publish_time": "2023-01-23 03:41:02+00:00", "update_time": "2023-01-23 03:41:02+00:00"}, {"id": "2301.09254", "url": "http://arxiv.org/abs/2301.09254v1", "pdf_url": "http://arxiv.org/pdf/2301.09254v1", "title": "Learning to Linearize Deep Neural Networks for Secure and Efficient   Private Inference", "abs": "The large number of ReLU non-linearity operations in existing deep neural networks makes them ill-suited for latency-efficient private inference (PI). Existing techniques to reduce ReLU operations often involve manual effort and sacrifice significant accuracy. In this paper, we first present a novel measure of non-linearity layers' ReLU sensitivity, enabling mitigation of the time-consuming manual efforts in identifying the same. Based on this sensitivity, we then present SENet, a three-stage training method that for a given ReLU budget, automatically assigns per-layer ReLU counts, decides the ReLU locations for each layer's activation map, and trains a model with significantly fewer ReLUs to potentially yield latency and communication efficient PI. Experimental evaluations with multiple models on various datasets show SENet's superior performance both in terms of reduced ReLUs and improved classification accuracy compared to existing alternatives. In particular, SENet can yield models that require up to ~2x fewer ReLUs while yielding similar accuracy. For a similar ReLU budget SENet can yield models with ~2.32% improved classification accuracy, evaluated on CIFAR-100.", "authors": "Souvik Kundu, Shunlin Lu, Yuke Zhang, Jacqueline Liu, Peter A. Beerel", "first_author": "Peter A. Beerel", "first_end_author": "Souvik Kundu; Peter A. Beerel", "publish_time": "2023-01-23 03:33:38+00:00", "update_time": "2023-01-23 03:33:38+00:00"}, {"id": "2301.09253", "url": "http://arxiv.org/abs/2301.09253v1", "pdf_url": "http://arxiv.org/pdf/2301.09253v1", "title": "CircNet: Meshing 3D Point Clouds with Circumcenter Detection", "abs": "Reconstructing 3D point clouds into triangle meshes is a key problem in computational geometry and surface reconstruction. Point cloud triangulation solves this problem by providing edge information to the input points. Since no vertex interpolation is involved, it is beneficial to preserve sharp details on the surface. Taking advantage of learning-based techniques in triangulation, existing methods enumerate the complete combinations of candidate triangles, which is both complex and inefficient. In this paper, we leverage the duality between a triangle and its circumcenter, and introduce a deep neural network that detects the circumcenters to achieve point cloud triangulation. Specifically, we introduce multiple anchor priors to divide the neighborhood space of each point. The neural network then learns to predict the presences and locations of circumcenters under the guidance of those anchors. We extract the triangles dual to the detected circumcenters to form a primitive mesh, from which an edge-manifold mesh is produced via simple post-processing. Unlike existing learning-based triangulation methods, the proposed method bypasses an exhaustive enumeration of triangle combinations and local surface parameterization. We validate the efficiency, generalization, and robustness of our method on prominent datasets of both watertight and open surfaces. The code and trained models are provided at https://github.com/Ruitao-L/CircNet.", "authors": "Huan Lei, Ruitao Leng, Liang Zheng, Hongdong Li", "first_author": "Hongdong Li", "first_end_author": "Huan Lei; Hongdong Li", "publish_time": "2023-01-23 03:32:57+00:00", "update_time": "2023-01-23 03:32:57+00:00"}, {"id": "2301.09249", "url": "http://arxiv.org/abs/2301.09249v1", "pdf_url": "http://arxiv.org/pdf/2301.09249v1", "title": "Exploring Active 3D Object Detection from a Generalization Perspective", "abs": "To alleviate the high annotation cost in LiDAR-based 3D object detection, active learning is a promising solution that learns to select only a small portion of unlabeled data to annotate, without compromising model performance. Our empirical study, however, suggests that mainstream uncertainty-based and diversity-based active learning policies are not effective when applied in the 3D detection task, as they fail to balance the trade-off between point cloud informativeness and box-level annotation costs. To overcome this limitation, we jointly investigate three novel criteria in our framework Crb for point cloud acquisition - label conciseness}, feature representativeness and geometric balance, which hierarchically filters out the point clouds of redundant 3D bounding box labels, latent features and geometric characteristics (e.g., point cloud density) from the unlabeled sample pool and greedily selects informative ones with fewer objects to annotate. Our theoretical analysis demonstrates that the proposed criteria align the marginal distributions of the selected subset and the prior distributions of the unseen test set, and minimizes the upper bound of the generalization error. To validate the effectiveness and applicability of \\textsc{Crb}, we conduct extensive experiments on the two benchmark 3D object detection datasets of KITTI and Waymo and examine both one-stage (\\textit{i.e.}, \\textsc{Second}) and two-stage 3D detectors (i.e., Pv-rcnn). Experiments evidence that the proposed approach outperforms existing active learning strategies and achieves fully supervised performance requiring $1\\%$ and $8\\%$ annotations of bounding boxes and point clouds, respectively. Source code: https://github.com/Luoyadan/CRB-active-3Ddet.", "authors": "Yadan Luo, Zhuoxiao Chen, Zijian Wang, Xin Yu, Zi Huang, Mahsa Baktashmotlagh", "first_author": "Mahsa Baktashmotlagh", "first_end_author": "Yadan Luo; Mahsa Baktashmotlagh", "publish_time": "2023-01-23 02:43:03+00:00", "update_time": "2023-01-23 02:43:03+00:00"}, {"id": "2301.07650", "url": "http://arxiv.org/abs/2301.07650v2", "pdf_url": "http://arxiv.org/pdf/2301.07650v2", "title": "Facial Thermal and Blood Perfusion Patterns of Human Emotions:   Proof-of-Concept", "abs": "In this work, a preliminary study of proof-of-concept was conducted to evaluate the performance of the thermographic and blood perfusion data when emotions of positive and negative valence are applied, where the blood perfusion data are obtained from the thermographic data. The images were obtained for baseline, positive, and negative valence according to the protocol of the Geneva Affective Picture Database. Absolute and percentage differences of average values of the data between the valences and the baseline were calculated for different regions of interest (forehead, periorbital eyes, cheeks, nose and upper lips). For negative valence, a decrease in temperature and blood perfusion was observed in the regions of interest, and the effect was greater on the left side than on the right side. In positive valence, the temperature and blood perfusion increased in some cases, showing a complex pattern. The temperature and perfusion of the nose was reduced for both valences, which is indicative of the arousal dimension. The blood perfusion images were found to be greater contrast; the percentage differences in the blood perfusion images are greater than those obtained in thermographic images. Moreover, the blood perfusion images, and vasomotor answer are consistent, therefore, they can be a better biomarker than thermographic analysis in identifying emotions.", "authors": "Victor H. Aristizabal-Tique, Marcela Henao-Perez, Diana Carolina Lopez-Medina, Renato Zambrano-Cruz, Gloria Diaz-Londo\u00f1od", "first_author": "Gloria Diaz-Londo\u00f1od", "first_end_author": "Victor H. Aristizabal-Tique; Gloria Diaz-Londo\u00f1od", "publish_time": "2023-01-18 16:52:40+00:00", "update_time": "2023-01-23 02:11:45+00:00"}, {"id": "2210.09520", "url": "http://arxiv.org/abs/2210.09520v4", "pdf_url": "http://arxiv.org/pdf/2210.09520v4", "title": "Using Language to Extend to Unseen Domains", "abs": "It is expensive to collect training data for every possible domain that a vision model may encounter when deployed. We instead consider how simply verbalizing the training domain (e.g. \"photos of birds\") as well as domains we want to extend to but do not have data for (e.g. \"paintings of birds\") can improve robustness. Using a multimodal model with a joint image and language embedding space, our method LADS learns a transformation of the image embeddings from the training domain to each unseen test domain, while preserving task relevant information. Without using any images from the unseen test domain, we show that over the extended domain containing both training and unseen test domains, LADS outperforms standard fine-tuning and ensemble approaches over a suite of four benchmarks targeting domain adaptation and dataset bias.", "authors": "Lisa Dunlap, Clara Mohri, Devin Guillory, Han Zhang, Trevor Darrell, Joseph E. Gonzalez, Aditi Raghunathan, Anja Rohrbach", "first_author": "Anja Rohrbach", "first_end_author": "Lisa Dunlap; Anja Rohrbach", "publish_time": "2022-10-18 01:14:02+00:00", "update_time": "2023-01-22 22:33:38+00:00"}, {"id": "2301.09219", "url": "http://arxiv.org/abs/2301.09219v1", "pdf_url": "http://arxiv.org/pdf/2301.09219v1", "title": "Applied Deep Learning to Identify and Localize Polyps from Endoscopic   Images", "abs": "Deep learning based neural networks have gained popularity for a variety of biomedical imaging applications. In the last few years several works have shown the use of these methods for colon cancer detection and the early results have been promising. These methods can potentially be utilized to assist doctor's and may help in identifying the number of lesions or abnormalities in a diagnosis session. From our literature survey we found out that there is a lack of publicly available labeled data. Thus, as part of this work, we have aimed at open sourcing a dataset which contains annotations of polyps and ulcers. This is the first dataset that's coming from India containing polyp and ulcer images. The dataset can be used for detection and classification tasks. We also evaluated our dataset with several popular deep learning object detection models that's trained on large publicly available datasets and found out empirically that the model trained on one dataset works well on our dataset that has data being captured in a different acquisition device.", "authors": "Chandana Raju, Sumedh Vilas Datar, Kushala Hari, Kavin Vijay, Suma Ningappa", "first_author": "Suma Ningappa", "first_end_author": "Chandana Raju; Suma Ningappa", "publish_time": "2023-01-22 22:14:25+00:00", "update_time": "2023-01-22 22:14:25+00:00"}, {"id": "2301.09213", "url": "http://arxiv.org/abs/2301.09213v1", "pdf_url": "http://arxiv.org/pdf/2301.09213v1", "title": "FRAME: Fast and Robust Autonomous 3D point cloud Map-merging for   Egocentric multi-robot exploration", "abs": "This article presents a 3D point cloud map-merging framework for egocentric heterogeneous multi-robot exploration, based on overlap detection and alignment, that is independent of a manual initial guess or prior knowledge of the robots' poses. The novel proposed solution utilizes state-of-the-art place recognition learned descriptors, that through the framework's main pipeline, offer a fast and robust region overlap estimation, hence eliminating the need for the time-consuming global feature extraction and feature matching process that is typically used in 3D map integration. The region overlap estimation provides a homogeneous rigid transform that is applied as an initial condition in the point cloud registration algorithm Fast-GICP, which provides the final and refined alignment. The efficacy of the proposed framework is experimentally evaluated based on multiple field multi-robot exploration missions in underground environments, where both ground and aerial robots are deployed, with different sensor configurations.", "authors": "Nikolaos Stathoulopoulos, Anton Koval, Ali-akbar Agha-mohammadi, George Nikolakopoulos", "first_author": "George Nikolakopoulos", "first_end_author": "Nikolaos Stathoulopoulos; George Nikolakopoulos", "publish_time": "2023-01-22 21:59:38+00:00", "update_time": "2023-01-22 21:59:38+00:00"}, {"id": "2301.09209", "url": "http://arxiv.org/abs/2301.09209v1", "pdf_url": "http://arxiv.org/pdf/2301.09209v1", "title": "Summarize the Past to Predict the Future: Natural Language Descriptions   of Context Boost Multimodal Object Interaction", "abs": "We study the task of object interaction anticipation in egocentric videos. Successful prediction of future actions and objects requires an understanding of the spatio-temporal context formed by past actions and object relationships. We propose TransFusion, a multimodal transformer-based architecture, that effectively makes use of the representational power of language by summarizing past actions concisely. TransFusion leverages pre-trained image captioning models and summarizes the caption, focusing on past actions and objects. This action context together with a single input frame is processed by a multimodal fusion module to forecast the next object interactions. Our model enables more efficient end-to-end learning by replacing dense video features with language representations, allowing us to benefit from knowledge encoded in large pre-trained models. Experiments on Ego4D and EPIC-KITCHENS-100 show the effectiveness of our multimodal fusion model and the benefits of using language-based context summaries. Our method outperforms state-of-the-art approaches by 40.4% in overall mAP on the Ego4D test set. We show the generality of TransFusion via experiments on EPIC-KITCHENS-100. Video and code are available at: https://eth-ait.github.io/transfusion-proj/.", "authors": "Razvan-George Pasca, Alexey Gavryushin, Yen-Ling Kuo, Otmar Hilliges, Xi Wang", "first_author": "Xi Wang", "first_end_author": "Razvan-George Pasca; Xi Wang", "publish_time": "2023-01-22 21:30:12+00:00", "update_time": "2023-01-22 21:30:12+00:00"}, {"id": "2301.09190", "url": "http://arxiv.org/abs/2301.09190v1", "pdf_url": "http://arxiv.org/pdf/2301.09190v1", "title": "Apples and Oranges? Assessing Image Quality over Content Recognition", "abs": "Image recognition and quality assessment are two important viewing tasks, while potentially following different visual mechanisms. This paper investigates if the two tasks can be performed in a multitask learning manner. A sequential spatial-channel attention module is proposed to simulate the visual attention and contrast sensitivity mechanisms that are crucial for content recognition and quality assessment. Spatial attention is shared between content recognition and quality assessment, while channel attention is solely for quality assessment. Such attention module is integrated into Transformer to build a uniform model for the two viewing tasks. The experimental results have demonstrated that the proposed uniform model can achieve promising performance for both quality assessment and content recognition tasks.", "authors": "Junyong You", "first_author": "Junyong You", "first_end_author": "Junyong You; Junyong You", "publish_time": "2023-01-22 19:51:22+00:00", "update_time": "2023-01-22 19:51:22+00:00"}, {"id": "2207.13798", "url": "http://arxiv.org/abs/2207.13798v5", "pdf_url": "http://arxiv.org/pdf/2207.13798v5", "title": "Look at Adjacent Frames: Video Anomaly Detection without Offline   Training", "abs": "We propose a solution to detect anomalous events in videos without the need to train a model offline. Specifically, our solution is based on a randomly-initialized multilayer perceptron that is optimized online to reconstruct video frames, pixel-by-pixel, from their frequency information. Based on the information shifts between adjacent frames, an incremental learner is used to update parameters of the multilayer perceptron after observing each frame, thus allowing to detect anomalous events along the video stream. Traditional solutions that require no offline training are limited to operating on videos with only a few abnormal frames. Our solution breaks this limit and achieves strong performance on benchmark datasets.", "authors": "Yuqi Ouyang, Guodong Shen, Victor Sanchez", "first_author": "Victor Sanchez", "first_end_author": "Yuqi Ouyang; Victor Sanchez", "publish_time": "2022-07-27 21:18:58+00:00", "update_time": "2023-01-22 18:34:16+00:00"}, {"id": "2301.09174", "url": "http://arxiv.org/abs/2301.09174v1", "pdf_url": "http://arxiv.org/pdf/2301.09174v1", "title": "MATT: Multimodal Attention Level Estimation for e-learning Platforms", "abs": "This work presents a new multimodal system for remote attention level estimation based on multimodal face analysis. Our multimodal approach uses different parameters and signals obtained from the behavior and physiological processes that have been related to modeling cognitive load such as faces gestures (e.g., blink rate, facial actions units) and user actions (e.g., head pose, distance to the camera). The multimodal system uses the following modules based on Convolutional Neural Networks (CNNs): Eye blink detection, head pose estimation, facial landmark detection, and facial expression features. First, we individually evaluate the proposed modules in the task of estimating the student's attention level captured during online e-learning sessions. For that we trained binary classifiers (high or low attention) based on Support Vector Machines (SVM) for each module. Secondly, we find out to what extent multimodal score level fusion improves the attention level estimation. The mEBAL database is used in the experimental framework, a public multi-modal database for attention level estimation obtained in an e-learning environment that contains data from 38 users while conducting several e-learning tasks of variable difficulty (creating changes in student cognitive loads).", "authors": "Roberto Daza, Luis F. Gomez, Aythami Morales, Julian Fierrez, Ruben Tolosana, Ruth Cobos, Javier Ortega-Garcia", "first_author": "Javier Ortega-Garcia", "first_end_author": "Roberto Daza; Javier Ortega-Garcia", "publish_time": "2023-01-22 18:18:20+00:00", "update_time": "2023-01-22 18:18:20+00:00"}, {"id": "2301.09164", "url": "http://arxiv.org/abs/2301.09164v1", "pdf_url": "http://arxiv.org/pdf/2301.09164v1", "title": "Unifying Synergies between Self-supervised Learning and Dynamic   Computation", "abs": "Self-supervised learning (SSL) approaches have made major strides forward by emulating the performance of their supervised counterparts on several computer vision benchmarks. This, however, comes at a cost of substantially larger model sizes, and computationally expensive training strategies, which eventually lead to larger inference times making it impractical for resource constrained industrial settings. Techniques like knowledge distillation (KD), dynamic computation (DC), and pruning are often used to obtain a lightweight sub-network, which usually involves multiple epochs of fine-tuning of a large pre-trained model, making it more computationally challenging.   In this work we propose a novel perspective on the interplay between SSL and DC paradigms that can be leveraged to simultaneously learn a dense and gated (sparse/lightweight) sub-network from scratch offering a good accuracy-efficiency trade-off, and therefore yielding a generic and multi-purpose architecture for application specific industrial settings. Our study overall conveys a constructive message: exhaustive experiments on several image classification benchmarks: CIFAR-10, STL-10, CIFAR-100, and ImageNet-100, demonstrates that the proposed training strategy provides a dense and corresponding sparse sub-network that achieves comparable (on-par) performance compared with the vanilla self-supervised setting, but at a significant reduction in computation in terms of FLOPs under a range of target budgets.", "authors": "Tarun Krishna, Ayush K Rai, Alexandru Drimbarean, Alan F Smeaton, Kevin McGuinness, Noel E O'Connor", "first_author": "Noel E O'Connor", "first_end_author": "Tarun Krishna; Noel E O'Connor", "publish_time": "2023-01-22 17:12:58+00:00", "update_time": "2023-01-22 17:12:58+00:00"}, {"id": "2212.14258", "url": "http://arxiv.org/abs/2212.14258v2", "pdf_url": "http://arxiv.org/pdf/2212.14258v2", "title": "HIER: Metric Learning Beyond Class Labels via Hierarchical   Regularization", "abs": "Supervision for metric learning has long been given in the form of equivalence between human-labeled classes. Although this type of supervision has been a basis of metric learning for decades, we argue that it hinders further advances of the field. In this regard, we propose a new regularization method, dubbed HIER, to discover the latent semantic hierarchy of training data, and to deploy the hierarchy to provide richer and more fine-grained supervision than inter-class separability induced by common metric learning losses. HIER achieved this goal with no annotation for the semantic hierarchy but by learning hierarchical proxies in hyperbolic spaces. The hierarchical proxies are learnable parameters, and each of them is trained to serve as an ancestor of a group of data or other proxies to approximate the semantic hierarchy among them. HIER deals with the proxies along with data in hyperbolic space since geometric properties of the space are well-suited to represent their hierarchical structure. The efficacy of HIER was evaluated on four standard benchmarks, where it consistently improved performance of conventional methods when integrated with them, and consequently achieved the best records, surpassing even the existing hyperbolic metric learning technique, in almost all settings.", "authors": "Sungyeon Kim, Boseung Jeong, Suha Kwak", "first_author": "Suha Kwak", "first_end_author": "Sungyeon Kim; Suha Kwak", "publish_time": "2022-12-29 11:05:47+00:00", "update_time": "2023-01-22 16:14:32+00:00"}, {"id": "2212.11696", "url": "http://arxiv.org/abs/2212.11696v2", "pdf_url": "http://arxiv.org/pdf/2212.11696v2", "title": "Reversible Column Networks", "abs": "We propose a new neural network design paradigm Reversible Column Network (RevCol). The main body of RevCol is composed of multiple copies of subnetworks, named columns respectively, between which multi-level reversible connections are employed. Such architectural scheme attributes RevCol very different behavior from conventional networks: during forward propagation, features in RevCol are learned to be gradually disentangled when passing through each column, whose total information is maintained rather than compressed or discarded as other network does. Our experiments suggest that CNN-style RevCol models can achieve very competitive performances on multiple computer vision tasks such as image classification, object detection and semantic segmentation, especially with large parameter budget and large dataset. For example, after ImageNet-22K pre-training, RevCol-XL obtains 88.2% ImageNet-1K accuracy. Given more pre-training data, our largest model RevCol-H reaches 90.0% on ImageNet-1K, 63.8% APbox on COCO detection minival set, 61.0% mIoU on ADE20k segmentation. To our knowledge, it is the best COCO detection and ADE20k segmentation result among pure (static) CNN models. Moreover, as a general macro architecture fashion, RevCol can also be introduced into transformers or other neural networks, which is demonstrated to improve the performances in both computer vision and NLP tasks. We release code and models at https://github.com/megvii-research/RevCol", "authors": "Yuxuan Cai, Yizhuang Zhou, Qi Han, Jianjian Sun, Xiangwen Kong, Jun Li, Xiangyu Zhang", "first_author": "Xiangyu Zhang", "first_end_author": "Yuxuan Cai; Xiangyu Zhang", "publish_time": "2022-12-22 13:37:59+00:00", "update_time": "2023-01-22 13:53:42+00:00"}, {"id": "2301.09123", "url": "http://arxiv.org/abs/2301.09123v1", "pdf_url": "http://arxiv.org/pdf/2301.09123v1", "title": "Face Generation from Textual Features using Conditionally Trained Inputs   to Generative Adversarial Networks", "abs": "Generative Networks have proved to be extremely effective in image restoration and reconstruction in the past few years. Generating faces from textual descriptions is one such application where the power of generative algorithms can be used. The task of generating faces can be useful for a number of applications such as finding missing persons, identifying criminals, etc. This paper discusses a novel approach to generating human faces given a textual description regarding the facial features. We use the power of state of the art natural language processing models to convert face descriptions into learnable latent vectors which are then fed to a generative adversarial network which generates faces corresponding to those features. While this paper focuses on high level descriptions of faces only, the same approach can be tailored to generate any image based on fine grained textual features.", "authors": "Sandeep Shinde, Tejas Pradhan, Aniket Ghorpade, Mihir Tale", "first_author": "Mihir Tale", "first_end_author": "Sandeep Shinde; Mihir Tale", "publish_time": "2023-01-22 13:27:12+00:00", "update_time": "2023-01-22 13:27:12+00:00"}, {"id": "2212.14613", "url": "http://arxiv.org/abs/2212.14613v7", "pdf_url": "http://arxiv.org/pdf/2212.14613v7", "title": "Delving into Semantic Scale Imbalance", "abs": "Model bias triggered by long-tailed data has been widely studied. However, measure based on the number of samples cannot explicate three phenomena simultaneously: (1) Given enough data, the classification performance gain is marginal with additional samples. (2) Classification performance decays precipitously as the number of training samples decreases when there is insufficient data. (3) Model trained on sample-balanced datasets still has different biases for different classes. In this work, we define and quantify the semantic scale of classes, which is used to measure the feature diversity of classes. It is exciting to find experimentally that there is a marginal effect of semantic scale, which perfectly describes the first two phenomena. Further, the quantitative measurement of semantic scale imbalance is proposed, which can accurately reflect model bias on multiple datasets, even on sample-balanced data, revealing a novel perspective for the study of class imbalance. Due to the prevalence of semantic scale imbalance, we propose semantic-scale-balanced learning, including a general loss improvement scheme and a dynamic re-weighting training framework that overcomes the challenge of calculating semantic scales in real-time during iterations. Comprehensive experiments show that dynamic semantic-scale-balanced learning consistently enables the model to perform superiorly on large-scale long-tailed and non-long-tailed natural and medical datasets, which is a good starting point for mitigating the prevalent but unnoticed model bias.", "authors": "Yanbiao Ma, Licheng Jiao, Fang Liu, Yuxin Li, Shuyuan Yang, Xu Liu", "first_author": "Xu Liu", "first_end_author": "Yanbiao Ma; Xu Liu", "publish_time": "2022-12-30 09:40:09+00:00", "update_time": "2023-01-22 13:12:48+00:00"}, {"id": "2301.09121", "url": "http://arxiv.org/abs/2301.09121v1", "pdf_url": "http://arxiv.org/pdf/2301.09121v1", "title": "Learning Open-vocabulary Semantic Segmentation Models From Natural   Language Supervision", "abs": "In this paper, we consider the problem of open-vocabulary semantic segmentation (OVS), which aims to segment objects of arbitrary classes instead of pre-defined, closed-set categories. The main contributions are as follows: First, we propose a transformer-based model for OVS, termed as OVSegmentor, which only exploits web-crawled image-text pairs for pre-training without using any mask annotations. OVSegmentor assembles the image pixels into a set of learnable group tokens via a slot-attention based binding module, and aligns the group tokens to the corresponding caption embedding. Second, we propose two proxy tasks for training, namely masked entity completion and cross-image mask consistency. The former aims to infer all masked entities in the caption given the group tokens, that enables the model to learn fine-grained alignment between visual groups and text entities. The latter enforces consistent mask predictions between images that contain shared entities, which encourages the model to learn visual invariance. Third, we construct CC4M dataset for pre-training by filtering CC12M with frequently appeared entities, which significantly improves training efficiency. Fourth, we perform zero-shot transfer on three benchmark datasets, PASCAL VOC 2012, PASCAL Context, and COCO Object. Our model achieves superior segmentation results over the state-of-the-art method by using only 3\\% data (4M vs 134M) for pre-training. Code and pre-trained models will be released for future research.", "authors": "Jilan Xu, Junlin Hou, Yuejie Zhang, Rui Feng, Yi Wang, Yu Qiao, Weidi Xie", "first_author": "Weidi Xie", "first_end_author": "Jilan Xu; Weidi Xie", "publish_time": "2023-01-22 13:10:05+00:00", "update_time": "2023-01-22 13:10:05+00:00"}, {"id": "2301.09120", "url": "http://arxiv.org/abs/2301.09120v1", "pdf_url": "http://arxiv.org/pdf/2301.09120v1", "title": "Causality-based Dual-Contrastive Learning Framework for Domain   Generalization", "abs": "Domain Generalization (DG) is essentially a sub-branch of out-of-distribution generalization, which trains models from multiple source domains and generalizes to unseen target domains. Recently, some domain generalization algorithms have emerged, but most of them were designed with non-transferable complex architecture. Additionally, contrastive learning has become a promising solution for simplicity and efficiency in DG. However, existing contrastive learning neglected domain shifts that caused severe model confusions. In this paper, we propose a Dual-Contrastive Learning (DCL) module on feature and prototype contrast. Moreover, we design a novel Causal Fusion Attention (CFA) module to fuse diverse views of a single image to attain prototype. Furthermore, we introduce a Similarity-based Hard-pair Mining (SHM) strategy to leverage information on diversity shift. Extensive experiments show that our method outperforms state-of-the-art algorithms on three DG datasets. The proposed algorithm can also serve as a plug-and-play module without usage of domain labels.", "authors": "Zining Chen, Weiqiu Wang, Zhicheng Zhao, Aidong Men", "first_author": "Aidong Men", "first_end_author": "Zining Chen; Aidong Men", "publish_time": "2023-01-22 13:07:24+00:00", "update_time": "2023-01-22 13:07:24+00:00"}, {"id": "2301.09091", "url": "http://arxiv.org/abs/2301.09091v1", "pdf_url": "http://arxiv.org/pdf/2301.09091v1", "title": "BallGAN: 3D-aware Image Synthesis with a Spherical Background", "abs": "3D-aware GANs aim to synthesize realistic 3D scenes such that they can be rendered in arbitrary perspectives to produce images. Although previous methods produce realistic images, they suffer from unstable training or degenerate solutions where the 3D geometry is unnatural. We hypothesize that the 3D geometry is underdetermined due to the insufficient constraint, i.e., being classified as real image to the discriminator is not enough. To solve this problem, we propose to approximate the background as a spherical surface and represent a scene as a union of the foreground placed in the sphere and the thin spherical background. It reduces the degree of freedom in the background field. Accordingly, we modify the volume rendering equation and incorporate dedicated constraints to design a novel 3D-aware GAN framework named BallGAN. BallGAN has multiple advantages as follows. 1) It produces more reasonable 3D geometry; the images of a scene across different viewpoints have better photometric consistency and fidelity than the state-of-the-art methods. 2) The training becomes much more stable. 3) The foreground can be separately rendered on top of different arbitrary backgrounds.", "authors": "Minjung Shin, Yunji Seo, Jeongmin Bae, Young Sun Choi, Hyunsu Kim, Hyeran Byun, Youngjung Uh", "first_author": "Youngjung Uh", "first_end_author": "Minjung Shin; Youngjung Uh", "publish_time": "2023-01-22 10:17:02+00:00", "update_time": "2023-01-22 10:17:02+00:00"}, {"id": "2209.12202", "url": "http://arxiv.org/abs/2209.12202v6", "pdf_url": "http://arxiv.org/pdf/2209.12202v6", "title": "Multimodal Exponentially Modified Gaussian Oscillators", "abs": "Acoustic modeling serves audio processing tasks such as de-noising, data reconstruction, model-based testing and classification. Previous work dealt with signal parameterization of wave envelopes either by multiple Gaussian distributions or a single asymmetric Gaussian curve, which both fall short in representing super-imposed echoes sufficiently well. This study presents a three-stage Multimodal Exponentially Modified Gaussian (MEMG) model with an optional oscillating term that regards captured echoes as a superposition of univariate probability distributions in the temporal domain. With this, synthetic ultrasound signals suffering from artifacts can be fully recovered, which is backed by quantitative assessment. Real data experimentation is carried out to demonstrate the classification capability of the acquired features with object reflections being detected at different points in time. The code is available at https://github.com/hahnec/multimodal_emg.", "authors": "Christopher Hahne", "first_author": "Christopher Hahne", "first_end_author": "Christopher Hahne; Christopher Hahne", "publish_time": "2022-09-25 11:48:09+00:00", "update_time": "2023-01-22 10:16:42+00:00"}, {"id": "2301.09077", "url": "http://arxiv.org/abs/2301.09077v1", "pdf_url": "http://arxiv.org/pdf/2301.09077v1", "title": "Bidirectional Propagation for Cross-Modal 3D Object Detection", "abs": "Recent works have revealed the superiority of feature-level fusion for cross-modal 3D object detection, where fine-grained feature propagation from 2D image pixels to 3D LiDAR points has been widely adopted for performance improvement. Still, the potential of heterogeneous feature propagation between 2D and 3D domains has not been fully explored. In this paper, in contrast to existing pixel-to-point feature propagation, we investigate an opposite point-to-pixel direction, allowing point-wise features to flow inversely into the 2D image branch. Thus, when jointly optimizing the 2D and 3D streams, the gradients back-propagated from the 2D image branch can boost the representation ability of the 3D backbone network working on LiDAR point clouds. Then, combining pixel-to-point and point-to-pixel information flow mechanisms, we construct an bidirectional feature propagation framework, dubbed BiProDet. In addition to the architectural design, we also propose normalized local coordinates map estimation, a new 2D auxiliary task for the training of the 2D image branch, which facilitates learning local spatial-aware features from the image modality and implicitly enhances the overall 3D detection performance. Extensive experiments and ablation studies validate the effectiveness of our method. Notably, we rank $\\mathbf{1^{\\mathrm{st}}}$ on the highly competitive KITTI benchmark on the cyclist class by the time of submission. The source code is available at https://github.com/Eaphan/BiProDet.", "authors": "Yifan Zhang, Qijian Zhang, Junhui Hou, Yixuan Yuan, Guoliang Xing", "first_author": "Guoliang Xing", "first_end_author": "Yifan Zhang; Guoliang Xing", "publish_time": "2023-01-22 08:26:58+00:00", "update_time": "2023-01-22 08:26:58+00:00"}, {"id": "2301.09071", "url": "http://arxiv.org/abs/2301.09071v1", "pdf_url": "http://arxiv.org/pdf/2301.09071v1", "title": "Variational Cross-Graph Reasoning and Adaptive Structured Semantics   Learning for Compositional Temporal Grounding", "abs": "Temporal grounding is the task of locating a specific segment from an untrimmed video according to a query sentence. This task has achieved significant momentum in the computer vision community as it enables activity grounding beyond pre-defined activity classes by utilizing the semantic diversity of natural language descriptions. The semantic diversity is rooted in the principle of compositionality in linguistics, where novel semantics can be systematically described by combining known words in novel ways (compositional generalization). However, existing temporal grounding datasets are not carefully designed to evaluate the compositional generalizability. To systematically benchmark the compositional generalizability of temporal grounding models, we introduce a new Compositional Temporal Grounding task and construct two new dataset splits, i.e., Charades-CG and ActivityNet-CG. When evaluating the state-of-the-art methods on our new dataset splits, we empirically find that they fail to generalize to queries with novel combinations of seen words. We argue that the inherent structured semantics inside the videos and language is the crucial factor to achieve compositional generalization. Based on this insight, we propose a variational cross-graph reasoning framework that explicitly decomposes video and language into hierarchical semantic graphs, respectively, and learns fine-grained semantic correspondence between the two graphs. Furthermore, we introduce a novel adaptive structured semantics learning approach to derive the structure-informed and domain-generalizable graph representations, which facilitate the fine-grained semantic correspondence reasoning between the two graphs. Extensive experiments validate the superior compositional generalizability of our approach.", "authors": "Juncheng Li, Siliang Tang, Linchao Zhu, Wenqiao Zhang, Yi Yang, Tat-Seng Chua, Fei Wu, Yueting Zhuang", "first_author": "Yueting Zhuang", "first_end_author": "Juncheng Li; Yueting Zhuang", "publish_time": "2023-01-22 08:02:23+00:00", "update_time": "2023-01-22 08:02:23+00:00"}, {"id": "2301.09063", "url": "http://arxiv.org/abs/2301.09063v1", "pdf_url": "http://arxiv.org/pdf/2301.09063v1", "title": "DASTSiam: Spatio-Temporal Fusion and Discriminative Augmentation for   Improved Siamese Tracking", "abs": "Tracking tasks based on deep neural networks have greatly improved with the emergence of Siamese trackers. However, the appearance of targets often changes during tracking, which can reduce the robustness of the tracker when facing challenges such as aspect ratio change, occlusion, and scale variation. In addition, cluttered backgrounds can lead to multiple high response points in the response map, leading to incorrect target positioning. In this paper, we introduce two transformer-based modules to improve Siamese tracking called DASTSiam: the spatio-temporal (ST) fusion module and the Discriminative Augmentation (DA) module. The ST module uses cross-attention based accumulation of historical cues to improve robustness against object appearance changes, while the DA module associates semantic information between the template and search region to improve target discrimination. Moreover, Modifying the label assignment of anchors also improves the reliability of the object location. Our modules can be used with all Siamese trackers and show improved performance on several public datasets through comparative and ablation experiments.", "authors": "Yucheng Huang, Eksan Firkat, Ziwang Xiao, Jihong Zhu, Askar Hamdulla", "first_author": "Askar Hamdulla", "first_end_author": "Yucheng Huang; Askar Hamdulla", "publish_time": "2023-01-22 06:23:53+00:00", "update_time": "2023-01-22 06:23:53+00:00"}, {"id": "2301.09060", "url": "http://arxiv.org/abs/2301.09060v1", "pdf_url": "http://arxiv.org/pdf/2301.09060v1", "title": "3D Reconstruction of Non-cooperative Resident Space Objects using   Instant NGP-accelerated NeRF and D-NeRF", "abs": "The proliferation of non-cooperative resident space objects (RSOs) in orbit has spurred the demand for active space debris removal, on-orbit servicing (OOS), classification, and functionality identification of these RSOs. Recent advances in computer vision have enabled high-definition 3D modeling of objects based on a set of 2D images captured from different viewing angles. This work adapts Instant NeRF and D-NeRF, variations of the neural radiance field (NeRF) algorithm to the problem of mapping RSOs in orbit for the purposes of functionality identification and assisting with OOS. The algorithms are evaluated for 3D reconstruction quality and hardware requirements using datasets of images of a spacecraft mock-up taken under two different lighting and motion conditions at the Orbital Robotic Interaction, On-Orbit Servicing and Navigation (ORION) Laboratory at Florida Institute of Technology. Instant NeRF is shown to learn high-fidelity 3D models with a computational cost that could feasibly be trained on on-board computers.", "authors": "Trupti Mahendrakar, Basilio Caruso, Van Minh Nguyen, Ryan T. White, Todd Steffen", "first_author": "Todd Steffen", "first_end_author": "Trupti Mahendrakar; Todd Steffen", "publish_time": "2023-01-22 05:26:08+00:00", "update_time": "2023-01-22 05:26:08+00:00"}, {"id": "2301.09059", "url": "http://arxiv.org/abs/2301.09059v1", "pdf_url": "http://arxiv.org/pdf/2301.09059v1", "title": "Autonomous Rendezvous with Non-cooperative Target Objects with Swarm   Chasers and Observers", "abs": "Space debris is on the rise due to the increasing demand for spacecraft for com-munication, navigation, and other applications. The Space Surveillance Network (SSN) tracks over 27,000 large pieces of debris and estimates the number of small, un-trackable fragments at over 1,00,000. To control the growth of debris, the for-mation of further debris must be reduced. Some solutions include deorbiting larger non-cooperative resident space objects (RSOs) or servicing satellites in or-bit. Both require rendezvous with RSOs, and the scale of the problem calls for autonomous missions. This paper introduces the Multipurpose Autonomous Ren-dezvous Vision-Integrated Navigation system (MARVIN) developed and tested at the ORION Facility at Florida Institution of Technology. MARVIN consists of two sub-systems: a machine vision-aided navigation system and an artificial po-tential field (APF) guidance algorithm which work together to command a swarm of chasers to safely rendezvous with the RSO. We present the MARVIN architec-ture and hardware-in-the-loop experiments demonstrating autonomous, collabo-rative swarm satellite operations successfully guiding three drones to rendezvous with a physical mockup of a non-cooperative satellite in motion.", "authors": "Trupti Mahendrakar, Steven Holmberg, Andrew Ekblad, Emma Conti, Ryan T. White, Markus Wilde, Isaac Silver", "first_author": "Isaac Silver", "first_end_author": "Trupti Mahendrakar; Isaac Silver", "publish_time": "2023-01-22 05:22:11+00:00", "update_time": "2023-01-22 05:22:11+00:00"}, {"id": "2301.09056", "url": "http://arxiv.org/abs/2301.09056v1", "pdf_url": "http://arxiv.org/pdf/2301.09056v1", "title": "Performance Study of YOLOv5 and Faster R-CNN for Autonomous Navigation   around Non-Cooperative Targets", "abs": "Autonomous navigation and path-planning around non-cooperative space objects is an enabling technology for on-orbit servicing and space debris removal systems. The navigation task includes the determination of target object motion, the identification of target object features suitable for grasping, and the identification of collision hazards and other keep-out zones. Given this knowledge, chaser spacecraft can be guided towards capture locations without damaging the target object or without unduly the operations of a servicing target by covering up solar arrays or communication antennas. One way to autonomously achieve target identification, characterization and feature recognition is by use of artificial intelligence algorithms. This paper discusses how the combination of cameras and machine learning algorithms can achieve the relative navigation task. The performance of two deep learning-based object detection algorithms, Faster Region-based Convolutional Neural Networks (R-CNN) and You Only Look Once (YOLOv5), is tested using experimental data obtained in formation flight simulations in the ORION Lab at Florida Institute of Technology. The simulation scenarios vary the yaw motion of the target object, the chaser approach trajectory, and the lighting conditions in order to test the algorithms in a wide range of realistic and performance limiting situations. The data analyzed include the mean average precision metrics in order to compare the performance of the object detectors. The paper discusses the path to implementing the feature recognition algorithms and towards integrating them into the spacecraft Guidance Navigation and Control system.", "authors": "Trupti Mahendrakar, Andrew Ekblad, Nathan Fischer, Ryan T. White, Markus Wilde, Brian Kish, Isaac Silver", "first_author": "Isaac Silver", "first_end_author": "Trupti Mahendrakar; Isaac Silver", "publish_time": "2023-01-22 04:53:38+00:00", "update_time": "2023-01-22 04:53:38+00:00"}, {"id": "2301.09055", "url": "http://arxiv.org/abs/2301.09055v1", "pdf_url": "http://arxiv.org/pdf/2301.09055v1", "title": "Resource-constrained FPGA Design for Satellite Component Feature   Extraction", "abs": "The effective use of computer vision and machine learning for on-orbit applications has been hampered by limited computing capabilities, and therefore limited performance. While embedded systems utilizing ARM processors have been shown to meet acceptable but low performance standards, the recent availability of larger space-grade field programmable gate arrays (FPGAs) show potential to exceed the performance of microcomputer systems. This work proposes use of neural network-based object detection algorithm that can be deployed on a comparably resource-constrained FPGA to automatically detect components of non-cooperative, satellites on orbit. Hardware-in-the-loop experiments were performed on the ORION Maneuver Kinematics Simulator at Florida Tech to compare the performance of the new model deployed on a small, resource-constrained FPGA to an equivalent algorithm on a microcomputer system. Results show the FPGA implementation increases the throughput and decreases latency while maintaining comparable accuracy. These findings suggest future missions should consider deploying computer vision algorithms on space-grade FPGAs.", "authors": "Andrew Ekblad, Trupti Mahendrakar, Ryan T. White, Markus Wilde, Isaac Silver, Brooke Wheeler", "first_author": "Brooke Wheeler", "first_end_author": "Andrew Ekblad; Brooke Wheeler", "publish_time": "2023-01-22 04:49:04+00:00", "update_time": "2023-01-22 04:49:04+00:00"}, {"id": "2301.09045", "url": "http://arxiv.org/abs/2301.09045v1", "pdf_url": "http://arxiv.org/pdf/2301.09045v1", "title": "Champion Solution for the WSDM2023 Toloka VQA Challenge", "abs": "In this report, we present our champion solution to the WSDM2023 Toloka Visual Question Answering (VQA) Challenge. Different from the common VQA and visual grounding (VG) tasks, this challenge involves a more complex scenario, i.e. inferring and locating the object implicitly specified by the given interrogative question. For this task, we leverage ViT-Adapter, a pre-training-free adapter network, to adapt multi-modal pre-trained Uni-Perceiver for better cross-modal localization. Our method ranks first on the leaderboard, achieving 77.5 and 76.347 IoU on public and private test sets, respectively. It shows that ViT-Adapter is also an effective paradigm for adapting the unified perception model to vision-language downstream tasks. Code and models will be released at https://github.com/czczup/ViT-Adapter/tree/main/wsdm2023.", "authors": "Shengyi Gao, Zhe Chen, Guo Chen, Wenhai Wang, Tong Lu", "first_author": "Tong Lu", "first_end_author": "Shengyi Gao; Tong Lu", "publish_time": "2023-01-22 03:14:31+00:00", "update_time": "2023-01-22 03:14:31+00:00"}, {"id": "2204.10704", "url": "http://arxiv.org/abs/2204.10704v2", "pdf_url": "http://arxiv.org/pdf/2204.10704v2", "title": "SUES-200: A Multi-height Multi-scene Cross-view Image Benchmark Across   Drone and Satellite", "abs": "Cross-view image matching aims to match images of the same target scene acquired from different platforms. With the rapid development of drone technology, cross-view matching by neural network models has been a widely accepted choice for drone position or navigation. However, existing public datasets do not include images obtained by drones at different heights, and the types of scenes are relatively homogeneous, which yields issues in assessing a model's capability to adapt to complex and changing scenes. In this end, we present a new cross-view dataset called SUES-200 to address these issues. SUES-200 contains 24120 images acquired by the drone at four different heights and corresponding satellite view images of the same target scene. To the best of our knowledge, SUES-200 is the first public dataset that considers the differences generated in aerial photography captured by drones flying at different heights. In addition, we developed an evaluation for efficient training, testing and evaluation of cross-view matching models, under which we comprehensively analyze the performance of nine architectures. Then, we propose a robust baseline model for use with SUES-200. Experimental results show that SUES-200 can help the model to learn highly discriminative features of the height of the drone.", "authors": "Runzhe Zhu, Ling Yin, Mingze Yang, Fei Wu, Yuncheng Yang, Wenbo Hu", "first_author": "Wenbo Hu", "first_end_author": "Runzhe Zhu; Wenbo Hu", "publish_time": "2022-04-22 13:49:52+00:00", "update_time": "2023-01-22 01:49:00+00:00"}, {"id": "2201.01415", "url": "http://arxiv.org/abs/2201.01415v3", "pdf_url": "http://arxiv.org/pdf/2201.01415v3", "title": "Problem-dependent attention and effort in neural networks with   application to image resolution and model selection", "abs": "This paper introduces a new ensemble-based approach to reduce the data and computation costs of accurate classification. When faced with a new test case, a low cost classifier is used first, only moving to a higher cost approach if the initial classifier does not have a high degree of confidence in its projection. This multi-stage strategy can be used with any set of classifiers and does not require additional training. The approach is first applied to reduce the amount of data required to classify test images; it is found to be effective for problems in which at least some fraction of cases can be correctly classified based upon coarser data than are typically used. For neural networks performing digit recognition, for example, the proposed approach reduces the number of bytes of data read by 60% to 85% with less than 5% reduction in accuracy. For the ImageNet data, the number of bytes read by the typical network is reduced by 20% with less than 5% reduction in accuracy -- and in some cases, the resource savings reach 40%. The second application is to reduce computational complexity, with simpler neural networks used for test cases that are easier to classify and complex networks used for more difficult cases. For classification both of digits and of ImageNet images, computation cost is reduced by as much as 82% to 89% with less than 5% reduction in accuracy. The results also show that, for situations in which computational cost is not a concern, calculating multiple models' projections and selecting the one from the most confident classifier can increase classification accuracy on ImageNet by as much as two percent over the best standalone classifier considered here.", "authors": "Chris Rohlfs", "first_author": "Chris Rohlfs", "first_end_author": "Chris Rohlfs; Chris Rohlfs", "publish_time": "2022-01-05 02:27:35+00:00", "update_time": "2023-01-21 22:01:14+00:00"}, {"id": "2301.09015", "url": "http://arxiv.org/abs/2301.09015v1", "pdf_url": "http://arxiv.org/pdf/2301.09015v1", "title": "E$^3$Pose: Energy-Efficient Edge-assisted Multi-camera System for   Multi-human 3D Pose Estimation", "abs": "Multi-human 3D pose estimation plays a key role in establishing a seamless connection between the real world and the virtual world. Recent efforts adopted a two-stage framework that first builds 2D pose estimations in multiple camera views from different perspectives and then synthesizes them into 3D poses. However, the focus has largely been on developing new computer vision algorithms on the offline video datasets without much consideration on the energy constraints in real-world systems with flexibly-deployed and battery-powered cameras. In this paper, we propose an energy-efficient edge-assisted multiple-camera system, dubbed E$^3$Pose, for real-time multi-human 3D pose estimation, based on the key idea of adaptive camera selection. Instead of always employing all available cameras to perform 2D pose estimations as in the existing works, E$^3$Pose selects only a subset of cameras depending on their camera view qualities in terms of occlusion and energy states in an adaptive manner, thereby reducing the energy consumption (which translates to extended battery lifetime) and improving the estimation accuracy. To achieve this goal, E$^3$Pose incorporates an attention-based LSTM to predict the occlusion information of each camera view and guide camera selection before cameras are selected to process the images of a scene, and runs a camera selection algorithm based on the Lyapunov optimization framework to make long-term adaptive selection decisions. We build a prototype of E$^3$Pose on a 5-camera testbed, demonstrate its feasibility and evaluate its performance. Our results show that a significant energy saving (up to 31.21%) can be achieved while maintaining a high 3D pose estimation accuracy comparable to state-of-the-art methods.", "authors": "Letian Zhang, Jie Xu", "first_author": "Jie Xu", "first_end_author": "Letian Zhang; Jie Xu", "publish_time": "2023-01-21 21:53:33+00:00", "update_time": "2023-01-21 21:53:33+00:00"}, {"id": "2301.09007", "url": "http://arxiv.org/abs/2301.09007v1", "pdf_url": "http://arxiv.org/pdf/2301.09007v1", "title": "MultiNet with Transformers: A Model for Cancer Diagnosis Using Images", "abs": "Cancer is a leading cause of death in many countries. An early diagnosis of cancer based on biomedical imaging ensures effective treatment and a better prognosis. However, biomedical imaging presents challenges to both clinical institutions and researchers. Physiological anomalies are often characterized by slight abnormalities in individual cells or tissues, making them difficult to detect visually. Traditionally, anomalies are diagnosed by radiologists and pathologists with extensive training. This procedure, however, demands the participation of professionals and incurs a substantial cost. The cost makes large-scale biological image classification impractical. In this study, we provide unique deep neural network designs for multiclass classification of medical images, in particular cancer images. We incorporated transformers into a multiclass framework to take advantage of data-gathering capability and perform more accurate classifications. We evaluated models on publicly accessible datasets using various measures to ensure the reliability of the models. Extensive assessment metrics suggest this method can be used for a multitude of classification tasks.", "authors": "Hosein Barzekar, Yash Patel, Ling Tong, Zeyun Yu", "first_author": "Zeyun Yu", "first_end_author": "Hosein Barzekar; Zeyun Yu", "publish_time": "2023-01-21 20:53:57+00:00", "update_time": "2023-01-21 20:53:57+00:00"}, {"id": "2212.07476", "url": "http://arxiv.org/abs/2212.07476v2", "pdf_url": "http://arxiv.org/pdf/2212.07476v2", "title": "The Infinite Index: Information Retrieval on Generative Text-To-Image   Models", "abs": "Conditional generative models such as DALL-E and Stable Diffusion generate images based on a user-defined text, the prompt. Finding and refining prompts that produce a desired image has become the art of prompt engineering. Generative models do not provide a built-in retrieval model for a user's information need expressed through prompts. In light of an extensive literature review, we reframe prompt engineering for generative models as interactive text-based retrieval on a novel kind of \"infinite index\". We apply these insights for the first time in a case study on image generation for game design with an expert. Finally, we envision how active learning may help to guide the retrieval of generated images.", "authors": "Niklas Deckers, Maik Fr\u00f6be, Johannes Kiesel, Gianluca Pandolfo, Christopher Schr\u00f6der, Benno Stein, Martin Potthast", "first_author": "Martin Potthast", "first_end_author": "Niklas Deckers; Martin Potthast", "publish_time": "2022-12-14 19:50:35+00:00", "update_time": "2023-01-21 18:16:14+00:00"}, {"id": "2111.05978", "url": "http://arxiv.org/abs/2111.05978v3", "pdf_url": "http://arxiv.org/pdf/2111.05978v3", "title": "SUPER-Net: Trustworthy Medical Image Segmentation with Uncertainty   Propagation in Encoder-Decoder Networks", "abs": "Deep Learning (DL) holds great promise in reshaping the healthcare industry owing to its precision, efficiency, and objectivity. However, the brittleness of DL models to noisy and out-of-distribution inputs is ailing their deployment in the clinic. Most models produce point estimates without further information about model uncertainty or confidence. This paper introduces a new Bayesian DL framework for uncertainty quantification in segmentation neural networks: SUPER-Net: trustworthy medical image Segmentation with Uncertainty Propagation in Encoder-decodeR Networks. SUPER-Net analytically propagates, using Taylor series approximations, the first two moments (mean and covariance) of the posterior distribution of the model parameters across the nonlinear layers. In particular, SUPER-Net simultaneously learns the mean and covariance without expensive post-hoc Monte Carlo sampling or model ensembling. The output consists of two simultaneous maps: the segmented image and its pixelwise uncertainty map, which corresponds to the covariance matrix of the predictive distribution. We conduct an extensive evaluation of SUPER-Net on medical image segmentation of Magnetic Resonances Imaging and Computed Tomography scans under various noisy and adversarial conditions. Our experiments on multiple benchmark datasets demonstrate that SUPER-Net is more robust to noise and adversarial attacks than state-of-the-art segmentation models. Moreover, the uncertainty map of the proposed SUPER-Net associates low confidence (or equivalently high uncertainty) to patches in the test input images that are corrupted with noise, artifacts, or adversarial attacks. Perhaps more importantly, the model exhibits the ability of self-assessment of its segmentation decisions, notably when making erroneous predictions due to noise or adversarial examples.", "authors": "Giuseppina Carannante, Dimah Dera, Nidhal C. Bouaynaya, Hassan M. Fathallah-Shaykh, Ghulam Rasool", "first_author": "Ghulam Rasool", "first_end_author": "Giuseppina Carannante; Ghulam Rasool", "publish_time": "2021-11-10 22:46:05+00:00", "update_time": "2023-01-21 17:26:07+00:00"}, {"id": "2301.08965", "url": "http://arxiv.org/abs/2301.08965v1", "pdf_url": "http://arxiv.org/pdf/2301.08965v1", "title": "Raw or Cooked? Object Detection on RAW Images", "abs": "Images fed to a deep neural network have in general undergone several handcrafted image signal processing (ISP) operations, all of which have been optimized to produce visually pleasing images. In this work, we investigate the hypothesis that the intermediate representation of visually pleasing images is sub-optimal for downstream computer vision tasks compared to the RAW image representation. We suggest that the operations of the ISP instead should be optimized towards the end task, by learning the parameters of the operations jointly during training. We extend previous works on this topic and propose a new learnable operation that enables an object detector to achieve superior performance when compared to both previous works and traditional RGB images. In experiments on the open PASCALRAW dataset, we empirically confirm our hypothesis.", "authors": "William Ljungbergh, Joakim Johnander, Christoffer Petersson, Michael Felsberg", "first_author": "Michael Felsberg", "first_end_author": "William Ljungbergh; Michael Felsberg", "publish_time": "2023-01-21 15:42:53+00:00", "update_time": "2023-01-21 15:42:53+00:00"}, {"id": "2209.12753", "url": "http://arxiv.org/abs/2209.12753v2", "pdf_url": "http://arxiv.org/pdf/2209.12753v2", "title": "On Investigating the Conservative Property of Score-Based Generative   Models", "abs": "Existing Score-based Generative Models (SGMs) can be categorized into constrained SGMs (CSGMs) or unconstrained SGMs (USGMs) according to their parameterization approaches. CSGMs model probability density functions as Boltzmann distributions, and assign their predictions as the negative gradients of some scalar-valued energy functions. On the other hand, USGMs employ flexible architectures capable of directly estimating scores without the need to explicitly model energy functions. In this paper, we demonstrate that the architectural constraints of CSGMs may limit their modeling ability. In addition, we show that USGMs' inability to preserve the property of conservativeness may lead to degraded sampling performance in practice. To address the above issues, we propose Quasi-Conservative Score-based Generative Models (QCSGMs) for keeping the advantages of both CSGMs and USGMs. Our theoretical derivations demonstrate that the training objective of QCSGMs can be efficiently integrated into the training processes by leveraging the Hutchinson trace estimator. In addition, our experimental results on the CIFAR-10, CIFAR-100, ImageNet, and SVHN datasets validate the effectiveness of QCSGMs. Finally, we justify the advantage of QCSGMs using an example of a one-layered autoencoder.", "authors": "Chen-Hao Chao, Wei-Fang Sun, Bo-Wun Cheng, Chun-Yi Lee", "first_author": "Chun-Yi Lee", "first_end_author": "Chen-Hao Chao; Chun-Yi Lee", "publish_time": "2022-09-26 15:00:18+00:00", "update_time": "2023-01-21 15:29:47+00:00"}, {"id": "2301.08959", "url": "http://arxiv.org/abs/2301.08959v1", "pdf_url": "http://arxiv.org/pdf/2301.08959v1", "title": "Successive Subspace Learning for Cardiac Disease Classification with   Two-phase Deformation Fields from Cine MRI", "abs": "Cardiac cine magnetic resonance imaging (MRI) has been used to characterize cardiovascular diseases (CVD), often providing a noninvasive phenotyping tool.~While recently flourished deep learning based approaches using cine MRI yield accurate characterization results, the performance is often degraded by small training samples. In addition, many deep learning models are deemed a ``black box,\" for which models remain largely elusive in how models yield a prediction and how reliable they are. To alleviate this, this work proposes a lightweight successive subspace learning (SSL) framework for CVD classification, based on an interpretable feedforward design, in conjunction with a cardiac atlas. Specifically, our hierarchical SSL model is based on (i) neighborhood voxel expansion, (ii) unsupervised subspace approximation, (iii) supervised regression, and (iv) multi-level feature integration. In addition, using two-phase 3D deformation fields, including end-diastolic and end-systolic phases, derived between the atlas and individual subjects as input offers objective means of assessing CVD, even with small training samples. We evaluate our framework on the ACDC2017 database, comprising one healthy group and four disease groups. Compared with 3D CNN-based approaches, our framework achieves superior classification performance with 140$\\times$ fewer parameters, which supports its potential value in clinical use.", "authors": "Xiaofeng Liu, Fangxu Xing, Hanna K. Gaggin, C. -C. Jay Kuo, Georges El Fakhri, Jonghye Woo", "first_author": "Jonghye Woo", "first_end_author": "Xiaofeng Liu; Jonghye Woo", "publish_time": "2023-01-21 15:00:59+00:00", "update_time": "2023-01-21 15:00:59+00:00"}, {"id": "2301.08957", "url": "http://arxiv.org/abs/2301.08957v1", "pdf_url": "http://arxiv.org/pdf/2301.08957v1", "title": "Slice Transformer and Self-supervised Learning for 6DoF Localization in   3D Point Cloud Maps", "abs": "Precise localization is critical for autonomous vehicles. We present a self-supervised learning method that employs Transformers for the first time for the task of outdoor localization using LiDAR data. We propose a pre-text task that reorganizes the slices of a $360^\\circ$ LiDAR scan to leverage its axial properties. Our model, called Slice Transformer, employs multi-head attention while systematically processing the slices. To the best of our knowledge, this is the first instance of leveraging multi-head attention for outdoor point clouds. We additionally introduce the Perth-WA dataset, which provides a large-scale LiDAR map of Perth city in Western Australia, covering $\\sim$4km$^2$ area. Localization annotations are provided for Perth-WA. The proposed localization method is thoroughly evaluated on Perth-WA and Appollo-SouthBay datasets. We also establish the efficacy of our self-supervised learning approach for the common downstream task of object classification using ModelNet40 and ScanNN datasets. The code and Perth-WA data will be publicly released.", "authors": "Muhammad Ibrahim, Naveed Akhtar, Saeed Anwar, Michael Wise, Ajmal Mian", "first_author": "Ajmal Mian", "first_end_author": "Muhammad Ibrahim; Ajmal Mian", "publish_time": "2023-01-21 14:33:02+00:00", "update_time": "2023-01-21 14:33:02+00:00"}, {"id": "2301.08951", "url": "http://arxiv.org/abs/2301.08951v1", "pdf_url": "http://arxiv.org/pdf/2301.08951v1", "title": "Time-Conditioned Generative Modeling of Object-Centric Representations   for Video Decomposition and Prediction", "abs": "When perceiving the world from multiple viewpoints, humans have the ability to reason about the complete objects in a compositional manner even when the object is completely occluded from partial viewpoints. Meanwhile, humans can imagine the novel views after observing multiple viewpoints. The remarkable recent advance in multi-view object-centric learning leaves some problems: 1) the partially or completely occluded shape of objects can not be well reconstructed. 2) the novel viewpoint prediction depends on expensive viewpoint annotations rather than implicit view rules. This makes the agent fail to perform like humans. In this paper, we introduce a time-conditioned generative model for videos. To reconstruct the complete shape of the object accurately, we enhance the disentanglement between different latent representations: view latent representations are jointly inferred based on the Transformer and then cooperate with the sequential extension of Slot Attention to learn object-centric representations. The model also achieves the new ability: Gaussian processes are employed as priors of view latent variables for generation and novel-view prediction without viewpoint annotations. Experiments on multiple specifically designed synthetic datasets have shown that the proposed model can 1) make the video decomposition, 2) reconstruct the complete shapes of objects, and 3) make the novel viewpoint prediction without viewpoint annotations.", "authors": "Chengmin Gao, Bin Li", "first_author": "Bin Li", "first_end_author": "Chengmin Gao; Bin Li", "publish_time": "2023-01-21 13:39:39+00:00", "update_time": "2023-01-21 13:39:39+00:00"}, {"id": "2301.08939", "url": "http://arxiv.org/abs/2301.08939v1", "pdf_url": "http://arxiv.org/pdf/2301.08939v1", "title": "Counterfactual Explanation and Instance-Generation using   Cycle-Consistent Generative Adversarial Networks", "abs": "The image-based diagnosis is now a vital aspect of modern automation assisted diagnosis. To enable models to produce pixel-level diagnosis, pixel-level ground-truth labels are essentially required. However, since it is often not straight forward to obtain the labels in many application domains such as in medical image, classification-based approaches have become the de facto standard to perform the diagnosis. Though they can identify class-salient regions, they may not be useful for diagnosis where capturing all of the evidences is important requirement. Alternatively, a counterfactual explanation (CX) aims at providing explanations using a casual reasoning process of form \"If X has not happend, Y would not heppend\". Existing CX approaches, however, use classifier to explain features that can change its predictions. Thus, they can only explain class-salient features, rather than entire object of interest. This hence motivates us to propose a novel CX strategy that is not reliant on image classification. This work is inspired from the recent developments in generative adversarial networks (GANs) based image-to-image domain translation, and leverages to translate an abnormal image to counterpart normal image (i.e. counterfactual instance CI) to find discrepancy maps between the two. Since it is generally not possible to obtain abnormal and normal image pairs, we leverage Cycle-Consistency principle (a.k.a CycleGAN) to perform the translation in unsupervised way. We formulate CX in terms of a discrepancy map that, when added from the abnormal image, will make it indistinguishable from the CI. We evaluate our method on three datasets including a synthetic, tuberculosis and BraTS dataset. All these experiments confirm the supremacy of propose method in generating accurate CX and CI.", "authors": "Tehseen Zia, Zeeshan Nisar, Shakeeb Murtaza", "first_author": "Shakeeb Murtaza", "first_end_author": "Tehseen Zia; Shakeeb Murtaza", "publish_time": "2023-01-21 11:14:34+00:00", "update_time": "2023-01-21 11:14:34+00:00"}, {"id": "2212.01222", "url": "http://arxiv.org/abs/2212.01222v5", "pdf_url": "http://arxiv.org/pdf/2212.01222v5", "title": "Evaluation of Explanation Methods of AI -- CNNs in Image Classification   Tasks with Reference-based and No-reference Metrics", "abs": "The most popular methods in AI-machine learning paradigm are mainly black boxes. This is why explanation of AI decisions is of emergency. Although dedicated explanation tools have been massively developed, the evaluation of their quality remains an open research question. In this paper, we generalize the methodologies of evaluation of post-hoc explainers of CNNs' decisions in visual classification tasks with reference and no-reference based metrics. We apply them on our previously developed explainers (FEM, MLFEM), and popular Grad-CAM. The reference-based metrics are Pearson correlation coefficient and Similarity computed between the explanation map and its ground truth represented by a Gaze Fixation Density Map obtained with a psycho-visual experiment. As a no-reference metric, we use stability metric, proposed by Alvarez-Melis and Jaakkola. We study its behaviour, consensus with reference-based metrics and show that in case of several kinds of degradation on input images, this metric is in agreement with reference-based ones. Therefore, it can be used for evaluation of the quality of explainers when the ground truth is not available.", "authors": "A. Zhukov, J. Benois-Pineau, R. Giot", "first_author": "R. Giot", "first_end_author": "A. Zhukov; R. Giot", "publish_time": "2022-12-02 14:55:31+00:00", "update_time": "2023-01-21 11:06:03+00:00"}, {"id": "2301.08930", "url": "http://arxiv.org/abs/2301.08930v1", "pdf_url": "http://arxiv.org/pdf/2301.08930v1", "title": "Dense RGB SLAM with Neural Implicit Maps", "abs": "There is an emerging trend of using neural implicit functions for map representation in Simultaneous Localization and Mapping (SLAM). Some pioneer works have achieved encouraging results on RGB-D SLAM. In this paper, we present a dense RGB SLAM method with neural implicit map representation. To reach this challenging goal without depth input, we introduce a hierarchical feature volume to facilitate the implicit map decoder. This design effectively fuses shape cues across different scales to facilitate map reconstruction. Our method simultaneously solves the camera motion and the neural implicit map by matching the rendered and input video frames. To facilitate optimization, we further propose a photometric warping loss in the spirit of multi-view stereo to better constrain the camera pose and scene geometry. We evaluate our method on commonly used benchmarks and compare it with modern RGB and RGB-D SLAM systems. Our method achieves favorable results than previous methods and even surpasses some recent RGB-D SLAM methods. Our source code will be publicly available.", "authors": "Heng Li, Xiaodong Gu, Weihao Yuan, Luwei Yang, Zilong Dong, Ping Tan", "first_author": "Ping Tan", "first_end_author": "Heng Li; Ping Tan", "publish_time": "2023-01-21 09:54:07+00:00", "update_time": "2023-01-21 09:54:07+00:00"}, {"id": "2301.08915", "url": "http://arxiv.org/abs/2301.08915v1", "pdf_url": "http://arxiv.org/pdf/2301.08915v1", "title": "Improving Deep Regression with Ordinal Entropy", "abs": "In computer vision, it is often observed that formulating regression problems as a classification task often yields better performance. We investigate this curious phenomenon and provide a derivation to show that classification, with the cross-entropy loss, outperforms regression with a mean squared error loss in its ability to learn high-entropy feature representations. Based on the analysis, we propose an ordinal entropy loss to encourage higher-entropy feature spaces while maintaining ordinal relationships to improve the performance of regression tasks. Experiments on synthetic and real-world regression tasks demonstrate the importance and benefits of increasing entropy for regression.", "authors": "Shihao Zhang, Linlin Yang, Michael Bi Mi, Xiaoxu Zheng, Angela Yao", "first_author": "Angela Yao", "first_end_author": "Shihao Zhang; Angela Yao", "publish_time": "2023-01-21 08:30:15+00:00", "update_time": "2023-01-21 08:30:15+00:00"}, {"id": "2301.08898", "url": "http://arxiv.org/abs/2301.08898v1", "pdf_url": "http://arxiv.org/pdf/2301.08898v1", "title": "Recurrent Contour-based Instance Segmentation with Progressive Learning", "abs": "Contour-based instance segmentation has been actively studied, thanks to its flexibility and elegance in processing visual objects within complex backgrounds. In this work, we propose a novel deep network architecture, i.e., PolySnake, for contour-based instance segmentation. Motivated by the classic Snake algorithm, the proposed PolySnake achieves superior and robust segmentation performance with an iterative and progressive contour refinement strategy. Technically, PolySnake introduces a recurrent update operator to estimate the object contour iteratively. It maintains a single estimate of the contour that is progressively deformed toward the object boundary. At each iteration, PolySnake builds a semantic-rich representation for the current contour and feeds it to the recurrent operator for further contour adjustment. Through the iterative refinements, the contour finally progressively converges to a stable status that tightly encloses the object instance. Moreover, with a compact design of the recurrent architecture, we ensure the running efficiency under multiple iterations. Extensive experiments are conducted to validate the merits of our method, and the results demonstrate that the proposed PolySnake outperforms the existing contour-based instance segmentation methods on several prevalent instance segmentation benchmarks. The codes and models are available at https://github.com/fh2019ustc/PolySnake.", "authors": "Hao Feng, Wengang Zhou, Yufei Yin, Jiajun Deng, Qi Sun, Houqiang Li", "first_author": "Houqiang Li", "first_end_author": "Hao Feng; Houqiang Li", "publish_time": "2023-01-21 05:34:29+00:00", "update_time": "2023-01-21 05:34:29+00:00"}, {"id": "2211.16570", "url": "http://arxiv.org/abs/2211.16570v2", "pdf_url": "http://arxiv.org/pdf/2211.16570v2", "title": "Performance Evaluation of Vanilla, Residual, and Dense 2D U-Net   Architectures for Skull Stripping of Augmented 3D T1-weighted MRI Head Scans", "abs": "Skull Stripping is a requisite preliminary step in most diagnostic neuroimaging applications. Manual Skull Stripping methods define the gold standard for the domain but are time-consuming and challenging to integrate into processing pipelines with a high number of data samples. Automated methods are an active area of research for head MRI segmentation, especially deep learning methods such as U-Net architecture implementations. This study compares Vanilla, Residual, and Dense 2D U-Net architectures for Skull Stripping. The Dense 2D U-Net architecture outperforms the Vanilla and Residual counterparts by achieving an accuracy of 99.75% on a test dataset. It is observed that dense interconnections in a U-Net encourage feature reuse across layers of the architecture and allow for shallower models with the strengths of a deeper network.", "authors": "Anway S. Pimpalkar, Rashmika K. Patole, Ketaki D. Kamble, Mahesh H. Shindikar", "first_author": "Mahesh H. Shindikar", "first_end_author": "Anway S. Pimpalkar; Mahesh H. Shindikar", "publish_time": "2022-11-29 20:11:11+00:00", "update_time": "2023-01-21 05:29:40+00:00"}, {"id": "2301.08888", "url": "http://arxiv.org/abs/2301.08888v1", "pdf_url": "http://arxiv.org/pdf/2301.08888v1", "title": "Pre-text Representation Transfer for Deep Learning with Limited   Imbalanced Data : Application to CT-based COVID-19 Detection", "abs": "Annotating medical images for disease detection is often tedious and expensive. Moreover, the available training samples for a given task are generally scarce and imbalanced. These conditions are not conducive for learning effective deep neural models. Hence, it is common to 'transfer' neural networks trained on natural images to the medical image domain. However, this paradigm lacks in performance due to the large domain gap between the natural and medical image data. To address that, we propose a novel concept of Pre-text Representation Transfer (PRT). In contrast to the conventional transfer learning, which fine-tunes a source model after replacing its classification layers, PRT retains the original classification layers and updates the representation layers through an unsupervised pre-text task. The task is performed with (original, not synthetic) medical images, without utilizing any annotations. This enables representation transfer with a large amount of training data. This high-fidelity representation transfer allows us to use the resulting model as a more effective feature extractor. Moreover, we can also subsequently perform the traditional transfer learning with this model. We devise a collaborative representation based classification layer for the case when we leverage the model as a feature extractor. We fuse the output of this layer with the predictions of a model induced with the traditional transfer learning performed over our pre-text transferred model. The utility of our technique for limited and imbalanced data classification problem is demonstrated with an extensive five-fold evaluation for three large-scale models, tested for five different class-imbalance ratios for CT based COVID-19 detection. Our results show a consistent gain over the conventional transfer learning with the proposed method.", "authors": "Fouzia Altaf, Syed M. S. Islam, Naeem K. Janjua, Naveed Akhtar", "first_author": "Naveed Akhtar", "first_end_author": "Fouzia Altaf; Naveed Akhtar", "publish_time": "2023-01-21 04:47:35+00:00", "update_time": "2023-01-21 04:47:35+00:00"}, {"id": "2112.09428", "url": "http://arxiv.org/abs/2112.09428v2", "pdf_url": "http://arxiv.org/pdf/2112.09428v2", "title": "Dynamics-aware Adversarial Attack of 3D Sparse Convolution Network", "abs": "In this paper, we investigate the dynamics-aware adversarial attack problem in deep neural networks. Most existing adversarial attack algorithms are designed under a basic assumption -- the network architecture is fixed throughout the attack process. However, this assumption does not hold for many recently proposed networks, e.g. 3D sparse convolution network, which contains input-dependent execution to improve computational efficiency. It results in a serious issue of lagged gradient, making the learned attack at the current step ineffective due to the architecture changes afterward. To address this issue, we propose a Leaded Gradient Method (LGM) and show the significant effects of the lagged gradient. More specifically, we re-formulate the gradients to be aware of the potential dynamic changes of network architectures, so that the learned attack better \"leads\" the next step than the dynamics-unaware methods when network architecture changes dynamically. Extensive experiments on various datasets show that our LGM achieves impressive performance on semantic segmentation and classification. Compared with the dynamic-unaware methods, LGM achieves about 20% lower mIoU averagely on the ScanNet and S3DIS datasets. LGM also outperforms the recent point cloud attacks.", "authors": "An Tao, Yueqi Duan, He Wang, Ziyi Wu, Pengliang Ji, Haowen Sun, Jie Zhou, Jiwen Lu", "first_author": "Jiwen Lu", "first_end_author": "An Tao; Jiwen Lu", "publish_time": "2021-12-17 10:53:35+00:00", "update_time": "2023-01-21 04:26:35+00:00"}, {"id": "2205.08738", "url": "http://arxiv.org/abs/2205.08738v2", "pdf_url": "http://arxiv.org/pdf/2205.08738v2", "title": "Passive Defense Against 3D Adversarial Point Clouds Through the Lens of   3D Steganalysis", "abs": "Nowadays, 3D data plays an indelible role in the computer vision field. However, extensive studies have proved that deep neural networks (DNNs) fed with 3D data, such as point clouds, are susceptible to adversarial examples, which aim to misguide DNNs and might bring immeasurable losses. Currently, 3D adversarial point clouds are chiefly generated in three fashions, i.e., point shifting, point adding, and point dropping. These point manipulations would modify geometrical properties and local correlations of benign point clouds more or less. Motivated by this basic fact, we propose to defend such adversarial examples with the aid of 3D steganalysis techniques. Specifically, we first introduce an adversarial attack and defense model adapted from the celebrated Prisoners' Problem in steganography to help us comprehend 3D adversarial attack and defense more generally. Then we rethink two significant but vague concepts in the field of adversarial example, namely, active defense and passive defense, from the perspective of steganalysis. Most importantly, we design a 3D adversarial point cloud detector through the lens of 3D steganalysis. Our detector is double-blind, that is to say, it does not rely on the exact knowledge of the adversarial attack means and victim models. To enable the detector to effectively detect malicious point clouds, we craft a 64-D discriminant feature set, including features related to first-order and second-order local descriptions of point clouds. To our knowledge, this work is the first to apply 3D steganalysis to 3D adversarial example defense. Extensive experimental results demonstrate that the proposed 3D adversarial point cloud detector can achieve good detection performance on multiple types of 3D adversarial point clouds.", "authors": "Jiahao Zhu", "first_author": "Jiahao Zhu", "first_end_author": "Jiahao Zhu; Jiahao Zhu", "publish_time": "2022-05-18 06:19:15+00:00", "update_time": "2023-01-21 04:21:18+00:00"}, {"id": "2301.08880", "url": "http://arxiv.org/abs/2301.08880v1", "pdf_url": "http://arxiv.org/pdf/2301.08880v1", "title": "A Large-scale Film Style Dataset for Learning Multi-frequency Driven   Film Enhancement", "abs": "Film, a classic image style, is culturally significant to the whole photographic industry since it marks the birth of photography. However, film photography is time-consuming and expensive, necessitating a more efficient method for collecting film-style photographs. Numerous datasets that have emerged in the field of image enhancement so far are not film-specific. In order to facilitate film-based image stylization research, we construct FilmSet, a large-scale and high-quality film style dataset. Our dataset includes three different film types and more than 5000 in-the-wild high resolution images. Inspired by the features of FilmSet images, we propose a novel framework called FilmNet based on Laplacian Pyramid for stylizing images across frequency bands and achieving film style outcomes. Experiments reveal that the performance of our model is superior than state-of-the-art techniques. Our dataset and code will be made publicly available.", "authors": "Xuhang Chen, Zinuo Li, Chi-Man Pun, Shuqiang Wang", "first_author": "Shuqiang Wang", "first_end_author": "Xuhang Chen; Shuqiang Wang", "publish_time": "2023-01-21 03:52:35+00:00", "update_time": "2023-01-21 03:52:35+00:00"}, {"id": "2301.08874", "url": "http://arxiv.org/abs/2301.08874v1", "pdf_url": "http://arxiv.org/pdf/2301.08874v1", "title": "Improving Accuracy of Zero-Shot Action Recognition with Handcrafted   Features", "abs": "With the development of machine learning, datasets for models are getting increasingly larger. This leads to increased data annotation costs and training time, which undoubtedly hinders the development of machine learning. To solve this problem, zero-shot learning is gaining considerable attention. With zero-shot learning, objects can be recognized or classified, even without having been seen before. Nevertheless, the accuracy of this method is still low, thus limiting its practical application. To solve this problem, we propose a video-text matching model, which can learn from handcrafted features. Our model can be used alone to predict the action classes and can also be added to any other model to improve its accuracy. Moreover, our model can be continuously optimized to improve its accuracy. We only need to manually annotate some features, which incurs some labor costs; in many situations, the costs are worth it. The results with UCF101 and HMDB51 show that our model achieves the best accuracy and also improves the accuracies of other models.", "authors": "Nan Wu, Hiroshi Kera, Kazuhiko Kawamoto", "first_author": "Kazuhiko Kawamoto", "first_end_author": "Nan Wu; Kazuhiko Kawamoto", "publish_time": "2023-01-21 03:41:07+00:00", "update_time": "2023-01-21 03:41:07+00:00"}, {"id": "2301.08868", "url": "http://arxiv.org/abs/2301.08868v1", "pdf_url": "http://arxiv.org/pdf/2301.08868v1", "title": "Dynamic MLP for MRI Reconstruction", "abs": "As convolutional neural networks (CNN) become the most successful reconstruction technique for accelerated Magnetic Resonance Imaging (MRI), CNN reaches its limit on image quality especially in sharpness. Further improvement on image quality often comes at massive computational costs, hindering their practicability in the clinic setting. MRI reconstruction is essentially a deconvolution problem, which demands long-distance information that is difficult to be captured by CNNs with small convolution kernels. The multi-layer perceptron (MLP) is able to model such long-distance information, but it restricts a fixed input size while the reconstruction of images in flexible resolutions is required in the clinic setting. In this paper, we proposed a hybrid CNN and MLP reconstruction strategy, featured by dynamic MLP (dMLP) that accepts arbitrary image sizes. Experiments were conducted using 3D multi-coil MRI. Our results suggested the proposed dMLP can improve image sharpness compared to its pure CNN counterpart, while costing minor additional GPU memory and computation time. We further compared the proposed dMLP with CNNs using large kernels and studied pure MLP-based reconstruction using a stack of 1D dMLPs, as well as its CNN counterpart using only 1D convolutions. We observed the enlarged receptive field has noticeably improved image quality, while simply using CNN with a large kernel leads to difficulties in training. Noticeably, the pure MLP-based method has been outperformed by CNN-involved methods, which matches the observations in other computer vision tasks for natural images.", "authors": "Chi Zhang, Eric Z. Chen, Xiao Chen, Yikang Liu, Terrence Chen, Shanhui Sun", "first_author": "Shanhui Sun", "first_end_author": "Chi Zhang; Shanhui Sun", "publish_time": "2023-01-21 02:58:51+00:00", "update_time": "2023-01-21 02:58:51+00:00"}, {"id": "2210.05861", "url": "http://arxiv.org/abs/2210.05861v2", "pdf_url": "http://arxiv.org/pdf/2210.05861v2", "title": "SlotFormer: Unsupervised Visual Dynamics Simulation with Object-Centric   Models", "abs": "Understanding dynamics from visual observations is a challenging problem that requires disentangling individual objects from the scene and learning their interactions. While recent object-centric models can successfully decompose a scene into objects, modeling their dynamics effectively still remains a challenge. We address this problem by introducing SlotFormer -- a Transformer-based autoregressive model operating on learned object-centric representations. Given a video clip, our approach reasons over object features to model spatio-temporal relationships and predicts accurate future object states. In this paper, we successfully apply SlotFormer to perform video prediction on datasets with complex object interactions. Moreover, the unsupervised SlotFormer's dynamics model can be used to improve the performance on supervised downstream tasks, such as Visual Question Answering (VQA), and goal-conditioned planning. Compared to past works on dynamics modeling, our method achieves significantly better long-term synthesis of object dynamics, while retaining high quality visual generation. Besides, SlotFormer enables VQA models to reason about the future without object-level labels, even outperforming counterparts that use ground-truth annotations. Finally, we show its ability to serve as a world model for model-based planning, which is competitive with methods designed specifically for such tasks.", "authors": "Ziyi Wu, Nikita Dvornik, Klaus Greff, Thomas Kipf, Animesh Garg", "first_author": "Animesh Garg", "first_end_author": "Ziyi Wu; Animesh Garg", "publish_time": "2022-10-12 01:53:58+00:00", "update_time": "2023-01-21 02:46:41+00:00"}, {"id": "2301.08849", "url": "http://arxiv.org/abs/2301.08849v1", "pdf_url": "http://arxiv.org/pdf/2301.08849v1", "title": "CADA-GAN: Context-Aware GAN with Data Augmentation", "abs": "Current child face generators are restricted by the limited size of the available datasets. In addition, feature selection can prove to be a significant challenge, especially due to the large amount of features that need to be trained for. To manage these problems, we proposed CADA-GAN, a \\textbf{C}ontext-\\textbf{A}ware GAN that allows optimal feature extraction, with added robustness from additional \\textbf{D}ata \\textbf{A}ugmentation. CADA-GAN is adapted from the popular StyleGAN2-Ada model, with attention on augmentation and segmentation of the parent images. The model has the lowest \\textit{Mean Squared Error Loss} (MSEloss) on latent feature representations and the generated child image is robust compared with the one that generated from baseline models.", "authors": "Sofie Daniels, Jiugeng Sun, Jiaqing Xie", "first_author": "Jiaqing Xie", "first_end_author": "Sofie Daniels; Jiaqing Xie", "publish_time": "2023-01-21 01:52:17+00:00", "update_time": "2023-01-21 01:52:17+00:00"}, {"id": "2301.08846", "url": "http://arxiv.org/abs/2301.08846v1", "pdf_url": "http://arxiv.org/pdf/2301.08846v1", "title": "Regeneration Learning: A Learning Paradigm for Data Generation", "abs": "Machine learning methods for conditional data generation usually build a mapping from source conditional data X to target data Y. The target Y (e.g., text, speech, music, image, video) is usually high-dimensional and complex, and contains information that does not exist in source data, which hinders effective and efficient learning on the source-target mapping. In this paper, we present a learning paradigm called regeneration learning for data generation, which first generates Y' (an abstraction/representation of Y) from X and then generates Y from Y'. During training, Y' is obtained from Y through either handcrafted rules or self-supervised learning and is used to learn X-->Y' and Y'-->Y. Regeneration learning extends the concept of representation learning to data generation tasks, and can be regarded as a counterpart of traditional representation learning, since 1) regeneration learning handles the abstraction (Y') of the target data Y for data generation while traditional representation learning handles the abstraction (X') of source data X for data understanding; 2) both the processes of Y'-->Y in regeneration learning and X-->X' in representation learning can be learned in a self-supervised way (e.g., pre-training); 3) both the mappings from X to Y' in regeneration learning and from X' to Y in representation learning are simpler than the direct mapping from X to Y. We show that regeneration learning can be a widely-used paradigm for data generation (e.g., text generation, speech recognition, speech synthesis, music composition, image generation, and video generation) and can provide valuable insights into developing data generation methods.", "authors": "Xu Tan, Tao Qin, Jiang Bian, Tie-Yan Liu, Yoshua Bengio", "first_author": "Yoshua Bengio", "first_end_author": "Xu Tan; Yoshua Bengio", "publish_time": "2023-01-21 01:33:34+00:00", "update_time": "2023-01-21 01:33:34+00:00"}, {"id": "2301.04746", "url": "http://arxiv.org/abs/2301.04746v3", "pdf_url": "http://arxiv.org/pdf/2301.04746v3", "title": "Switchable Lightweight Anti-symmetric Processing (SLAP) with CNN   Outspeeds Data Augmentation by Smaller Sample -- Application in Gomoku   Reinforcement Learning", "abs": "To replace data augmentation, this paper proposed a method called SLAP to intensify experience to speed up machine learning and reduce the sample size. SLAP is a model-independent protocol/function to produce the same output given different transformation variants. SLAP improved the convergence speed of convolutional neural network learning by 83% in the experiments with Gomoku game states, with only one eighth of the sample size compared with data augmentation. In reinforcement learning for Gomoku, using AlphaGo Zero/AlphaZero algorithm with data augmentation as baseline, SLAP reduced the number of training samples by a factor of 8 and achieved similar winning rate against the same evaluator, but it was not yet evident that it could speed up reinforcement learning. The benefits should at least apply to domains that are invariant to symmetry or certain transformations. As future work, SLAP may aid more explainable learning and transfer learning for domains that are not invariant to symmetry, as a small step towards artificial general intelligence.", "authors": "Chi-Hang Suen", "first_author": "Chi-Hang Suen", "first_end_author": "Chi-Hang Suen; Chi-Hang Suen", "publish_time": "2023-01-11 22:55:05+00:00", "update_time": "2023-01-21 01:06:34+00:00"}, {"id": "2301.07836", "url": "http://arxiv.org/abs/2301.07836v2", "pdf_url": "http://arxiv.org/pdf/2301.07836v2", "title": "Self Supervision Does Not Help Natural Language Supervision at Scale", "abs": "Self supervision and natural language supervision have emerged as two exciting ways to train general purpose image encoders which excel at a variety of downstream tasks. Recent works such as M3AE and SLIP have suggested that these approaches can be effectively combined, but most notably their results use small pre-training datasets (<50M samples) and don't effectively reflect the large-scale regime (>100M examples) that is commonly used for these approaches. Here we investigate whether a similar approach can be effective when trained with a much larger amount of data. We find that a combination of two state of the art approaches: masked auto-encoders, MAE and contrastive language image pre-training, CLIP provides a benefit over CLIP when trained on a corpus of 11.3M image-text pairs, but little to no benefit (as evaluated on a suite of common vision tasks) over CLIP when trained on a large corpus of 1.4B images. Our work provides some much needed clarity into the effectiveness (or lack thereof) of self supervision for large-scale image-text training.", "authors": "Floris Weers, Vaishaal Shankar, Angelos Katharopoulos, Yinfei Yang, Tom Gunter", "first_author": "Tom Gunter", "first_end_author": "Floris Weers; Tom Gunter", "publish_time": "2023-01-19 01:05:18+00:00", "update_time": "2023-01-20 22:26:21+00:00"}, {"id": "2301.08815", "url": "http://arxiv.org/abs/2301.08815v1", "pdf_url": "http://arxiv.org/pdf/2301.08815v1", "title": "DiffusionCT: Latent Diffusion Model for CT Image Standardization", "abs": "Computed tomography (CT) imaging is a widely used modality for early lung cancer diagnosis, treatment, and prognosis. Features extracted from CT images are now accepted to quantify spatial and temporal variations in tumor architecture and function. However, CT images are often acquired using scanners from different vendors with customized acquisition standards, resulting in significantly different texture features even for the same patient, posing a fundamental challenge to downstream studies. Existing CT image harmonization models rely on supervised or semi-supervised techniques, with limited performance. In this paper, we have proposed a diffusion-based CT image standardization model called DiffusionCT which works on latent space by mapping latent distribution into a standard distribution. DiffusionCT incorporates an Unet-based encoder-decoder and a diffusion model embedded in its bottleneck part. The Unet first trained without the diffusion model to learn the latent representation of the input data. The diffusion model is trained in the next training phase. All the trained models work together on image standardization. The encoded representation outputted from the Unet encoder passes through the diffusion model, and the diffusion model maps the distribution in to target standard image domain. Finally, the decode takes that transformed latent representation to synthesize a standardized image. The experimental results show that DiffusionCT significantly improves the performance of the standardization task.", "authors": "Md Selim, Jie Zhang, Michael A. Brooks, Ge Wang, Jin Chen", "first_author": "Jin Chen", "first_end_author": "Md Selim; Jin Chen", "publish_time": "2023-01-20 22:13:48+00:00", "update_time": "2023-01-20 22:13:48+00:00"}, {"id": "2212.01548", "url": "http://arxiv.org/abs/2212.01548v2", "pdf_url": "http://arxiv.org/pdf/2212.01548v2", "title": "FedRolex: Model-Heterogeneous Federated Learning with Rolling Sub-Model   Extraction", "abs": "Most cross-device federated learning (FL) studies focus on the model-homogeneous setting where the global server model and local client models are identical. However, such constraint not only excludes low-end clients who would otherwise make unique contributions to model training but also restrains clients from training large models due to on-device resource bottlenecks. In this work, we propose FedRolex, a partial training (PT)-based approach that enables model-heterogeneous FL and can train a global server model larger than the largest client model. At its core, FedRolex employs a rolling sub-model extraction scheme that allows different parts of the global server model to be evenly trained, which mitigates the client drift induced by the inconsistency between individual client models and server model architectures. We show that FedRolex outperforms state-of-the-art PT-based model-heterogeneous FL methods (e.g. Federated Dropout) and reduces the gap between model-heterogeneous and model-homogeneous FL, especially under the large-model large-dataset regime. In addition, we provide theoretical statistical analysis on its advantage over Federated Dropout and evaluate FedRolex on an emulated real-world device distribution to show that FedRolex can enhance the inclusiveness of FL and boost the performance of low-end devices that would otherwise not benefit from FL. Our code is available at: https://github.com/AIoT-MLSys-Lab/FedRolex", "authors": "Samiul Alam, Luyang Liu, Ming Yan, Mi Zhang", "first_author": "Mi Zhang", "first_end_author": "Samiul Alam; Mi Zhang", "publish_time": "2022-12-03 06:04:11+00:00", "update_time": "2023-01-20 21:53:11+00:00"}, {"id": "2208.11050", "url": "http://arxiv.org/abs/2208.11050v2", "pdf_url": "http://arxiv.org/pdf/2208.11050v2", "title": "Self-Trained Proposal Networks for the Open World", "abs": "Current state-of-the-art object proposal networks are trained with a closed-world assumption, meaning they learn to only detect objects of the training classes. These models fail to provide high recall in open-world environments where important novel objects may be encountered. While a handful of recent works attempt to tackle this problem, they fail to consider that the optimal behavior of a proposal network can vary significantly depending on the data and application. Our goal is to provide a flexible proposal solution that can be easily tuned to suit a variety of open-world settings. To this end, we design a Self-Trained Proposal Network (STPN) that leverages an adjustable hybrid architecture, a novel self-training procedure, and dynamic loss components to optimize the tradeoff between known and unknown object detection performance. To thoroughly evaluate our method, we devise several new challenges which invoke varying degrees of label bias by altering known class diversity and label count. We find that in every task, STPN easily outperforms existing baselines (e.g., RPN, OLN). Our method is also highly data efficient, surpassing baseline recall with a fraction of the labeled data.", "authors": "Matthew Inkawhich, Nathan Inkawhich, Hai Li, Yiran Chen", "first_author": "Yiran Chen", "first_end_author": "Matthew Inkawhich; Yiran Chen", "publish_time": "2022-08-23 15:57:19+00:00", "update_time": "2023-01-20 21:17:20+00:00"}, {"id": "2301.08802", "url": "http://arxiv.org/abs/2301.08802v1", "pdf_url": "http://arxiv.org/pdf/2301.08802v1", "title": "Impact of PCA-based preprocessing and different CNN structures on   deformable registration of sonograms", "abs": "Central venous catheters (CVC) are commonly inserted into the large veins of the neck, e.g. the internal jugular vein (IJV). CVC insertion may cause serious complications like misplacement into an artery or perforation of cervical vessels. Placing a CVC under sonographic guidance is an appropriate method to reduce such adverse events, if anatomical landmarks like venous and arterial vessels can be detected reliably. This task shall be solved by registration of patient individual images vs. an anatomically labelled reference image. In this work, a linear, affine transformation is performed on cervical sonograms, followed by a non-linear transformation to achieve a more precise registration. Voxelmorph (VM), a learning-based library for deformable image registration using a convolutional neural network (CNN) with U-Net structure was used for non-linear transformation. The impact of principal component analysis (PCA)-based pre-denoising of patient individual images, as well as the impact of modified net structures with differing complexities on registration results were examined visually and quantitatively, the latter using metrics for deformation and image similarity. Using the PCA-approximated cervical sonograms resulted in decreased mean deformation lengths between 18% and 66% compared to their original image counterparts, depending on net structure. In addition, reducing the number of convolutional layers led to improved image similarity with PCA images, while worsening in original images. Despite a large reduction of network parameters, no overall decrease in registration quality was observed, leading to the conclusion that the original net structure is oversized for the task at hand.", "authors": "Christian Schmidt, Heinrich Martin Overhoff", "first_author": "Heinrich Martin Overhoff", "first_end_author": "Christian Schmidt; Heinrich Martin Overhoff", "publish_time": "2023-01-20 21:01:39+00:00", "update_time": "2023-01-20 21:01:39+00:00"}, {"id": "2301.08800", "url": "http://arxiv.org/abs/2301.08800v1", "pdf_url": "http://arxiv.org/pdf/2301.08800v1", "title": "In-situ Water quality monitoring in Oil and Gas operations", "abs": "From agriculture to mining, to energy, surface water quality monitoring is an essential task. As oil and gas operators work to reduce the consumption of freshwater, it is increasingly important to actively manage fresh and non-fresh water resources over the long term. For large-scale monitoring, manual sampling at many sites has become too time-consuming and unsustainable, given the sheer number of dispersed ponds, small lakes, playas, and wetlands over a large area. Therefore, satellite-based environmental monitoring presents great potential. Many existing satellite-based monitoring studies utilize index-based methods to monitor large water bodies such as rivers and oceans. However, these existing methods fail when monitoring small ponds-the reflectance signal received from small water bodies is too weak to detect. To address this challenge, we propose a new Water Quality Enhanced Index (WQEI) Model, which is designed to enable users to determine contamination levels in water bodies with weak reflectance patterns. Our results show that 1) WQEI is a good indicator of water turbidity validated with 1200 water samples measured in the laboratory, and 2) by applying our method to commonly available satellite data (e.g. LandSat8), one can achieve high accuracy water quality monitoring efficiently in large regions. This provides a tool for operators to optimize the quality of water stored within surface storage ponds and increasing the readiness and availability of non-fresh water.", "authors": "Satish Kumar, Rui Kou, Henry Hill, Jake Lempges, Eric Qian, Vikram Jayaram", "first_author": "Vikram Jayaram", "first_end_author": "Satish Kumar; Vikram Jayaram", "publish_time": "2023-01-20 20:56:52+00:00", "update_time": "2023-01-20 20:56:52+00:00"}, {"id": "2301.08798", "url": "http://arxiv.org/abs/2301.08798v1", "pdf_url": "http://arxiv.org/pdf/2301.08798v1", "title": "DeepCOVID-Fuse: A Multi-modality Deep Learning Model Fusing Chest   X-Radiographs and Clinical Variables to Predict COVID-19 Risk Levels", "abs": "Propose: To present DeepCOVID-Fuse, a deep learning fusion model to predict risk levels in patients with confirmed coronavirus disease 2019 (COVID-19) and to evaluate the performance of pre-trained fusion models on full or partial combination of chest x-ray (CXRs) or chest radiograph and clinical variables.   Materials and Methods: The initial CXRs, clinical variables and outcomes (i.e., mortality, intubation, hospital length of stay, ICU admission) were collected from February 2020 to April 2020 with reverse-transcription polymerase chain reaction (RT-PCR) test results as the reference standard. The risk level was determined by the outcome. The fusion model was trained on 1657 patients (Age: 58.30 +/- 17.74; Female: 807) and validated on 428 patients (56.41 +/- 17.03; 190) from Northwestern Memorial HealthCare system and was tested on 439 patients (56.51 +/- 17.78; 205) from a single holdout hospital. Performance of pre-trained fusion models on full or partial modalities were compared on the test set using the DeLong test for the area under the receiver operating characteristic curve (AUC) and the McNemar test for accuracy, precision, recall and F1.   Results: The accuracy of DeepCOVID-Fuse trained on CXRs and clinical variables is 0.658, with an AUC of 0.842, which significantly outperformed (p < 0.05) models trained only on CXRs with an accuracy of 0.621 and AUC of 0.807 and only on clinical variables with an accuracy of 0.440 and AUC of 0.502. The pre-trained fusion model with only CXRs as input increases accuracy to 0.632 and AUC to 0.813 and with only clinical variables as input increases accuracy to 0.539 and AUC to 0.733.   Conclusion: The fusion model learns better feature representations across different modalities during training and achieves good outcome predictions even when only some of the modalities are used in testing.", "authors": "Yunan Wu, Amil Dravid, Ramsey Michael Wehbe, Aggelos K. Katsaggelos", "first_author": "Aggelos K. Katsaggelos", "first_end_author": "Yunan Wu; Aggelos K. Katsaggelos", "publish_time": "2023-01-20 20:54:25+00:00", "update_time": "2023-01-20 20:54:25+00:00"}, {"id": "2301.08794", "url": "http://arxiv.org/abs/2301.08794v1", "pdf_url": "http://arxiv.org/pdf/2301.08794v1", "title": "Robot Skill Learning Via Classical Robotics-Based Generated Datasets:   Advantages, Disadvantages, and Future Improvement", "abs": "Why do we not profit from our long-existing classical robotics knowledge and look for some alternative way for data collection? The situation ignoring all existing methods might be such a waste. This article argues that a dataset created using a classical robotics algorithm is a crucial part of future development. This developed classic algorithm has a perfect domain adaptation and generalization property, and most importantly, collecting datasets based on them is quite easy. It is well known that current robot skill-learning approaches perform exceptionally badly in the unseen domain, and their performance against adversarial attacks is quite limited as long as they do not have a very exclusive big dataset. Our experiment is the initial steps of using a dataset created by classical robotics codes. Our experiment investigated possible trajectory collection based on classical robotics. It addressed some advantages and disadvantages and pointed out other future development ideas.", "authors": "Batu Kaan Oezen", "first_author": "Batu Kaan Oezen", "first_end_author": "Batu Kaan Oezen; Batu Kaan Oezen", "publish_time": "2023-01-20 20:37:46+00:00", "update_time": "2023-01-20 20:37:46+00:00"}, {"id": "2301.08784", "url": "http://arxiv.org/abs/2301.08784v1", "pdf_url": "http://arxiv.org/pdf/2301.08784v1", "title": "Visual Semantic Relatedness Dataset for Image Captioning", "abs": "Modern image captioning system relies heavily on extracting knowledge from images to capture the concept of a static story. In this paper, we propose a textual visual context dataset for captioning, in which the publicly available dataset COCO Captions (Lin et al., 2014) has been extended with information about the scene (such as objects in the image). Since this information has a textual form, it can be used to leverage any NLP task, such as text similarity or semantic relation methods, into captioning systems, either as an end-to-end training strategy or a post-processing based approach.", "authors": "Ahmed Sabir, Francesc Moreno-Noguer, Llu\u00eds Padr\u00f3", "first_author": "Llu\u00eds Padr\u00f3", "first_end_author": "Ahmed Sabir; Llu\u00eds Padr\u00f3", "publish_time": "2023-01-20 20:04:35+00:00", "update_time": "2023-01-20 20:04:35+00:00"}, {"id": "2301.08783", "url": "http://arxiv.org/abs/2301.08783v1", "pdf_url": "http://arxiv.org/pdf/2301.08783v1", "title": "An Asynchronous Intensity Representation for Framed and Event Video   Sources", "abs": "Neuromorphic \"event\" cameras, designed to mimic the human vision system with asynchronous sensing, unlock a new realm of high-speed and high dynamic range applications. However, researchers often either revert to a framed representation of event data for applications, or build bespoke applications for a particular camera's event data type. To usher in the next era of video systems, accommodate new event camera designs, and explore the benefits to asynchronous video in classical applications, we argue that there is a need for an asynchronous, source-agnostic video representation. In this paper, we introduce a novel, asynchronous intensity representation for both framed and non-framed data sources. We show that our representation can increase intensity precision and greatly reduce the number of samples per pixel compared to grid-based representations. With framed sources, we demonstrate that by permitting a small amount of loss through the temporal averaging of similar pixel values, we can reduce our representational sample rate by more than half, while incurring a drop in VMAF quality score of only 4.5. We also demonstrate lower latency than the state-of-the-art method for fusing and transcoding framed and event camera data to an intensity representation, while maintaining $2000\\times$ the temporal resolution. We argue that our method provides the computational efficiency and temporal granularity necessary to build real-time intensity-based applications for event cameras.", "authors": "Andrew C. Freeman, Montek Singh, Ketan Mayer-Patel", "first_author": "Ketan Mayer-Patel", "first_end_author": "Andrew C. Freeman; Ketan Mayer-Patel", "publish_time": "2023-01-20 19:46:23+00:00", "update_time": "2023-01-20 19:46:23+00:00"}, {"id": "2301.08782", "url": "http://arxiv.org/abs/2301.08782v1", "pdf_url": "http://arxiv.org/pdf/2301.08782v1", "title": "Estimation of mitral valve hinge point coordinates -- deep neural net   for echocardiogram segmentation", "abs": "Cardiac image segmentation is a powerful tool in regard to diagnostics and treatment of cardiovascular diseases. Purely feature-based detection of anatomical structures like the mitral valve is a laborious task due to specifically required feature engineering and is especially challenging in echocardiograms, because of their inherently low contrast and blurry boundaries between some anatomical structures. With the publication of further annotated medical datasets and the increase in GPU processing power, deep learning-based methods in medical image segmentation became more feasible in the past years. We propose a fully automatic detection method for mitral valve hinge points, which uses a U-Net based deep neural net to segment cardiac chambers in echocardiograms in a first step, and subsequently extracts the mitral valve hinge points from the resulting segmentations in a second step. Results measured with this automatic detection method were compared to reference coordinate values, which with median absolute hinge point coordinate errors of 1.35 mm for the x- (15-85 percentile range: [0.3 mm; 3.15 mm]) and 0.75 mm for the y- coordinate (15-85 percentile range: [0.15 mm; 1.88 mm]).", "authors": "Christian Schmidt, Heinrich Martin Overhoff", "first_author": "Heinrich Martin Overhoff", "first_end_author": "Christian Schmidt; Heinrich Martin Overhoff", "publish_time": "2023-01-20 19:46:16+00:00", "update_time": "2023-01-20 19:46:16+00:00"}, {"id": "2301.08739", "url": "http://arxiv.org/abs/2301.08739v1", "pdf_url": "http://arxiv.org/pdf/2301.08739v1", "title": "FlatFormer: Flattened Window Attention for Efficient Point Cloud   Transformer", "abs": "Transformer, as an alternative to CNN, has been proven effective in many modalities (e.g., texts and images). For 3D point cloud transformers, existing efforts focus primarily on pushing their accuracy to the state-of-the-art level. However, their latency lags behind sparse convolution-based models (3x slower), hindering their usage in resource-constrained, latency-sensitive applications (such as autonomous driving). This inefficiency comes from point clouds' sparse and irregular nature, whereas transformers are designed for dense, regular workloads. This paper presents FlatFormer to close this latency gap by trading spatial proximity for better computational regularity. We first flatten the point cloud with window-based sorting and partition points into groups of equal sizes rather than windows of equal shapes. This effectively avoids expensive structuring and padding overheads. We then apply self-attention within groups to extract local features, alternate sorting axis to gather features from different directions, and shift windows to exchange features across groups. FlatFormer delivers state-of-the-art accuracy on Waymo Open Dataset with 4.6x speedup over (transformer-based) SST and 1.4x speedup over (sparse convolutional) CenterPoint. This is the first point cloud transformer that achieves real-time performance on edge GPUs and is faster than sparse convolutional methods while achieving on-par or even superior accuracy on large-scale benchmarks. Code to reproduce our results will be made publicly available.", "authors": "Zhijian Liu, Xinyu Yang, Haotian Tang, Shang Yang, Song Han", "first_author": "Song Han", "first_end_author": "Zhijian Liu; Song Han", "publish_time": "2023-01-20 18:59:57+00:00", "update_time": "2023-01-20 18:59:57+00:00"}, {"id": "2208.05834", "url": "http://arxiv.org/abs/2208.05834v2", "pdf_url": "http://arxiv.org/pdf/2208.05834v2", "title": "Joint reconstruction-segmentation on graphs", "abs": "Practical image segmentation tasks concern images which must be reconstructed from noisy, distorted, and/or incomplete observations. A recent approach for solving such tasks is to perform this reconstruction jointly with the segmentation, using each to guide the other. However, this work has so far employed relatively simple segmentation methods, such as the Chan--Vese algorithm. In this paper, we present a method for joint reconstruction-segmentation using graph-based segmentation methods, which have been seeing increasing recent interest. Complications arise due to the large size of the matrices involved, and we show how these complications can be managed. We then analyse the convergence properties of our scheme. Finally, we apply this scheme to distorted versions of ``two cows'' images familiar from previous graph-based segmentation literature, first to a highly noised version and second to a blurred version, achieving highly accurate segmentations in both cases. We compare these results to those obtained by sequential reconstruction-segmentation approaches, finding that our method competes with, or even outperforms, those approaches in terms of reconstruction and segmentation accuracy.", "authors": "Jeremy Budd, Yves van Gennip, Jonas Latz, Simone Parisotto, Carola-Bibiane Sch\u00f6nlieb", "first_author": "Carola-Bibiane Sch\u00f6nlieb", "first_end_author": "Jeremy Budd; Carola-Bibiane Sch\u00f6nlieb", "publish_time": "2022-08-11 14:01:38+00:00", "update_time": "2023-01-20 17:58:02+00:00"}, {"id": "2212.10772", "url": "http://arxiv.org/abs/2212.10772v3", "pdf_url": "http://arxiv.org/pdf/2212.10772v3", "title": "Low-Light Image and Video Enhancement: A Comprehensive Survey and Beyond", "abs": "This paper presents a comprehensive survey of low-light image and video enhancement. We begin with the challenging mixed over-/under-exposed images, which are under-performed by existing methods. To this end, we propose two variants of the SICE dataset named SICE_Grad and SICE_Mix. Next, we introduce Night Wenzhou, a large-scale, high-resolution video dataset, to address the issue of the lack of a low-light video dataset that discount the use of low-light image enhancement (LLIE) to videos. Our Night Wenzhou dataset is challenging since it consists of fast-moving aerial scenes and streetscapes with varying illuminations and degradation. We conduct extensive key technique analysis and experimental comparisons for representative LLIE approaches using these newly proposed datasets and the current benchmark datasets. Finally, we address unresolved issues and propose future research topics for the LLIE community. Our datasets are available at https://github.com/ShenZheng2000/LLIE_Survey.", "authors": "Shen Zheng, Yiling Ma, Jinqian Pan, Changjie Lu, Gaurav Gupta", "first_author": "Gaurav Gupta", "first_end_author": "Shen Zheng; Gaurav Gupta", "publish_time": "2022-12-21 05:08:37+00:00", "update_time": "2023-01-20 17:10:15+00:00"}, {"id": "2301.08669", "url": "http://arxiv.org/abs/2301.08669v1", "pdf_url": "http://arxiv.org/pdf/2301.08669v1", "title": "Holistically Explainable Vision Transformers", "abs": "Transformers increasingly dominate the machine learning landscape across many tasks and domains, which increases the importance for understanding their outputs. While their attention modules provide partial insight into their inner workings, the attention scores have been shown to be insufficient for explaining the models as a whole. To address this, we propose B-cos transformers, which inherently provide holistic explanations for their decisions. Specifically, we formulate each model component - such as the multi-layer perceptrons, attention layers, and the tokenisation module - to be dynamic linear, which allows us to faithfully summarise the entire transformer via a single linear transform. We apply our proposed design to Vision Transformers (ViTs) and show that the resulting models, dubbed Bcos-ViTs, are highly interpretable and perform competitively to baseline ViTs on ImageNet. Code will be made available soon.", "authors": "Moritz B\u00f6hle, Mario Fritz, Bernt Schiele", "first_author": "Bernt Schiele", "first_end_author": "Moritz B\u00f6hle; Bernt Schiele", "publish_time": "2023-01-20 16:45:34+00:00", "update_time": "2023-01-20 16:45:34+00:00"}, {"id": "2301.08664", "url": "http://arxiv.org/abs/2301.08664v1", "pdf_url": "http://arxiv.org/pdf/2301.08664v1", "title": "AccDecoder: Accelerated Decoding for Neural-enhanced Video Analytics", "abs": "The quality of the video stream is key to neural network-based video analytics. However, low-quality video is inevitably collected by existing surveillance systems because of poor quality cameras or over-compressed/pruned video streaming protocols, e.g., as a result of upstream bandwidth limit. To address this issue, existing studies use quality enhancers (e.g., neural super-resolution) to improve the quality of videos (e.g., resolution) and eventually ensure inference accuracy. Nevertheless, directly applying quality enhancers does not work in practice because it will introduce unacceptable latency. In this paper, we present AccDecoder, a novel accelerated decoder for real-time and neural-enhanced video analytics. AccDecoder can select a few frames adaptively via Deep Reinforcement Learning (DRL) to enhance the quality by neural super-resolution and then up-scale the unselected frames that reference them, which leads to 6-21% accuracy improvement. AccDecoder provides efficient inference capability via filtering important frames using DRL for DNN-based inference and reusing the results for the other frames via extracting the reference relationship among frames and blocks, which results in a latency reduction of 20-80% than baselines.", "authors": "Tingting Yuan, Liang Mi, Weijun Wang, Haipeng Dai, Xiaoming Fu", "first_author": "Xiaoming Fu", "first_end_author": "Tingting Yuan; Xiaoming Fu", "publish_time": "2023-01-20 16:30:44+00:00", "update_time": "2023-01-20 16:30:44+00:00"}, {"id": "2301.08654", "url": "http://arxiv.org/abs/2301.08654v1", "pdf_url": "http://arxiv.org/pdf/2301.08654v1", "title": "Automated extraction of capacitive coupling for quantum dot systems", "abs": "Gate-defined quantum dots (QDs) have appealing attributes as a quantum computing platform, however, near-term devices possess a range of possible imperfections that need to be accounted for during the tuning and operation of QD devices. One such problem is the capacitive cross-talk between the metallic gates that define and control QD qubits. A way to compensate for the capacitive cross-talk and enable targeted control of specific QDs independent of coupling is by the use of virtual gates. Here, we demonstrate a reliable automated capacitive coupling identification method that combines machine learning with traditional fitting to take advantage of the desirable properties of each. We also show how the cross-capacitance measurement may be used for the identification of spurious QDs sometimes formed during tuning experimental devices. Our systems can autonomously flag devices with spurious dots near the operating regime which is crucial information for reliable tuning to a regime suitable for qubit operations.", "authors": "Joshua Ziegler, Florian Luthi, Mick Ramsey, Felix Borjans, Guoji Zheng, Justyna P. Zwolak", "first_author": "Justyna P. Zwolak", "first_end_author": "Joshua Ziegler; Justyna P. Zwolak", "publish_time": "2023-01-20 16:03:30+00:00", "update_time": "2023-01-20 16:03:30+00:00"}, {"id": "2301.08647", "url": "http://arxiv.org/abs/2301.08647v1", "pdf_url": "http://arxiv.org/pdf/2301.08647v1", "title": "Image Memorability Prediction with Vision Transformers", "abs": "Behavioral studies have shown that the memorability of images is similar across groups of people, suggesting that memorability is a function of the intrinsic properties of images, and is unrelated to people's individual experiences and traits. Deep learning networks can be trained on such properties and be used to predict memorability in new data sets. Convolutional neural networks (CNN) have pioneered image memorability prediction, but more recently developed vision transformer (ViT) models may have the potential to yield even better predictions. In this paper, we present the ViTMem, a new memorability model based on ViT, and evaluate memorability predictions obtained by it with state-of-the-art CNN-derived models. Results showed that ViTMem performed equal to or better than state-of-the-art models on all data sets. Additional semantic level analyses revealed that ViTMem is particularly sensitive to the semantic content that drives memorability in images. We conclude that ViTMem provides a new step forward, and propose that ViT-derived models can replace CNNs for computational prediction of image memorability. Researchers, educators, advertisers, visual designers and other interested parties can leverage the model to improve the memorability of their image material.", "authors": "Thomas Hagen, Thomas Espeseth", "first_author": "Thomas Espeseth", "first_end_author": "Thomas Hagen; Thomas Espeseth", "publish_time": "2023-01-20 15:55:35+00:00", "update_time": "2023-01-20 15:55:35+00:00"}, {"id": "2203.14645", "url": "http://arxiv.org/abs/2203.14645v2", "pdf_url": "http://arxiv.org/pdf/2203.14645v2", "title": "REx: Data-Free Residual Quantization Error Expansion", "abs": "Deep neural networks (DNNs) are ubiquitous in computer vision and natural language processing, but suffer from high inference cost. This problem can be addressed by quantization, which consists in converting floating point operations into a lower bit-width format. With the growing concerns on privacy rights, we focus our efforts on data-free methods. However, such techniques suffer from their lack of adaptability to the target devices, as a hardware typically only support specific bit widths. Thus, to adapt to a variety of devices, a quantization method shall be flexible enough to find good accuracy v.s. speed trade-offs for every bit width and target device. To achieve this, we propose REx, a quantization method that leverages residual error expansion, along with group sparsity and an ensemble approximation for better parallelization. REx is backed off by strong theoretical guarantees and achieves superior performance on every benchmarked application (from vision to NLP tasks), architecture (ConvNets, transformers) and bit-width (from int8 to ternary quantization).", "authors": "Edouard Yvinec, Arnaud Dapgony, Matthieu Cord, Kevin Bailly", "first_author": "Kevin Bailly", "first_end_author": "Edouard Yvinec; Kevin Bailly", "publish_time": "2022-03-28 11:04:45+00:00", "update_time": "2023-01-20 15:03:41+00:00"}, {"id": "2206.06484", "url": "http://arxiv.org/abs/2206.06484v3", "pdf_url": "http://arxiv.org/pdf/2206.06484v3", "title": "On Image Segmentation With Noisy Labels: Characterization and Volume   Properties of the Optimal Solutions to Accuracy and Dice", "abs": "We study two of the most popular performance metrics in medical image segmentation, Accuracy and Dice, when the target labels are noisy. For both metrics, several statements related to characterization and volume properties of the set of optimal segmentations are proved, and associated experiments are provided. Our main insights are: (i) the volume of the solutions to both metrics may deviate significantly from the expected volume of the target, (ii) the volume of a solution to Accuracy is always less than or equal to the volume of a solution to Dice and (iii) the optimal solutions to both of these metrics coincide when the set of feasible segmentations is constrained to the set of segmentations with the volume equal to the expected volume of the target.", "authors": "Marcus Nordstr\u00f6m, Henrik Hult, Jonas S\u00f6derberg, Fredrik L\u00f6fman", "first_author": "Fredrik L\u00f6fman", "first_end_author": "Marcus Nordstr\u00f6m; Fredrik L\u00f6fman", "publish_time": "2022-06-13 21:30:29+00:00", "update_time": "2023-01-20 15:02:33+00:00"}, {"id": "2301.08605", "url": "http://arxiv.org/abs/2301.08605v1", "pdf_url": "http://arxiv.org/pdf/2301.08605v1", "title": "A Deep Learning Approach for SAR Tomographic Imaging of Forested Areas", "abs": "Synthetic aperture radar tomographic imaging reconstructs the three-dimensional reflectivity of a scene from a set of coherent acquisitions performed in an interferometric configuration. In forest areas, a large number of elements backscatter the radar signal within each resolution cell. To reconstruct the vertical reflectivity profile, state-of-the-art techniques perform a regularized inversion implemented in the form of iterative minimization algorithms. We show that light-weight neural networks can be trained to perform the tomographic inversion with a single feed-forward pass, leading to fast reconstructions that could better scale to the amount of data provided by the future BIOMASS mission. We train our encoder-decoder network using simulated data and validate our technique on real L-band and P-band data.", "authors": "Zo\u00e9 Berenger, Lo\u00efc Denis, Florence Tupin, Laurent Ferro-Famil, Yue Huang", "first_author": "Yue Huang", "first_end_author": "Zo\u00e9 Berenger; Yue Huang", "publish_time": "2023-01-20 14:34:03+00:00", "update_time": "2023-01-20 14:34:03+00:00"}, {"id": "2208.07360", "url": "http://arxiv.org/abs/2208.07360v2", "pdf_url": "http://arxiv.org/pdf/2208.07360v2", "title": "Evaluating the Evaluators: Which UDA validation methods are most   effective? Can they be improved?", "abs": "This paper compares and ranks 8 UDA validation methods. Validators estimate model accuracy, which makes them an essential component of any UDA train-test pipeline. We rank these validators to indicate which of them are most useful for the purpose of selecting optimal model checkpoints and hyperparameters. To the best of our knowledge, this large-scale benchmark study is the first of its kind in the UDA field. In addition, we propose three new validators that outperform all the existing checkpoint-based validators that we were able to find in the existing literature. Code is available at https://www.github.com/KevinMusgrave/powerful-benchmarker.", "authors": "Kevin Musgrave, Serge Belongie, Ser-Nam Lim", "first_author": "Ser-Nam Lim", "first_end_author": "Kevin Musgrave; Ser-Nam Lim", "publish_time": "2022-08-15 17:55:26+00:00", "update_time": "2023-01-20 14:13:08+00:00"}, {"id": "2301.08590", "url": "http://arxiv.org/abs/2301.08590v1", "pdf_url": "http://arxiv.org/pdf/2301.08590v1", "title": "Improving Sketch Colorization using Adversarial Segmentation Consistency", "abs": "We propose a new method for producing color images from sketches. Current solutions in sketch colorization either necessitate additional user instruction or are restricted to the \"paired\" translation strategy. We leverage semantic image segmentation from a general-purpose panoptic segmentation network to generate an additional adversarial loss function. The proposed loss function is compatible with any GAN model. Our method is not restricted to datasets with segmentation labels and can be applied to unpaired translation tasks as well. Using qualitative, and quantitative analysis, and based on a user study, we demonstrate the efficacy of our method on four distinct image datasets. On the FID metric, our model improves the baseline by up to 35 points. Our code, pretrained models, scripts to produce newly introduced datasets and corresponding sketch images are available at https://github.com/giddyyupp/AdvSegLoss.", "authors": "Samet Hicsonmez, Nermin Samet, Emre Akbas, Pinar Duygulu", "first_author": "Pinar Duygulu", "first_end_author": "Samet Hicsonmez; Pinar Duygulu", "publish_time": "2023-01-20 14:07:30+00:00", "update_time": "2023-01-20 14:07:30+00:00"}, {"id": "2205.13268", "url": "http://arxiv.org/abs/2205.13268v2", "pdf_url": "http://arxiv.org/pdf/2205.13268v2", "title": "MemeTector: Enforcing deep focus for meme detection", "abs": "Image memes and specifically their widely-known variation image macros, is a special new media type that combines text with images and is used in social media to playfully or subtly express humour, irony, sarcasm and even hate. It is important to accurately retrieve image memes from social media to better capture the cultural and social aspects of online phenomena and detect potential issues (hate-speech, disinformation). Essentially, the background image of an image macro is a regular image easily recognized as such by humans but cumbersome for the machine to do so due to feature map similarity with the complete image macro. Hence, accumulating suitable feature maps in such cases can lead to deep understanding of the notion of image memes. To this end, we propose a methodology, called Visual Part Utilization, that utilizes the visual part of image memes as instances of the regular image class and the initial image memes as instances of the image meme class to force the model to concentrate on the critical parts that characterize an image meme. Additionally, we employ a trainable attention mechanism on top of a standard ViT architecture to enhance the model's ability to focus on these critical parts and make the predictions interpretable. Several training and test scenarios involving web-scraped regular images of controlled text presence are considered for evaluating the model in terms of robustness and accuracy. The findings indicate that light visual part utilization combined with sufficient text presence during training provides the best and most robust model, surpassing state of the art. Source code and dataset are available at https://github.com/mever-team/memetector.", "authors": "Christos Koutlis, Manos Schinas, Symeon Papadopoulos", "first_author": "Symeon Papadopoulos", "first_end_author": "Christos Koutlis; Symeon Papadopoulos", "publish_time": "2022-05-26 10:50:29+00:00", "update_time": "2023-01-20 14:00:26+00:00"}, {"id": "2211.16191", "url": "http://arxiv.org/abs/2211.16191v2", "pdf_url": "http://arxiv.org/pdf/2211.16191v2", "title": "SgVA-CLIP: Semantic-guided Visual Adapting of Vision-Language Models for   Few-shot Image Classification", "abs": "Although significant progress has been made in few-shot learning, most of existing few-shot image classification methods require supervised pre-training on a large amount of samples of base classes, which limits their generalization ability in real world application. Recently, large-scale Vision-Language Pre-trained models (VLPs) have been gaining increasing attention in few-shot learning because they can provide a new paradigm for transferable visual representation learning with easily available text on the Web. However, the VLPs may neglect detailed visual information that is difficult to describe by language sentences, but important for learning an effective classifier to distinguish different images. To address the above problem, we propose a new framework, named Semantic-guided Visual Adapting (SgVA), which can effectively extend vision-language pre-trained models to produce discriminative adapted visual features by comprehensively using an implicit knowledge distillation, a vision-specific contrastive loss, and a cross-modal contrastive loss. The implicit knowledge distillation is designed to transfer the fine-grained cross-modal knowledge to guide the updating of the vision adapter. State-of-the-art results on 13 datasets demonstrate that the adapted visual features can well complement the cross-modal features to improve few-shot image classification.", "authors": "Fang Peng, Xiaoshan Yang, Linhui Xiao, Yaowei Wang, Changsheng Xu", "first_author": "Changsheng Xu", "first_end_author": "Fang Peng; Changsheng Xu", "publish_time": "2022-11-28 14:58:15+00:00", "update_time": "2023-01-20 13:56:39+00:00"}, {"id": "2301.08571", "url": "http://arxiv.org/abs/2301.08571v1", "pdf_url": "http://arxiv.org/pdf/2301.08571v1", "title": "Visual Writing Prompts: Character-Grounded Story Generation with Curated   Image Sequences", "abs": "Current work on image-based story generation suffers from the fact that the existing image sequence collections do not have coherent plots behind them. We improve visual story generation by producing a new image-grounded dataset, Visual Writing Prompts (VWP). VWP contains almost 2K selected sequences of movie shots, each including 5-10 images. The image sequences are aligned with a total of 12K stories which were collected via crowdsourcing given the image sequences and a set of grounded characters from the corresponding image sequence. Our new image sequence collection and filtering process has allowed us to obtain stories that are more coherent and have more narrativity compared to previous work. We also propose a character-based story generation model driven by coherence as a strong baseline. Evaluations show that our generated stories are more coherent, visually grounded, and have more narrativity than stories generated with the current state-of-the-art model.", "authors": "Xudong Hong, Asad Sayeed, Khushboo Mehra, Vera Demberg, Bernt Schiele", "first_author": "Bernt Schiele", "first_end_author": "Xudong Hong; Bernt Schiele", "publish_time": "2023-01-20 13:38:24+00:00", "update_time": "2023-01-20 13:38:24+00:00"}, {"id": "2212.11614", "url": "http://arxiv.org/abs/2212.11614v2", "pdf_url": "http://arxiv.org/pdf/2212.11614v2", "title": "Hybrid Quantum-Classical Generative Adversarial Network for High   Resolution Image Generation", "abs": "Quantum machine learning (QML) has received increasing attention due to its potential to outperform classical machine learning methods in problems pertaining classification and identification tasks. A subclass of QML methods is quantum generative adversarial networks (QGANs) which have been studied as a quantum counterpart of classical GANs widely used in image manipulation and generation tasks. The existing work on QGANs is still limited to small-scale proof-of-concept examples based on images with significant downscaling. Here we integrate classical and quantum techniques to propose a new hybrid quantum-classical GAN framework. We demonstrate its superior learning capabilities by generating $28 \\times 28$ pixels grey-scale images without dimensionality reduction or classical pre/post-processing on multiple classes of the standard MNIST and Fashion MNIST datasets, which achieves comparable results to classical frameworks with three orders of magnitude less trainable generator parameters. To gain further insight into the working of our hybrid approach, we systematically explore the impact of its parameter space by varying the number of qubits, the size of image patches, the number of layers in the generator, the shape of the patches and the choice of prior distribution. Our results show that increasing the quantum generator size generally improves the learning capability of the network. The developed framework provides a foundation for future design of QGANs with optimal parameter set tailored for complex image generation tasks.", "authors": "Shu Lok Tsang, Maxwell T. West, Sarah M. Erfani, Muhammad Usman", "first_author": "Muhammad Usman", "first_end_author": "Shu Lok Tsang; Muhammad Usman", "publish_time": "2022-12-22 11:18:35+00:00", "update_time": "2023-01-20 12:38:30+00:00"}, {"id": "2301.08534", "url": "http://arxiv.org/abs/2301.08534v1", "pdf_url": "http://arxiv.org/pdf/2301.08534v1", "title": "Prodromal Diagnosis of Lewy Body Diseases Based on the Assessment of   Graphomotor and Handwriting Difficulties", "abs": "To this date, studies focusing on the prodromal diagnosis of Lewy body diseases (LBDs) based on quantitative analysis of graphomotor and handwriting difficulties are missing. In this work, we enrolled 18 subjects diagnosed with possible or probable mild cognitive impairment with Lewy bodies (MCI-LB), 7 subjects having more than 50% probability of developing Parkinson's disease (PD), 21 subjects with both possible/probable MCI-LB and probability of PD > 50%, and 37 age- and gender-matched healthy controls (HC). Each participant performed three tasks: Archimedean spiral drawing (to quantify graphomotor difficulties), sentence writing task (to quantify handwriting difficulties), and pentagon copying test (to quantify cognitive decline). Next, we parameterized the acquired data by various temporal, kinematic, dynamic, spatial, and task-specific features. And finally, we trained classification models for each task separately as well as a model for their combination to estimate the predictive power of the features for the identification of LBDs. Using this approach we were able to identify prodromal LBDs with 74% accuracy and showed the promising potential of computerized objective and non-invasive diagnosis of LBDs based on the assessment of graphomotor and handwriting difficulties.", "authors": "Zoltan Galaz, Jiri Mekyska, Jan Mucha, Vojtech Zvoncak, Zdenek Smekal, Marcos Faundez-Zanuy, Lubos Brabenec, Ivona Moravkova, Irena Rektorova", "first_author": "Irena Rektorova", "first_end_author": "Zoltan Galaz; Irena Rektorova", "publish_time": "2023-01-20 12:30:28+00:00", "update_time": "2023-01-20 12:30:28+00:00"}, {"id": "2301.08529", "url": "http://arxiv.org/abs/2301.08529v1", "pdf_url": "http://arxiv.org/pdf/2301.08529v1", "title": "Exploration of Various Fractional Order Derivatives in Parkinson's   Disease Dysgraphia Analysis", "abs": "Parkinson's disease (PD) is a common neurodegenerative disorder with a prevalence rate estimated to 2.0% for people aged over 65 years. Cardinal motor symptoms of PD such as rigidity and bradykinesia affect the muscles involved in the handwriting process resulting in handwriting abnormalities called PD dysgraphia. Nowadays, online handwritten signal (signal with temporal information) acquired by the digitizing tablets is the most advanced approach of graphomotor difficulties analysis. Although the basic kinematic features were proved to effectively quantify the symptoms of PD dysgraphia, a recent research identified that the theory of fractional calculus can be used to improve the graphomotor difficulties analysis. Therefore, in this study, we follow up on our previous research, and we aim to explore the utilization of various approaches of fractional order derivative (FD) in the analysis of PD dysgraphia. For this purpose, we used the repetitive loops task from the Parkinson's disease handwriting database (PaHaW). Handwritten signals were parametrized by the kinematic features employing three FD approximations: Gr\\\"unwald-Letnikov's, Riemann-Liouville's, and Caputo's. Results of the correlation analysis revealed a significant relationship between the clinical state and the handwriting features based on the velocity. The extracted features by Caputo's FD approximation outperformed the rest of the analyzed FD approaches. This was also confirmed by the results of the classification analysis, where the best model trained by Caputo's handwriting features resulted in a balanced accuracy of 79.73% with a sensitivity of 83.78% and a specificity of 75.68%.", "authors": "Jan Mucha, Zoltan Galaz, Jiri Mekyska, Marcos Faundez-Zanuy, Vojtech Zvoncak, Zdenek Smekal, Lubos Brabenec, Irena Rektorova", "first_author": "Irena Rektorova", "first_end_author": "Jan Mucha; Irena Rektorova", "publish_time": "2023-01-20 12:18:05+00:00", "update_time": "2023-01-20 12:18:05+00:00"}, {"id": "2106.07075", "url": "http://arxiv.org/abs/2106.07075v5", "pdf_url": "http://arxiv.org/pdf/2106.07075v5", "title": "Revisiting consistency for semi-supervised semantic segmentation", "abs": "Semi-supervised learning an attractive technique in practical deployments of deep models since it relaxes the dependence on labeled data. It is especially important in the scope of dense prediction because pixel-level annotation requires significant effort. This paper considers semi-supervised algorithms that enforce consistent predictions over perturbed unlabeled inputs. We study the advantages of perturbing only one of the two model instances and preventing the backward pass through the unperturbed instance. We also propose a competitive perturbation model as a composition of geometric warp and photometric jittering. We experiment with efficient models due to their importance for real-time and low-power applications. Our experiments show clear advantages of (1) one-way consistency, (2) perturbing only the student branch, and (3) strong photometric and geometric perturbations. Our perturbation model outperforms recent work and most of the contribution comes from photometric component. Experiments with additional data from the large coarsely annotated subset of Cityscapes suggest that semi-supervised training can outperform supervised training with the coarse labels.", "authors": "Ivan Grubi\u0161i\u0107, Marin Or\u0161i\u0107, Sini\u0161a \u0160egvi\u0107", "first_author": "Sini\u0161a \u0160egvi\u0107", "first_end_author": "Ivan Grubi\u0161i\u0107; Sini\u0161a \u0160egvi\u0107", "publish_time": "2021-06-13 19:31:59+00:00", "update_time": "2023-01-20 10:52:30+00:00"}, {"id": "2208.08241", "url": "http://arxiv.org/abs/2208.08241v3", "pdf_url": "http://arxiv.org/pdf/2208.08241v3", "title": "ILLUME: Rationalizing Vision-Language Models through Human Interactions", "abs": "Bootstrapping from pre-trained language models has been proven to be an efficient approach for building vision-language models (VLM) for tasks such as image captioning or visual question answering. However, outputs of these models rarely align with user's rationales for specific answers. In order to improve this alignment and reinforce commonsense reasons, we propose a tuning paradigm based on human interactions with machine generated data. Our ILLUME executes the following loop: Given an image-question-answer prompt, the VLM samples multiple candidate rationales, and a human critic provides minimal feedback via preference selection, used for fine-tuning. This loop increases the training data and gradually carves out the VLM's rationalization capabilities that are aligned with human intend. Our exhaustive experiments demonstrate that ILLUME is competitive with standard supervised fine-tuning while using significantly fewer training data and only requiring minimal feedback.", "authors": "Manuel Brack, Patrick Schramowski, Bj\u00f6rn Deiseroth, Kristian Kersting", "first_author": "Kristian Kersting", "first_end_author": "Manuel Brack; Kristian Kersting", "publish_time": "2022-08-17 11:41:43+00:00", "update_time": "2023-01-20 10:38:03+00:00"}, {"id": "2212.14447", "url": "http://arxiv.org/abs/2212.14447v2", "pdf_url": "http://arxiv.org/pdf/2212.14447v2", "title": "A Theoretical Framework for AI Models Explainability", "abs": "EXplainable Artificial Intelligence (XAI) is a vibrant research topic in the artificial intelligence community, with growing interest across methods and domains. Much has been written about the subject, yet XAI still lacks shared terminology and a framework capable of providing structural soundness to explanations. In our work, we address these issues by proposing a novel definition of explanation that is a synthesis of what can be found in the literature. We recognize that explanations are not atomic but the combination of evidence stemming from the model and its input-output mapping, and the human interpretation of this evidence. Furthermore, we fit explanations into the properties of faithfulness (i.e., the explanation being a true description of the model's inner workings and decision-making process) and plausibility (i.e., how much the explanation looks convincing to the user). Using our proposed theoretical framework simplifies how these properties are operationalized and it provides new insight into common explanation methods that we analyze as case studies.", "authors": "Matteo Rizzo, Alberto Veneri, Andrea Albarelli, Claudio Lucchese, Cristina Conati", "first_author": "Cristina Conati", "first_end_author": "Matteo Rizzo; Cristina Conati", "publish_time": "2022-12-29 20:05:26+00:00", "update_time": "2023-01-20 10:01:31+00:00"}, {"id": "2301.08479", "url": "http://arxiv.org/abs/2301.08479v1", "pdf_url": "http://arxiv.org/pdf/2301.08479v1", "title": "Pneumonia Detection in Chest X-Ray Images : Handling Class Imbalance", "abs": "People all over the globe are affected by pneumonia but deaths due to it are highest in Sub-Saharan Asia and South Asia. In recent years, the overall incidence and mortality rate of pneumonia regardless of the utilization of effective vaccines and compelling antibiotics has escalated. Thus, pneumonia remains a disease that needs spry prevention and treatment. The widespread prevalence of pneumonia has caused the research community to come up with a framework that helps detect, diagnose and analyze diseases accurately and promptly. One of the major hurdles faced by the Artificial Intelligence (AI) research community is the lack of publicly available datasets for chest diseases, including pneumonia . Secondly, few of the available datasets are highly imbalanced (normal examples are over sampled, while samples with ailment are in severe minority) making the problem even more challenging. In this article we present a novel framework for the detection of pneumonia. The novelty of the proposed methodology lies in the tackling of class imbalance problem. The Generative Adversarial Network (GAN), specifically a combination of Deep Convolutional Generative Adversarial Network (DCGAN) and Wasserstein GAN gradient penalty (WGAN-GP) was applied on the minority class ``Pneumonia'' for augmentation, whereas Random Under-Sampling (RUS) was done on the majority class ``No Findings'' to deal with the imbalance problem. The ChestX-Ray8 dataset, one of the biggest datasets, is used to validate the performance of the proposed framework. The learning phase is completed using transfer learning on state-of-the-art deep learning models i.e. ResNet-50, Xception, and VGG-16. Results obtained exceed state-of-the-art.", "authors": "Wardah Ali, Eesha Qureshi, Omama Ahmed Farooqi, Rizwan Ahmed Khan", "first_author": "Rizwan Ahmed Khan", "first_end_author": "Wardah Ali; Rizwan Ahmed Khan", "publish_time": "2023-01-20 09:17:39+00:00", "update_time": "2023-01-20 09:17:39+00:00"}, {"id": "2108.02104", "url": "http://arxiv.org/abs/2108.02104v3", "pdf_url": "http://arxiv.org/pdf/2108.02104v3", "title": "Point Discriminative Learning for Data-efficient 3D Point Cloud Analysis", "abs": "3D point cloud analysis has drawn a lot of research attention due to its wide applications. However, collecting massive labelled 3D point cloud data is both time-consuming and labor-intensive. This calls for data-efficient learning methods. In this work we propose PointDisc, a point discriminative learning method to leverage self-supervisions for data-efficient 3D point cloud classification and segmentation. PointDisc imposes a novel point discrimination loss on the middle and global level features produced by the backbone network. This point discrimination loss enforces learned features to be consistent with points belonging to the corresponding local shape region and inconsistent with randomly sampled noisy points. We conduct extensive experiments on 3D object classification, 3D semantic and part segmentation, showing the benefits of PointDisc for data-efficient learning. Detailed analysis demonstrate that PointDisc learns unsupervised features that well capture local and global geometry.", "authors": "Fayao Liu, Guosheng Lin, Chuan-Sheng Foo, Chaitanya K. Joshi, Jie Lin", "first_author": "Jie Lin", "first_end_author": "Fayao Liu; Jie Lin", "publish_time": "2021-08-04 15:11:48+00:00", "update_time": "2023-01-20 08:46:35+00:00"}, {"id": "2211.09590", "url": "http://arxiv.org/abs/2211.09590v2", "pdf_url": "http://arxiv.org/pdf/2211.09590v2", "title": "Hypergraph Transformer for Skeleton-based Action Recognition", "abs": "Skeleton-based action recognition aims to predict human actions given human joint coordinates with skeletal interconnections. To model such off-grid data points and their co-occurrences, Transformer-based formulations would be a natural choice. However, Transformers still lag behind state-of-the-art methods using graph convolutional networks (GCNs). Transformers assume that the input is permutation-invariant and homogeneous (partially alleviated by positional encoding), which ignores an important characteristic of skeleton data, i.e., bone connectivity. Furthermore, each type of body joint has a clear physical meaning in human motion, i.e., motion retains an intrinsic relationship regardless of the joint coordinates, which is not explored in Transformers. In fact, certain re-occurring groups of body joints are often involved in specific actions, such as the subconscious hand movement for keeping balance. Vanilla attention is incapable of describing such underlying relations that are persistent and beyond pair-wise. In this work, we aim to exploit these unique aspects of skeleton data to close the performance gap between Transformers and GCNs. Specifically, we propose a new self-attention (SA) extension, named Hypergraph Self-Attention (HyperSA), to incorporate inherently higher-order relations into the model. The K-hop relative positional embeddings are also employed to take bone connectivity into account. We name the resulting model Hyperformer, and it achieves comparable or better performance w.r.t. accuracy and efficiency than state-of-the-art GCN architectures on NTU RGB+D, NTU RGB+D 120, and Northwestern-UCLA datasets. On the largest NTU RGB+D 120 dataset, the significantly improved performance reached by our Hyperformer demonstrates the underestimated potential of Transformer models in this field.", "authors": "Yuxuan Zhou, Chao Li, Zhi-Qi Cheng, Yifeng Geng, Xuansong Xie, Margret Keuper", "first_author": "Margret Keuper", "first_end_author": "Yuxuan Zhou; Margret Keuper", "publish_time": "2022-11-17 15:36:48+00:00", "update_time": "2023-01-20 07:38:35+00:00"}, {"id": "2301.08455", "url": "http://arxiv.org/abs/2301.08455v1", "pdf_url": "http://arxiv.org/pdf/2301.08455v1", "title": "Spatial Steerability of GANs via Self-Supervision from Discriminator", "abs": "Generative models make huge progress to the photorealistic image synthesis in recent years. To enable human to steer the image generation process and customize the output, many works explore the interpretable dimensions of the latent space in GANs. Existing methods edit the attributes of the output image such as orientation or color scheme by varying the latent code along certain directions. However, these methods usually require additional human annotations for each pretrained model, and they mostly focus on editing global attributes. In this work, we propose a self-supervised approach to improve the spatial steerability of GANs without searching for steerable directions in the latent space or requiring extra annotations. Specifically, we design randomly sampled Gaussian heatmaps to be encoded into the intermediate layers of generative models as spatial inductive bias. Along with training the GAN model from scratch, these heatmaps are being aligned with the emerging attention of the GAN's discriminator in a self-supervised learning manner. During inference, human users can intuitively interact with the spatial heatmaps to edit the output image, such as varying the scene layout or moving objects in the scene. Extensive experiments show that the proposed method not only enables spatial editing over human faces, animal faces, outdoor scenes, and complicated indoor scenes, but also brings improvement in synthesis quality.", "authors": "Jianyuan Wang, Ceyuan Yang, Yinghao Xu, Yujun Shen, Hongdong Li, Bolei Zhou", "first_author": "Bolei Zhou", "first_end_author": "Jianyuan Wang; Bolei Zhou", "publish_time": "2023-01-20 07:36:29+00:00", "update_time": "2023-01-20 07:36:29+00:00"}, {"id": "2204.10419", "url": "http://arxiv.org/abs/2204.10419v2", "pdf_url": "http://arxiv.org/pdf/2204.10419v2", "title": "Learning Sequential Latent Variable Models from Multimodal Time Series   Data", "abs": "Sequential modelling of high-dimensional data is an important problem that appears in many domains including model-based reinforcement learning and dynamics identification for control. Latent variable models applied to sequential data (i.e., latent dynamics models) have been shown to be a particularly effective probabilistic approach to solve this problem, especially when dealing with images. However, in many application areas (e.g., robotics), information from multiple sensing modalities is available -- existing latent dynamics methods have not yet been extended to effectively make use of such multimodal sequential data. Multimodal sensor streams can be correlated in a useful manner and often contain complementary information across modalities. In this work, we present a self-supervised generative modelling framework to jointly learn a probabilistic latent state representation of multimodal data and the respective dynamics. Using synthetic and real-world datasets from a multimodal robotic planar pushing task, we demonstrate that our approach leads to significant improvements in prediction and representation quality. Furthermore, we compare to the common learning baseline of concatenating each modality in the latent space and show that our principled probabilistic formulation performs better. Finally, despite being fully self-supervised, we demonstrate that our method is nearly as effective as an existing supervised approach that relies on ground truth labels.", "authors": "Oliver Limoyo, Trevor Ablett, Jonathan Kelly", "first_author": "Jonathan Kelly", "first_end_author": "Oliver Limoyo; Jonathan Kelly", "publish_time": "2022-04-21 21:59:24+00:00", "update_time": "2023-01-20 07:11:44+00:00"}, {"id": "2204.04457", "url": "http://arxiv.org/abs/2204.04457v3", "pdf_url": "http://arxiv.org/pdf/2204.04457v3", "title": "Refining time-space traffic diagrams: A multiple linear regression model", "abs": "A time-space traffic (TS) diagram, which presents traffic states in time-space cells with color, is an important traffic analysis and visualization tool. Despite its importance for transportation research and engineering, most TS diagrams that have already existed or are being produced are too coarse to exhibit detailed traffic dynamics due to the limitations of existing information technology and traffic infrastructure investment. To increase the resolution of a TS diagram and enable it to present ample traffic details, this paper introduces the TS diagram refinement problem and proposes a multiple linear regression-based model to solve the problem. Two tests, which attempt to increase the resolution of a TS diagram 4 and 16 times, are carried out to evaluate the performance of the proposed model. Data collected at different times, in different locations and even in different countries are employed to thoroughly evaluate the accuracy and transferability of the proposed model. Strict tests with diverse data show that the proposed model, despite its simplicity, is able to refine a TS diagram with promising accuracy and reliable transferability. The proposed refinement model will \"save\" widely existing TS diagrams from their blurry \"faces\" and enable TS diagrams to show more traffic details.", "authors": "Zhengbing He", "first_author": "Zhengbing He", "first_end_author": "Zhengbing He; Zhengbing He", "publish_time": "2022-04-09 12:02:50+00:00", "update_time": "2023-01-20 07:05:59+00:00"}, {"id": "2301.08448", "url": "http://arxiv.org/abs/2301.08448v1", "pdf_url": "http://arxiv.org/pdf/2301.08448v1", "title": "Source-free Subject Adaptation for EEG-based Visual Recognition", "abs": "This paper focuses on subject adaptation for EEG-based visual recognition. It aims at building a visual stimuli recognition system customized for the target subject whose EEG samples are limited, by transferring knowledge from abundant data of source subjects. Existing approaches consider the scenario that samples of source subjects are accessible during training. However, it is often infeasible and problematic to access personal biological data like EEG signals due to privacy issues. In this paper, we introduce a novel and practical problem setup, namely source-free subject adaptation, where the source subject data are unavailable and only the pre-trained model parameters are provided for subject adaptation. To tackle this challenging problem, we propose classifier-based data generation to simulate EEG samples from source subjects using classifier responses. Using the generated samples and target subject data, we perform subject-independent feature learning to exploit the common knowledge shared across different subjects. Notably, our framework is generalizable and can adopt any subject-independent learning method. In the experiments on the EEG-ImageNet40 benchmark, our model brings consistent improvements regardless of the choice of subject-independent learning. Also, our method shows promising performance, recording top-1 test accuracy of 74.6% under the 5-shot setting even without relying on source data. Our code can be found at https://github.com/DeepBCI/Deep-BCI/tree/master/1_Intelligent_BCI/Source_Free_Subject_Adaptation_for_EEG.", "authors": "Pilhyeon Lee, Seogkyu Jeon, Sunhee Hwang, Minjung Shin, Hyeran Byun", "first_author": "Hyeran Byun", "first_end_author": "Pilhyeon Lee; Hyeran Byun", "publish_time": "2023-01-20 07:01:01+00:00", "update_time": "2023-01-20 07:01:01+00:00"}, {"id": "2301.08443", "url": "http://arxiv.org/abs/2301.08443v1", "pdf_url": "http://arxiv.org/pdf/2301.08443v1", "title": "DIFAI: Diverse Facial Inpainting using StyleGAN Inversion", "abs": "Image inpainting is an old problem in computer vision that restores occluded regions and completes damaged images. In the case of facial image inpainting, most of the methods generate only one result for each masked image, even though there are other reasonable possibilities. To prevent any potential biases and unnatural constraints stemming from generating only one image, we propose a novel framework for diverse facial inpainting exploiting the embedding space of StyleGAN. Our framework employs pSp encoder and SeFa algorithm to identify semantic components of the StyleGAN embeddings and feed them into our proposed SPARN decoder that adopts region normalization for plausible inpainting. We demonstrate that our proposed method outperforms several state-of-the-art methods.", "authors": "Dongsik Yoon, Jeong-gi Kwak, Yuanming Li, David Han, Hanseok Ko", "first_author": "Hanseok Ko", "first_end_author": "Dongsik Yoon; Hanseok Ko", "publish_time": "2023-01-20 06:51:34+00:00", "update_time": "2023-01-20 06:51:34+00:00"}, {"id": "2301.08433", "url": "http://arxiv.org/abs/2301.08433v1", "pdf_url": "http://arxiv.org/pdf/2301.08433v1", "title": "Unsupervised Light Field Depth Estimation via Multi-view Feature   Matching with Occlusion Prediction", "abs": "Depth estimation from light field (LF) images is a fundamental step for some applications. Recently, learning-based methods have achieved higher accuracy and efficiency than the traditional methods. However, it is costly to obtain sufficient depth labels for supervised training. In this paper, we propose an unsupervised framework to estimate depth from LF images. First, we design a disparity estimation network (DispNet) with a coarse-to-fine structure to predict disparity maps from different view combinations by performing multi-view feature matching to learn the correspondences more effectively. As occlusions may cause the violation of photo-consistency, we design an occlusion prediction network (OccNet) to predict the occlusion maps, which are used as the element-wise weights of photometric loss to solve the occlusion issue and assist the disparity learning. With the disparity maps estimated by multiple input combinations, we propose a disparity fusion strategy based on the estimated errors with effective occlusion handling to obtain the final disparity map. Experimental results demonstrate that our method achieves superior performance on both the dense and sparse LF images, and also has better generalization ability to the real-world LF images.", "authors": "Shansi Zhang, Nan Meng, Edmund Y. Lam", "first_author": "Edmund Y. Lam", "first_end_author": "Shansi Zhang; Edmund Y. Lam", "publish_time": "2023-01-20 06:11:17+00:00", "update_time": "2023-01-20 06:11:17+00:00"}, {"id": "2204.14030", "url": "http://arxiv.org/abs/2204.14030v4", "pdf_url": "http://arxiv.org/pdf/2204.14030v4", "title": "Neural Implicit Representations for Physical Parameter Inference from a   Single Video", "abs": "Neural networks have recently been used to analyze diverse physical systems and to identify the underlying dynamics. While existing methods achieve impressive results, they are limited by their strong demand for training data and their weak generalization abilities to out-of-distribution data. To overcome these limitations, in this work we propose to combine neural implicit representations for appearance modeling with neural ordinary differential equations (ODEs) for modelling physical phenomena to obtain a dynamic scene representation that can be identified directly from visual observations. Our proposed model combines several unique advantages: (i) Contrary to existing approaches that require large training datasets, we are able to identify physical parameters from only a single video. (ii) The use of neural implicit representations enables the processing of high-resolution videos and the synthesis of photo-realistic images. (iii) The embedded neural ODE has a known parametric form that allows for the identification of interpretable physical parameters, and (iv) long-term prediction in state space. (v) Furthermore, the photo-realistic rendering of novel scenes with modified physical parameters becomes possible.", "authors": "Florian Hofherr, Lukas Koestler, Florian Bernard, Daniel Cremers", "first_author": "Daniel Cremers", "first_end_author": "Florian Hofherr; Daniel Cremers", "publish_time": "2022-04-29 11:55:35+00:00", "update_time": "2023-01-20 05:47:22+00:00"}, {"id": "2301.09416", "url": "http://arxiv.org/abs/2301.09416v1", "pdf_url": "http://arxiv.org/pdf/2301.09416v1", "title": "Towards Robust Video Instance Segmentation with Temporal-Aware   Transformer", "abs": "Most existing transformer based video instance segmentation methods extract per frame features independently, hence it is challenging to solve the appearance deformation problem. In this paper, we observe the temporal information is important as well and we propose TAFormer to aggregate spatio-temporal features both in transformer encoder and decoder. Specifically, in transformer encoder, we propose a novel spatio-temporal joint multi-scale deformable attention module which dynamically integrates the spatial and temporal information to obtain enriched spatio-temporal features. In transformer decoder, we introduce a temporal self-attention module to enhance the frame level box queries with the temporal relation. Moreover, TAFormer adopts an instance level contrastive loss to increase the discriminability of instance query embeddings. Therefore the tracking error caused by visually similar instances can be decreased. Experimental results show that TAFormer effectively leverages the spatial and temporal information to obtain context-aware feature representation and outperforms state-of-the-art methods.", "authors": "Zhenghao Zhang, Fangtao Shao, Zuozhuo Dai, Siyu Zhu", "first_author": "Siyu Zhu", "first_end_author": "Zhenghao Zhang; Siyu Zhu", "publish_time": "2023-01-20 05:22:16+00:00", "update_time": "2023-01-20 05:22:16+00:00"}, {"id": "2301.08414", "url": "http://arxiv.org/abs/2301.08414v1", "pdf_url": "http://arxiv.org/pdf/2301.08414v1", "title": "FG-Depth: Flow-Guided Unsupervised Monocular Depth Estimation", "abs": "The great potential of unsupervised monocular depth estimation has been demonstrated by many works due to low annotation cost and impressive accuracy comparable to supervised methods. To further improve the performance, recent works mainly focus on designing more complex network structures and exploiting extra supervised information, e.g., semantic segmentation. These methods optimize the models by exploiting the reconstructed relationship between the target and reference images in varying degrees. However, previous methods prove that this image reconstruction optimization is prone to get trapped in local minima. In this paper, our core idea is to guide the optimization with prior knowledge from pretrained Flow-Net. And we show that the bottleneck of unsupervised monocular depth estimation can be broken with our simple but effective framework named FG-Depth. In particular, we propose (i) a flow distillation loss to replace the typical photometric loss that limits the capacity of the model and (ii) a prior flow based mask to remove invalid pixels that bring the noise in training loss. Extensive experiments demonstrate the effectiveness of each component, and our approach achieves state-of-the-art results on both KITTI and NYU-Depth-v2 datasets.", "authors": "Junyu Zhu, Lina Liu, Yong Liu, Wanlong Li, Feng Wen, Hongbo Zhang", "first_author": "Hongbo Zhang", "first_end_author": "Junyu Zhu; Hongbo Zhang", "publish_time": "2023-01-20 04:02:13+00:00", "update_time": "2023-01-20 04:02:13+00:00"}, {"id": "2301.08413", "url": "http://arxiv.org/abs/2301.08413v1", "pdf_url": "http://arxiv.org/pdf/2301.08413v1", "title": "When Source-Free Domain Adaptation Meets Label Propagation", "abs": "Source-free domain adaptation, where only a pre-trained source model is used to adapt to the target distribution, is a more general approach to achieving domain adaptation. However, it can be challenging to capture the inherent structure of the target features accurately due to the lack of supervised information on the target domain. To tackle this problem, we propose a novel approach called Adaptive Local Transfer (ALT) that tries to achieve efficient feature clustering from the perspective of label propagation. ALT divides the target data into inner and outlier samples based on the adaptive threshold of the learning state, and applies a customized learning strategy to best fits the data property. Specifically, inner samples are utilized for learning intra-class structure thanks to their relatively well-clustered properties. The low-density outlier samples are regularized by input consistency to achieve high accuracy with respect to the ground truth labels. In this way, local clustering can be prevented from forming spurious clusters while effectively propagating label information among subpopulations. Empirical evidence demonstrates that ALT outperforms the state of the arts on three public benchmarks: Office-31, Office-Home, and VisDA.", "authors": "Chunwei Wu, Guitao Cao, Yan Li, Xidong Xi, Wenming Cao, Hong Wang", "first_author": "Hong Wang", "first_end_author": "Chunwei Wu; Hong Wang", "publish_time": "2023-01-20 03:39:35+00:00", "update_time": "2023-01-20 03:39:35+00:00"}, {"id": "2301.09420", "url": "http://arxiv.org/abs/2301.09420v1", "pdf_url": "http://arxiv.org/pdf/2301.09420v1", "title": "On Multi-Agent Deep Deterministic Policy Gradients and their   Explainability for SMARTS Environment", "abs": "Multi-Agent RL or MARL is one of the complex problems in Autonomous Driving literature that hampers the release of fully-autonomous vehicles today. Several simulators have been in iteration after their inception to mitigate the problem of complex scenarios with multiple agents in Autonomous Driving. One such simulator--SMARTS, discusses the importance of cooperative multi-agent learning. For this problem, we discuss two approaches--MAPPO and MADDPG, which are based on-policy and off-policy RL approaches. We compare our results with the state-of-the-art results for this challenge and discuss the potential areas of improvement while discussing the explainability of these approaches in conjunction with waypoints in the SMARTS environment.", "authors": "Ansh Mittal, Aditya Malte", "first_author": "Aditya Malte", "first_end_author": "Ansh Mittal; Aditya Malte", "publish_time": "2023-01-20 03:17:16+00:00", "update_time": "2023-01-20 03:17:16+00:00"}, {"id": "2301.01201", "url": "http://arxiv.org/abs/2301.01201v2", "pdf_url": "http://arxiv.org/pdf/2301.01201v2", "title": "Uncertainty in Real-Time Semantic Segmentation on Embedded Systems", "abs": "Application for semantic segmentation models in areas such as autonomous vehicles and human computer interaction require real-time predictive capabilities. The challenges of addressing real-time application is amplified by the need to operate on resource constrained hardware. Whilst development of real-time methods for these platforms has increased, these models are unable to sufficiently reason about uncertainty present. This paper addresses this by combining deep feature extraction from pre-trained models with Bayesian regression and moment propagation for uncertainty aware predictions. We demonstrate how the proposed method can yield meaningful uncertainty on embedded hardware in real-time whilst maintaining predictive performance.", "authors": "Ethan Goan, Clinton Fookes", "first_author": "Clinton Fookes", "first_end_author": "Ethan Goan; Clinton Fookes", "publish_time": "2022-12-20 07:32:12+00:00", "update_time": "2023-01-20 03:13:47+00:00"}, {"id": "2301.08408", "url": "http://arxiv.org/abs/2301.08408v1", "pdf_url": "http://arxiv.org/pdf/2301.08408v1", "title": "Identity masking effectiveness and gesture recognition: Effects of eye   enhancement in seeing through the mask", "abs": "Face identity masking algorithms developed in recent years aim to protect the privacy of people in video recordings. These algorithms are designed to interfere with identification, while preserving information about facial actions. An important challenge is to preserve subtle actions in the eye region, while obscuring the salient identity cues from the eyes. We evaluated the effectiveness of identity-masking algorithms based on Canny filters, applied with and without eye enhancement, for interfering with identification and preserving facial actions. In Experiments 1 and 2, we tested human participants' ability to match the facial identity of a driver in a low resolution video to a high resolution facial image. Results showed that both masking methods impaired identification, and that eye enhancement did not alter the effectiveness of the Canny filter mask. In Experiment 3, we tested action preservation and found that neither method interfered significantly with driver action perception. We conclude that relatively simple, filter-based masking algorithms, which are suitable for application to low quality video, can be used in privacy protection without compromising action perception.", "authors": "Madeline Rachow, Thomas Karnowski, Alice J. O'Toole", "first_author": "Alice J. O'Toole", "first_end_author": "Madeline Rachow; Alice J. O'Toole", "publish_time": "2023-01-20 03:10:19+00:00", "update_time": "2023-01-20 03:10:19+00:00"}, {"id": "2209.08162", "url": "http://arxiv.org/abs/2209.08162v2", "pdf_url": "http://arxiv.org/pdf/2209.08162v2", "title": "Uncertainty Quantification of Collaborative Detection for Self-Driving", "abs": "Sharing information between connected and autonomous vehicles (CAVs) fundamentally improves the performance of collaborative object detection for self-driving. However, CAVs still have uncertainties on object detection due to practical challenges, which will affect the later modules in self-driving such as planning and control. Hence, uncertainty quantification is crucial for safety-critical systems such as CAVs. Our work is the first to estimate the uncertainty of collaborative object detection. We propose a novel uncertainty quantification method, called Double-M Quantification, which tailors a moving block bootstrap (MBB) algorithm with direct modeling of the multivariant Gaussian distribution of each corner of the bounding box. Our method captures both the epistemic uncertainty and aleatoric uncertainty with one inference pass based on the offline Double-M training process. And it can be used with different collaborative object detectors. Through experiments on the comprehensive collaborative perception dataset, we show that our Double-M method achieves more than 4X improvement on uncertainty score and more than 3% accuracy improvement, compared with the state-of-the-art uncertainty quantification methods. Our code is public on https://coperception.github.io/double-m-quantification.", "authors": "Sanbao Su, Yiming Li, Sihong He, Songyang Han, Chen Feng, Caiwen Ding, Fei Miao", "first_author": "Fei Miao", "first_end_author": "Sanbao Su; Fei Miao", "publish_time": "2022-09-16 20:30:45+00:00", "update_time": "2023-01-20 02:57:46+00:00"}, {"id": "2201.09169", "url": "http://arxiv.org/abs/2201.09169v2", "pdf_url": "http://arxiv.org/pdf/2201.09169v2", "title": "Rich Action-semantic Consistent Knowledge for Early Action Prediction", "abs": "Early action prediction (EAP) aims to recognize human actions from a part of action execution in ongoing videos, which is an important task for many practical applications. Most prior works treat partial or full videos as a whole, ignoring rich action knowledge hidden in videos, i.e., semantic consistencies among different partial videos. In contrast, we partition original partial or full videos to form a new series of partial videos and mine the Action Semantic Consistent Knowledge (ASCK) among these new partial videos evolving in arbitrary progress levels. Moreover, a novel Rich Action-semantic Consistent Knowledge network (RACK) under the teacher-student framework is proposed for EAP. Firstly, we use a two-stream pre-trained model to extract features of videos. Secondly, we treat the RGB or flow features of the partial videos as nodes and their action semantic consistencies as edges. Next, we build a bi-directional semantic graph for the teacher network and a single-directional semantic graph for the student network to model rich ASCK among partial videos. The MSE and MMD losses are incorporated as our distillation loss to enrich the ASCK of partial videos from the teacher to the student network. Finally, we obtain the final prediction by summering the logits of different sub-networks and applying a softmax layer. Extensive experiments and ablative studies have been conducted, demonstrating the effectiveness of modeling rich ASCK for EAP. With the proposed RACK, we have achieved state-of-the-art performance on three benchmarks. The code will be released if the paper is accepted.", "authors": "Xiaoli Liu, Jianqin Yin, Di Guo", "first_author": "Di Guo", "first_end_author": "Xiaoli Liu; Di Guo", "publish_time": "2022-01-23 03:39:31+00:00", "update_time": "2023-01-20 02:57:36+00:00"}, {"id": "2301.09422", "url": "http://arxiv.org/abs/2301.09422v1", "pdf_url": "http://arxiv.org/pdf/2301.09422v1", "title": "HALOC: Hardware-Aware Automatic Low-Rank Compression for Compact Neural   Networks", "abs": "Low-rank compression is an important model compression strategy for obtaining compact neural network models. In general, because the rank values directly determine the model complexity and model accuracy, proper selection of layer-wise rank is very critical and desired. To date, though many low-rank compression approaches, either selecting the ranks in a manual or automatic way, have been proposed, they suffer from costly manual trials or unsatisfied compression performance. In addition, all of the existing works are not designed in a hardware-aware way, limiting the practical performance of the compressed models on real-world hardware platforms.   To address these challenges, in this paper we propose HALOC, a hardware-aware automatic low-rank compression framework. By interpreting automatic rank selection from an architecture search perspective, we develop an end-to-end solution to determine the suitable layer-wise ranks in a differentiable and hardware-aware way. We further propose design principles and mitigation strategy to efficiently explore the rank space and reduce the potential interference problem.   Experimental results on different datasets and hardware platforms demonstrate the effectiveness of our proposed approach. On CIFAR-10 dataset, HALOC enables 0.07% and 0.38% accuracy increase over the uncompressed ResNet-20 and VGG-16 models with 72.20% and 86.44% fewer FLOPs, respectively. On ImageNet dataset, HALOC achieves 0.9% higher top-1 accuracy than the original ResNet-18 model with 66.16% fewer FLOPs. HALOC also shows 0.66% higher top-1 accuracy increase than the state-of-the-art automatic low-rank compression solution with fewer computational and memory costs. In addition, HALOC demonstrates the practical speedups on different hardware platforms, verified by the measurement results on desktop GPU, embedded GPU and ASIC accelerator.", "authors": "Jinqi Xiao, Chengming Zhang, Yu Gong, Miao Yin, Yang Sui, Lizhi Xiang, Dingwen Tao, Bo Yuan", "first_author": "Bo Yuan", "first_end_author": "Jinqi Xiao; Bo Yuan", "publish_time": "2023-01-20 01:57:34+00:00", "update_time": "2023-01-20 01:57:34+00:00"}, {"id": "2301.08390", "url": "http://arxiv.org/abs/2301.08390v1", "pdf_url": "http://arxiv.org/pdf/2301.08390v1", "title": "Open-Set Likelihood Maximization for Few-Shot Learning", "abs": "We tackle the Few-Shot Open-Set Recognition (FSOSR) problem, i.e. classifying instances among a set of classes for which we only have a few labeled samples, while simultaneously detecting instances that do not belong to any known class. We explore the popular transductive setting, which leverages the unlabelled query instances at inference. Motivated by the observation that existing transductive methods perform poorly in open-set scenarios, we propose a generalization of the maximum likelihood principle, in which latent scores down-weighing the influence of potential outliers are introduced alongside the usual parametric model. Our formulation embeds supervision constraints from the support set and additional penalties discouraging overconfident predictions on the query set. We proceed with a block-coordinate descent, with the latent scores and parametric model co-optimized alternately, thereby benefiting from each other. We call our resulting formulation \\textit{Open-Set Likelihood Optimization} (OSLO). OSLO is interpretable and fully modular; it can be applied on top of any pre-trained model seamlessly. Through extensive experiments, we show that our method surpasses existing inductive and transductive methods on both aspects of open-set recognition, namely inlier classification and outlier detection.", "authors": "Malik Boudiaf, Etienne Bennequin, Myriam Tami, Antoine Toubhans, Pablo Piantanida, C\u00e9line Hudelot, Ismail Ben Ayed", "first_author": "Ismail Ben Ayed", "first_end_author": "Malik Boudiaf; Ismail Ben Ayed", "publish_time": "2023-01-20 01:56:19+00:00", "update_time": "2023-01-20 01:56:19+00:00"}, {"id": "2301.08387", "url": "http://arxiv.org/abs/2301.08387v1", "pdf_url": "http://arxiv.org/pdf/2301.08387v1", "title": "Occlusion Reasoning for Skeleton Extraction of Self-Occluded Tree   Canopies", "abs": "In this work, we present a method to extract the skeleton of a self-occluded tree canopy by estimating the unobserved structures of the tree. A tree skeleton compactly describes the topological structure and contains useful information such as branch geometry, positions and hierarchy. This can be critical to planning contact interactions for agricultural manipulation, yet is difficult to gain due to occlusion by leaves, fruits and other branches. Our method uses an instance segmentation network to detect visible trunk, branches, and twigs. Then, based on the observed tree structures, we build a custom 3D likelihood map in the form of an occupancy grid to hypothesize on the presence of occluded skeletons through a series of minimum cost path searches. We show that our method outperforms baseline methods in highly occluded scenes, demonstrated through a set of experiments on a synthetic tree dataset. Qualitative results are also presented on a real tree dataset collected from the field.", "authors": "Chung Hee Kim, George Kantor", "first_author": "George Kantor", "first_end_author": "Chung Hee Kim; George Kantor", "publish_time": "2023-01-20 01:46:07+00:00", "update_time": "2023-01-20 01:46:07+00:00"}, {"id": "2212.07398", "url": "http://arxiv.org/abs/2212.07398v2", "pdf_url": "http://arxiv.org/pdf/2212.07398v2", "title": "Self-Play and Self-Describe: Policy Adaptation with Vision-Language   Foundation Models", "abs": "Recent progress on vision-language foundation models have brought significant advancement to building general-purpose robots. By using the pre-trained models to encode the scene and instructions as inputs for decision making, the instruction-conditioned policy can generalize across different objects and tasks. While this is encouraging, the policy still fails in most cases given an unseen task or environment. To adapt the policy to unseen tasks and environments, we explore a new paradigm on leveraging the pre-trained foundation models with Self-PLAY and Self-Describe (SPLAYD). When deploying the trained policy to a new task or a new environment, we first let the policy self-play with randomly generated instructions to record the demonstrations. While the execution could be wrong, we can use the pre-trained foundation models to accurately self-describe (i.e., re-label or classify) the demonstrations. This automatically provides new pairs of demonstration-instruction data for policy fine-tuning. We evaluate our method on a broad range of experiments with the focus on generalization on unseen objects, unseen tasks, unseen environments, and sim-to-real transfer. We show SPLAYD improves baselines by a large margin in all cases. Our project page is available at https://geyuying.github.io/SPLAYD/", "authors": "Yuying Ge, Annabella Macaluso, Li Erran Li, Ping Luo, Xiaolong Wang", "first_author": "Xiaolong Wang", "first_end_author": "Yuying Ge; Xiaolong Wang", "publish_time": "2022-12-14 18:31:47+00:00", "update_time": "2023-01-20 00:47:23+00:00"}, {"id": "2301.08365", "url": "http://arxiv.org/abs/2301.08365v1", "pdf_url": "http://arxiv.org/pdf/2301.08365v1", "title": "On Retrospective $k$-space Subsampling schemes For Deep MRI   Reconstruction", "abs": "$\\textbf{Purpose:}$ The MRI $k$-space acquisition is time consuming. Traditional techniques aim to acquire accelerated data, which in conjunction with recent DL methods, aid in producing high-fidelity images in truncated times. Conventionally, subsampling the $k$-space is performed by utilizing Cartesian-rectilinear trajectories, which even with the use of DL, provide imprecise reconstructions, though, a plethora of non-rectilinear or non-Cartesian trajectories can be implemented in modern MRI scanners. This work investigates the effect of the $k$-space subsampling scheme on the quality of reconstructed accelerated MRI measurements produced by trained DL models.   $\\textbf{Methods:}$ The RecurrentVarNet was used as the DL-based MRI-reconstruction architecture. Cartesian fully-sampled multi-coil $k$-space measurements from three datasets with different accelerations were retrospectively subsampled using eight distinct subsampling schemes (four Cartesian-rectilinear, two Cartesian non-rectilinear, two non-Cartesian). Experiments were conducted in two frameworks: Scheme-specific, where a distinct model was trained and evaluated for each dataset-subsampling scheme pair, and multi-scheme, where for each dataset a single model was trained on data randomly subsampled by any of the eight schemes and evaluated on data subsampled by all schemes.   $\\textbf{Results:}$ In the scheme-specific setting RecurrentVarNets trained and evaluated on non-rectilinearly subsampled data demonstrated superior performance especially for high accelerations, whilst in the multi-scheme setting, reconstruction performance on rectilinearly subsampled data improved when compared to the scheme-specific experiments.   $\\textbf{Conclusion:}$ Training DL-based MRI reconstruction algorithms on non-rectilinearly subsampled measurements can produce more faithful reconstructions.", "authors": "George Yiasemis, Clara I. S\u00e1nchez, Jan-Jakob Sonke, Jonas Teuwen", "first_author": "Jonas Teuwen", "first_end_author": "George Yiasemis; Jonas Teuwen", "publish_time": "2023-01-20 00:05:18+00:00", "update_time": "2023-01-20 00:05:18+00:00"}, {"id": "2110.05668", "url": "http://arxiv.org/abs/2110.05668v6", "pdf_url": "http://arxiv.org/pdf/2110.05668v6", "title": "NAS-Bench-360: Benchmarking Neural Architecture Search on Diverse Tasks", "abs": "Most existing neural architecture search (NAS) benchmarks and algorithms prioritize well-studied tasks, e.g. image classification on CIFAR or ImageNet. This makes the performance of NAS approaches in more diverse areas poorly understood. In this paper, we present NAS-Bench-360, a benchmark suite to evaluate methods on domains beyond those traditionally studied in architecture search, and use it to address the following question: do state-of-the-art NAS methods perform well on diverse tasks? To construct the benchmark, we curate ten tasks spanning a diverse array of application domains, dataset sizes, problem dimensionalities, and learning objectives. Each task is carefully chosen to interoperate with modern CNN-based search methods while possibly being far-afield from its original development domain. To speed up and reduce the cost of NAS research, for two of the tasks we release the precomputed performance of 15,625 architectures comprising a standard CNN search space. Experimentally, we show the need for more robust NAS evaluation of the kind NAS-Bench-360 enables by showing that several modern NAS procedures perform inconsistently across the ten tasks, with many catastrophically poor results. We also demonstrate how NAS-Bench-360 and its associated precomputed results will enable future scientific discoveries by testing whether several recent hypotheses promoted in the NAS literature hold on diverse tasks. NAS-Bench-360 is hosted at https://nb360.ml.cmu.edu.", "authors": "Renbo Tu, Nicholas Roberts, Mikhail Khodak, Junhong Shen, Frederic Sala, Ameet Talwalkar", "first_author": "Ameet Talwalkar", "first_end_author": "Renbo Tu; Ameet Talwalkar", "publish_time": "2021-10-12 01:13:18+00:00", "update_time": "2023-01-19 23:17:16+00:00"}, {"id": "2007.12140", "url": "http://arxiv.org/abs/2007.12140v5", "pdf_url": "http://arxiv.org/pdf/2007.12140v5", "title": "HITNet: Hierarchical Iterative Tile Refinement Network for Real-time   Stereo Matching", "abs": "This paper presents HITNet, a novel neural network architecture for real-time stereo matching. Contrary to many recent neural network approaches that operate on a full cost volume and rely on 3D convolutions, our approach does not explicitly build a volume and instead relies on a fast multi-resolution initialization step, differentiable 2D geometric propagation and warping mechanisms to infer disparity hypotheses. To achieve a high level of accuracy, our network not only geometrically reasons about disparities but also infers slanted plane hypotheses allowing to more accurately perform geometric warping and upsampling operations. Our architecture is inherently multi-resolution allowing the propagation of information across different levels. Multiple experiments prove the effectiveness of the proposed approach at a fraction of the computation required by state-of-the-art methods. At the time of writing, HITNet ranks 1st-3rd on all the metrics published on the ETH3D website for two view stereo, ranks 1st on most of the metrics among all the end-to-end learning approaches on Middlebury-v3, ranks 1st on the popular KITTI 2012 and 2015 benchmarks among the published methods faster than 100ms.", "authors": "Vladimir Tankovich, Christian H\u00e4ne, Yinda Zhang, Adarsh Kowdle, Sean Fanello, Sofien Bouaziz", "first_author": "Sofien Bouaziz", "first_end_author": "Vladimir Tankovich; Sofien Bouaziz", "publish_time": "2020-07-23 17:11:48+00:00", "update_time": "2023-01-19 23:15:04+00:00"}, {"id": "2301.03045", "url": "http://arxiv.org/abs/2301.03045v2", "pdf_url": "http://arxiv.org/pdf/2301.03045v2", "title": "Seamless Multimodal Biometrics for Continuous Personalised Wellbeing   Monitoring", "abs": "Artificially intelligent perception is increasingly present in the lives of every one of us. Vehicles are no exception, (...) In the near future, pattern recognition will have an even stronger role in vehicles, as self-driving cars will require automated ways to understand what is happening around (and within) them and act accordingly. (...) This doctoral work focused on advancing in-vehicle sensing through the research of novel computer vision and pattern recognition methodologies for both biometrics and wellbeing monitoring. The main focus has been on electrocardiogram (ECG) biometrics, a trait well-known for its potential for seamless driver monitoring. Major efforts were devoted to achieving improved performance in identification and identity verification in off-the-person scenarios, well-known for increased noise and variability. Here, end-to-end deep learning ECG biometric solutions were proposed and important topics were addressed such as cross-database and long-term performance, waveform relevance through explainability, and interlead conversion. Face biometrics, a natural complement to the ECG in seamless unconstrained scenarios, was also studied in this work. The open challenges of masked face recognition and interpretability in biometrics were tackled in an effort to evolve towards algorithms that are more transparent, trustworthy, and robust to significant occlusions. Within the topic of wellbeing monitoring, improved solutions to multimodal emotion recognition in groups of people and activity/violence recognition in in-vehicle scenarios were proposed. At last, we also proposed a novel way to learn template security within end-to-end models, dismissing additional separate encryption processes, and a self-supervised learning approach tailored to sequential data, in order to ensure data security and optimal performance. (...)", "authors": "Jo\u00e3o Ribeiro Pinto", "first_author": "Jo\u00e3o Ribeiro Pinto", "first_end_author": "Jo\u00e3o Ribeiro Pinto; Jo\u00e3o Ribeiro Pinto", "publish_time": "2023-01-08 14:07:01+00:00", "update_time": "2023-01-19 22:29:35+00:00"}, {"id": "2208.09198", "url": "http://arxiv.org/abs/2208.09198v2", "pdf_url": "http://arxiv.org/pdf/2208.09198v2", "title": "TTT-UCDR: Test-time Training for Universal Cross-Domain Retrieval", "abs": "Image retrieval under generalized test scenarios has gained significant momentum in literature, and the recently proposed protocol of Universal Cross-domain Retrieval is a pioneer in this direction. A common practice in any such generalized classification or retrieval algorithm is to exploit samples from multiple domains during training to learn a domain-invariant representation of data. Such criterion is often restrictive, and thus in this work, for the first time, we explore the challenges associated with generalized retrieval problems under a low-data regime, which is quite relevant in many real-world scenarios. We attempt to make any retrieval model trained on a small cross-domain dataset (containing just two training domains) more generalizable towards any unknown query domain or category by quickly adapting it to the test data during inference. This form of test-time training or adaptation of the retrieval model is explored by means of a number of self-supervision-based loss functions, for example, Rotnet, Jigsaw-puzzle, Barlow twins, etc., in this work. Extensive experiments on multiple large-scale datasets demonstrate the effectiveness of the proposed approach.", "authors": "Soumava Paul, Titir Dutta, Aheli Saha, Abhishek Samanta, Soma Biswas", "first_author": "Soma Biswas", "first_end_author": "Soumava Paul; Soma Biswas", "publish_time": "2022-08-19 07:50:04+00:00", "update_time": "2023-01-19 22:14:54+00:00"}, {"id": "2301.08330", "url": "http://arxiv.org/abs/2301.08330v1", "pdf_url": "http://arxiv.org/pdf/2301.08330v1", "title": "The role of noise in denoising models for anomaly detection in medical   images", "abs": "Pathological brain lesions exhibit diverse appearance in brain images, in terms of intensity, texture, shape, size, and location. Comprehensive sets of data and annotations are difficult to acquire. Therefore, unsupervised anomaly detection approaches have been proposed using only normal data for training, with the aim of detecting outlier anomalous voxels at test time. Denoising methods, for instance classical denoising autoencoders (DAEs) and more recently emerging diffusion models, are a promising approach, however naive application of pixelwise noise leads to poor anomaly detection performance. We show that optimization of the spatial resolution and magnitude of the noise improves the performance of different model training regimes, with similar noise parameter adjustments giving good performance for both DAEs and diffusion models. Visual inspection of the reconstructions suggests that the training noise influences the trade-off between the extent of the detail that is reconstructed and the extent of erasure of anomalies, both of which contribute to better anomaly detection performance. We validate our findings on two real-world datasets (tumor detection in brain MRI and hemorrhage/ischemia/tumor detection in brain CT), showing good detection on diverse anomaly appearances. Overall, we find that a DAE trained with coarse noise is a fast and simple method that gives state-of-the-art accuracy. Diffusion models applied to anomaly detection are as yet in their infancy and provide a promising avenue for further research.", "authors": "Antanas Kascenas, Pedro Sanchez, Patrick Schrempf, Chaoyang Wang, William Clackett, Shadia S. Mikhael, Jeremy P. Voisey, Keith Goatman, Alexander Weir, Nicolas Pugeault, Sotirios A. Tsaftaris, Alison Q. O'Neil", "first_author": "Alison Q. O'Neil", "first_end_author": "Antanas Kascenas; Alison Q. O'Neil", "publish_time": "2023-01-19 21:39:38+00:00", "update_time": "2023-01-19 21:39:38+00:00"}, {"id": "2301.08317", "url": "http://arxiv.org/abs/2301.08317v1", "pdf_url": "http://arxiv.org/pdf/2301.08317v1", "title": "Learning ultrasound plane pose regression: assessing generalized pose   coordinates in the fetal brain", "abs": "In obstetric ultrasound (US) scanning, the learner's ability to mentally build a three-dimensional (3D) map of the fetus from a two-dimensional (2D) US image represents a significant challenge in skill acquisition. We aim to build a US plane localization system for 3D visualization, training, and guidance without integrating additional sensors. This work builds on top of our previous work, which predicts the six-dimensional (6D) pose of arbitrarily-oriented US planes slicing the fetal brain with respect to a normalized reference frame using a convolutional neural network (CNN) regression network. Here, we analyze in detail the assumptions of the normalized fetal brain reference frame and quantify its accuracy with respect to the acquisition of transventricular (TV) standard plane (SP) for fetal biometry. We investigate the impact of registration quality in the training and testing data and its subsequent effect on trained models. Finally, we introduce data augmentations and larger training sets that improve the results of our previous work, achieving median errors of 3.53 mm and 6.42 degrees for translation and rotation, respectively.", "authors": "Chiara Di Vece, Maela Le Lous, Brian Dromey, Francisco Vasconcelos, Anna L David, Donald Peebles, Danail Stoyanov", "first_author": "Danail Stoyanov", "first_end_author": "Chiara Di Vece; Danail Stoyanov", "publish_time": "2023-01-19 21:16:36+00:00", "update_time": "2023-01-19 21:16:36+00:00"}, {"id": "2301.08247", "url": "http://arxiv.org/abs/2301.08247v1", "pdf_url": "http://arxiv.org/pdf/2301.08247v1", "title": "Multiview Compressive Coding for 3D Reconstruction", "abs": "A central goal of visual recognition is to understand objects and scenes from a single image. 2D recognition has witnessed tremendous progress thanks to large-scale learning and general-purpose representations. Comparatively, 3D poses new challenges stemming from occlusions not depicted in the image. Prior works try to overcome these by inferring from multiple views or rely on scarce CAD models and category-specific priors which hinder scaling to novel settings. In this work, we explore single-view 3D reconstruction by learning generalizable representations inspired by advances in self-supervised learning. We introduce a simple framework that operates on 3D points of single objects or whole scenes coupled with category-agnostic large-scale training from diverse RGB-D videos. Our model, Multiview Compressive Coding (MCC), learns to compress the input appearance and geometry to predict the 3D structure by querying a 3D-aware decoder. MCC's generality and efficiency allow it to learn from large-scale and diverse data sources with strong generalization to novel objects imagined by DALL$\\cdot$E 2 or captured in-the-wild with an iPhone.", "authors": "Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph Feichtenhofer, Georgia Gkioxari", "first_author": "Georgia Gkioxari", "first_end_author": "Chao-Yuan Wu; Georgia Gkioxari", "publish_time": "2023-01-19 18:59:52+00:00", "update_time": "2023-01-19 18:59:52+00:00"}, {"id": "2211.02578", "url": "http://arxiv.org/abs/2211.02578v2", "pdf_url": "http://arxiv.org/pdf/2211.02578v2", "title": "Data Models for Dataset Drift Controls in Machine Learning With Images", "abs": "Camera images are ubiquitous in machine learning research. They also play a central role in the delivery of important services spanning medicine and environmental surveying. However, the application of machine learning models in these domains has been limited because of robustness concerns. A primary failure mode are performance drops due to differences between the training and deployment data. While there are methods to prospectively validate the robustness of machine learning models to such dataset drifts, existing approaches do not account for explicit models of the primary object of interest: the data. This limits our ability to study and understand the relationship between data generation and downstream machine learning model performance in a physically accurate manner. In this study, we demonstrate how to overcome this limitation by pairing traditional machine learning with physical optics to obtain explicit and differentiable data models. We demonstrate how such data models can be constructed for image data and used to control downstream machine learning model performance related to dataset drift. The findings are distilled into three applications. First, drift synthesis enables the controlled generation of physically faithful drift test cases to power model selection and targeted generalization. Second, the gradient connection between machine learning task model and data model allows advanced, precise tolerancing of task model sensitivity to changes in the data generation. These drift forensics can be used to precisely specify the acceptable data environments in which a task model may be run. Third, drift optimization opens up the possibility to create drifts that can help the task model learn better faster, effectively optimizing the data generating process itself. A guide to access the open code and datasets is available at https://github.com/aiaudit-org/raw2logit.", "authors": "Luis Oala, Marco Aversa, Gabriel Nobis, Kurt Willis, Yoan Neuenschwander, Mich\u00e8le Buck, Christian Matek, Jerome Extermann, Enrico Pomarico, Wojciech Samek, Roderick Murray-Smith, Christoph Clausen, Bruno Sanguinetti", "first_author": "Bruno Sanguinetti", "first_end_author": "Luis Oala; Bruno Sanguinetti", "publish_time": "2022-11-04 16:50:10+00:00", "update_time": "2023-01-19 18:59:45+00:00"}, {"id": "2301.08245", "url": "http://arxiv.org/abs/2301.08245v1", "pdf_url": "http://arxiv.org/pdf/2301.08245v1", "title": "Booster: a Benchmark for Depth from Images of Specular and Transparent   Surfaces", "abs": "Estimating depth from images nowadays yields outstanding results, both in terms of in-domain accuracy and generalization. However, we identify two main challenges that remain open in this field: dealing with non-Lambertian materials and effectively processing high-resolution images. Purposely, we propose a novel dataset that includes accurate and dense ground-truth labels at high resolution, featuring scenes containing several specular and transparent surfaces. Our acquisition pipeline leverages a novel deep space-time stereo framework, enabling easy and accurate labeling with sub-pixel precision. The dataset is composed of 606 samples collected in 85 different scenes, each sample includes both a high-resolution pair (12 Mpx) as well as an unbalanced stereo pair (Left: 12 Mpx, Right: 1.1 Mpx). Additionally, we provide manually annotated material segmentation masks and 15K unlabeled samples. We divide the dataset into a training set, and two testing sets, the latter devoted to the evaluation of stereo and monocular depth estimation networks respectively to highlight the open challenges and future research directions in this field.", "authors": "Pierluigi Zama Ramirez, Alex Costanzino, Fabio Tosi, Matteo Poggi, Samuele Salti, Stefano Mattoccia, Luigi Di Stefano", "first_author": "Luigi Di Stefano", "first_end_author": "Pierluigi Zama Ramirez; Luigi Di Stefano", "publish_time": "2023-01-19 18:59:28+00:00", "update_time": "2023-01-19 18:59:28+00:00"}, {"id": "2301.08243", "url": "http://arxiv.org/abs/2301.08243v1", "pdf_url": "http://arxiv.org/pdf/2301.08243v1", "title": "Self-Supervised Learning from Images with a Joint-Embedding Predictive   Architecture", "abs": "This paper demonstrates an approach for learning highly semantic image representations without relying on hand-crafted data-augmentations. We introduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), a non-generative approach for self-supervised learning from images. The idea behind I-JEPA is simple: from a single context block, predict the representations of various target blocks in the same image. A core design choice to guide I-JEPA towards producing semantic representations is the masking strategy; specifically, it is crucial to (a) predict several target blocks in the image, (b) sample target blocks with sufficiently large scale (occupying 15%-20% of the image), and (c) use a sufficiently informative (spatially distributed) context block. Empirically, when combined with Vision Transformers, we find I-JEPA to be highly scalable. For instance, we train a ViT-Huge/16 on ImageNet using 32 A100 GPUs in under 38 hours to achieve strong downstream performance across a wide range of tasks requiring various levels of abstraction, from linear classification to object counting and depth prediction.", "authors": "Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, Nicolas Ballas", "first_author": "Nicolas Ballas", "first_end_author": "Mahmoud Assran; Nicolas Ballas", "publish_time": "2023-01-19 18:59:01+00:00", "update_time": "2023-01-19 18:59:01+00:00"}, {"id": "2301.08237", "url": "http://arxiv.org/abs/2301.08237v1", "pdf_url": "http://arxiv.org/pdf/2301.08237v1", "title": "LoCoNet: Long-Short Context Network for Active Speaker Detection", "abs": "Active Speaker Detection (ASD) aims to identify who is speaking in each frame of a video. ASD reasons from audio and visual information from two contexts: long-term intra-speaker context and short-term inter-speaker context. Long-term intra-speaker context models the temporal dependencies of the same speaker, while short-term inter-speaker context models the interactions of speakers in the same scene. These two contexts are complementary to each other and can help infer the active speaker. Motivated by these observations, we propose LoCoNet, a simple yet effective Long-Short Context Network that models the long-term intra-speaker context and short-term inter-speaker context. We use self-attention to model long-term intra-speaker context due to its effectiveness in modeling long-range dependencies, and convolutional blocks that capture local patterns to model short-term inter-speaker context. Extensive experiments show that LoCoNet achieves state-of-the-art performance on multiple datasets, achieving an mAP of 95.2%(+1.1%) on AVA-ActiveSpeaker, 68.1%(+22%) on Columbia dataset, 97.2%(+2.8%) on Talkies dataset and 59.7%(+8.0%) on Ego4D dataset. Moreover, in challenging cases where multiple speakers are present, or face of active speaker is much smaller than other faces in the same scene, LoCoNet outperforms previous state-of-the-art methods by 3.4% on the AVA-ActiveSpeaker dataset. The code will be released at https://github.com/SJTUwxz/LoCoNet_ASD.", "authors": "Xizi Wang, Feng Cheng, Gedas Bertasius, David Crandall", "first_author": "David Crandall", "first_end_author": "Xizi Wang; David Crandall", "publish_time": "2023-01-19 18:54:43+00:00", "update_time": "2023-01-19 18:54:43+00:00"}, {"id": "2301.08229", "url": "http://arxiv.org/abs/2301.08229v1", "pdf_url": "http://arxiv.org/pdf/2301.08229v1", "title": "Estimating Remaining Lifespan from the Face", "abs": "The face is a rich source of information that can be utilized to infer a person's biological age, sex, phenotype, genetic defects, and health status. All of these factors are relevant for predicting an individual's remaining lifespan. In this study, we collected a dataset of over 24,000 images (from Wikidata/Wikipedia) of individuals who died of natural causes, along with the number of years between when the image was taken and when the person passed away. We made this dataset publicly available. We fine-tuned multiple Convolutional Neural Network (CNN) models on this data, at best achieving a mean absolute error of 8.3 years in the validation data using VGGFace. However, the model's performance diminishes when the person was younger at the time of the image. To demonstrate the potential applications of our remaining lifespan model, we present examples of using it to estimate the average loss of life (in years) due to the COVID-19 pandemic and to predict the increase in life expectancy that might result from a health intervention such as weight loss. Additionally, we discuss the ethical considerations associated with such models.", "authors": "Amir Fekrazad", "first_author": "Amir Fekrazad", "first_end_author": "Amir Fekrazad; Amir Fekrazad", "publish_time": "2023-01-19 18:38:04+00:00", "update_time": "2023-01-19 18:38:04+00:00"}, {"id": "2301.08189", "url": "http://arxiv.org/abs/2301.08189v1", "pdf_url": "http://arxiv.org/pdf/2301.08189v1", "title": "Benchmarking YOLOv5 and YOLOv7 models with DeepSORT for droplet tracking   applications", "abs": "Tracking droplets in microfluidics is a challenging task. The difficulty arises in choosing a tool to analyze general microfluidic videos to infer physical quantities. The state-of-the-art object detector algorithm You Only Look Once (YOLO) and the object tracking algorithm Simple Online and Realtime Tracking with a Deep Association Metric (DeepSORT) are customizable for droplet identification and tracking. The customization includes training YOLO and DeepSORT networks to identify and track the objects of interest. We trained several YOLOv5 and YOLOv7 models and the DeepSORT network for droplet identification and tracking from microfluidic experimental videos. We compare the performance of the droplet tracking applications with YOLOv5 and YOLOv7 in terms of training time and time to analyze a given video across various hardware configurations. Despite the latest YOLOv7 being 10% faster, the real-time tracking is only achieved by lighter YOLO models on RTX 3070 Ti GPU machine due to additional significant droplet tracking costs arising from the DeepSORT algorithm. This work is a benchmark study for the YOLOv5 and YOLOv7 networks with DeepSORT in terms of the training time and inference time for a custom dataset of microfluidic droplets.", "authors": "Mihir Durve, Sibilla Orsini, Adriano Tiribocchi, Andrea Montessori, Jean-Michel Tucny, Marco Lauricella, Andrea Camposeo, Dario Pisignano, Sauro Succi", "first_author": "Sauro Succi", "first_end_author": "Mihir Durve; Sauro Succi", "publish_time": "2023-01-19 17:37:40+00:00", "update_time": "2023-01-19 17:37:40+00:00"}, {"id": "2301.08187", "url": "http://arxiv.org/abs/2301.08187v1", "pdf_url": "http://arxiv.org/pdf/2301.08187v1", "title": "A Multi-Resolution Framework for U-Nets with Applications to   Hierarchical VAEs", "abs": "U-Net architectures are ubiquitous in state-of-the-art deep learning, however their regularisation properties and relationship to wavelets are understudied. In this paper, we formulate a multi-resolution framework which identifies U-Nets as finite-dimensional truncations of models on an infinite-dimensional function space. We provide theoretical results which prove that average pooling corresponds to projection within the space of square-integrable functions and show that U-Nets with average pooling implicitly learn a Haar wavelet basis representation of the data. We then leverage our framework to identify state-of-the-art hierarchical VAEs (HVAEs), which have a U-Net architecture, as a type of two-step forward Euler discretisation of multi-resolution diffusion processes which flow from a point mass, introducing sampling instabilities. We also demonstrate that HVAEs learn a representation of time which allows for improved parameter efficiency through weight-sharing. We use this observation to achieve state-of-the-art HVAE performance with half the number of parameters of existing models, exploiting the properties of our continuous-time formulation.", "authors": "Fabian Falck, Christopher Williams, Dominic Danks, George Deligiannidis, Christopher Yau, Chris Holmes, Arnaud Doucet, Matthew Willetts", "first_author": "Matthew Willetts", "first_end_author": "Fabian Falck; Matthew Willetts", "publish_time": "2023-01-19 17:33:48+00:00", "update_time": "2023-01-19 17:33:48+00:00"}, {"id": "2301.08174", "url": "http://arxiv.org/abs/2301.08174v1", "pdf_url": "http://arxiv.org/pdf/2301.08174v1", "title": "Collaborative Robotic Ultrasound Tissue Scanning for Surgical Resection   Guidance in Neurosurgery", "abs": "The aim of this paper is to introduce a robotic platform for autonomous iUS tissue scanning to optimise intraoperative diagnosis and improve surgical resection during robot-assisted operations. To guide anatomy specific robotic scanning and generate a representation of the robot task space, fast and accurate techniques for the recovery of 3D morphological structures of the surgical cavity are developed. The prototypic DLR MIRO surgical robotic arm is used to control the applied force and the in-plane motion of the US transducer. A key application of the proposed platform is the scanning of brain tissue to guide tumour resection.", "authors": "Alistair Weld, Michael Dyck, Julian Klodmann, Giulio Anichini, Luke Dixon, Sophie Camp, Alin Albu-Sch\u00e4ffer, Stamatia Giannarou", "first_author": "Stamatia Giannarou", "first_end_author": "Alistair Weld; Stamatia Giannarou", "publish_time": "2023-01-19 17:05:07+00:00", "update_time": "2023-01-19 17:05:07+00:00"}, {"id": "2209.12684", "url": "http://arxiv.org/abs/2209.12684v2", "pdf_url": "http://arxiv.org/pdf/2209.12684v2", "title": "Soft-labeling Strategies for Rapid Sub-Typing", "abs": "The challenge of labeling large example datasets for computer vision continues to limit the availability and scope of image repositories. This research provides a new method for automated data collection, curation, labeling, and iterative training with minimal human intervention for the case of overhead satellite imagery and object detection. The new operational scale effectively scanned an entire city (68 square miles) in grid search and yielded a prediction of car color from space observations. A partially trained yolov5 model served as an initial inference seed to output further, more refined model predictions in iterative cycles. Soft labeling here refers to accepting label noise as a potentially valuable augmentation to reduce overfitting and enhance generalized predictions to previously unseen test data. The approach takes advantage of a real-world instance where a cropped image of a car can automatically receive sub-type information as white or colorful from pixel values alone, thus completing an end-to-end pipeline without overdependence on human labor.", "authors": "Grant Rosario, David Noever, Matt Ciolino", "first_author": "Matt Ciolino", "first_end_author": "Grant Rosario; Matt Ciolino", "publish_time": "2022-09-23 03:04:27+00:00", "update_time": "2023-01-19 16:41:19+00:00"}, {"id": "2301.08160", "url": "http://arxiv.org/abs/2301.08160v1", "pdf_url": "http://arxiv.org/pdf/2301.08160v1", "title": "FECANet: Boosting Few-Shot Semantic Segmentation with Feature-Enhanced   Context-Aware Network", "abs": "Few-shot semantic segmentation is the task of learning to locate each pixel of the novel class in the query image with only a few annotated support images. The current correlation-based methods construct pair-wise feature correlations to establish the many-to-many matching because the typical prototype-based approaches cannot learn fine-grained correspondence relations. However, the existing methods still suffer from the noise contained in naive correlations and the lack of context semantic information in correlations. To alleviate these problems mentioned above, we propose a Feature-Enhanced Context-Aware Network (FECANet). Specifically, a feature enhancement module is proposed to suppress the matching noise caused by inter-class local similarity and enhance the intra-class relevance in the naive correlation. In addition, we propose a novel correlation reconstruction module that encodes extra correspondence relations between foreground and background and multi-scale context semantic features, significantly boosting the encoder to capture a reliable matching pattern. Experiments on PASCAL-$5^i$ and COCO-$20^i$ datasets demonstrate that our proposed FECANet leads to remarkable improvement compared to previous state-of-the-arts, demonstrating its effectiveness.", "authors": "Huafeng Liu, Pai Peng, Tao Chen, Qiong Wang, Yazhou Yao, Xian-Sheng Hua", "first_author": "Xian-Sheng Hua", "first_end_author": "Huafeng Liu; Xian-Sheng Hua", "publish_time": "2023-01-19 16:31:13+00:00", "update_time": "2023-01-19 16:31:13+00:00"}, {"id": "2301.08157", "url": "http://arxiv.org/abs/2301.08157v1", "pdf_url": "http://arxiv.org/pdf/2301.08157v1", "title": "SoftEnNet: Symbiotic Monocular Depth Estimation and Lumen Segmentation   for Colonoscopy Endorobots", "abs": "Colorectal cancer is the third most common cause of cancer death worldwide. Optical colonoscopy is the gold standard for detecting colorectal cancer; however, about 25 percent of polyps are missed during the procedure. A vision-based autonomous endorobot can improve colonoscopy procedures significantly through systematic, complete screening of the colonic mucosa. The reliable robot navigation needed requires a three-dimensional understanding of the environment and lumen tracking to support autonomous tasks. We propose a novel multi-task model that simultaneously predicts dense depth and lumen segmentation with an ensemble of deep networks. The depth estimation sub-network is trained in a self-supervised fashion guided by view synthesis; the lumen segmentation sub-network is supervised. The two sub-networks are interconnected with pathways that enable information exchange and thereby mutual learning. As the lumen is in the image's deepest visual space, lumen segmentation helps with the depth estimation at the farthest location. In turn, the estimated depth guides the lumen segmentation network as the lumen location defines the farthest scene location. Unlike other environments, view synthesis often fails in the colon because of the deformable wall, textureless surface, specularities, and wide field of view image distortions, all challenges that our pipeline addresses. We conducted qualitative analysis on a synthetic dataset and quantitative analysis on a colon training model and real colonoscopy videos. The experiments show that our model predicts accurate scale-invariant depth maps and lumen segmentation from colonoscopy images in near real-time.", "authors": "Alwyn Mathew, Ludovic Magerand, Emanuele Trucco, Luigi Manfredi", "first_author": "Luigi Manfredi", "first_end_author": "Alwyn Mathew; Luigi Manfredi", "publish_time": "2023-01-19 16:22:17+00:00", "update_time": "2023-01-19 16:22:17+00:00"}, {"id": "2301.08153", "url": "http://arxiv.org/abs/2301.08153v1", "pdf_url": "http://arxiv.org/pdf/2301.08153v1", "title": "SwiftAvatar: Efficient Auto-Creation of Parameterized Stylized Character   on Arbitrary Avatar Engines", "abs": "The creation of a parameterized stylized character involves careful selection of numerous parameters, also known as the \"avatar vectors\" that can be interpreted by the avatar engine. Existing unsupervised avatar vector estimation methods that auto-create avatars for users, however, often fail to work because of the domain gap between realistic faces and stylized avatar images. To this end, we propose SwiftAvatar, a novel avatar auto-creation framework that is evidently superior to previous works. SwiftAvatar introduces dual-domain generators to create pairs of realistic faces and avatar images using shared latent codes. The latent codes can then be bridged with the avatar vectors as pairs, by performing GAN inversion on the avatar images rendered from the engine using avatar vectors. Through this way, we are able to synthesize paired data in high-quality as many as possible, consisting of avatar vectors and their corresponding realistic faces. We also propose semantic augmentation to improve the diversity of synthesis. Finally, a light-weight avatar vector estimator is trained on the synthetic pairs to implement efficient auto-creation. Our experiments demonstrate the effectiveness and efficiency of SwiftAvatar on two different avatar engines. The superiority and advantageous flexibility of SwiftAvatar are also verified in both subjective and objective evaluations.", "authors": "Shizun Wang, Weihong Zeng, Xu Wang, Hao Yang, Li Chen, Chuang Zhang, Ming Wu, Yi Yuan, Yunzhao Zeng, Min Zheng", "first_author": "Min Zheng", "first_end_author": "Shizun Wang; Min Zheng", "publish_time": "2023-01-19 16:14:28+00:00", "update_time": "2023-01-19 16:14:28+00:00"}, {"id": "2205.11606", "url": "http://arxiv.org/abs/2205.11606v3", "pdf_url": "http://arxiv.org/pdf/2205.11606v3", "title": "Discriminative Feature Learning through Feature Distance Loss", "abs": "Ensembles of Convolutional neural networks have shown remarkable results in learning discriminative semantic features for image classification tasks. Though, the models in the ensemble often concentrate on similar regions in images. This work proposes a novel method that forces a set of base models to learn different features for a classification task. These models are combined in an ensemble to make a collective classification. The key finding is that by forcing the models to concentrate on different features, the classification accuracy is increased. To learn different feature concepts, a so-called feature distance loss is implemented on the feature maps. The experiments on benchmark convolutional neural networks (VGG16, ResNet, AlexNet), popular datasets (Cifar10, Cifar100, miniImageNet, NEU, BSD, TEX), and different training samples (3, 5, 10, 20, 50, 100 per class) show the effectiveness of the proposed feature loss. The proposed method outperforms classical ensemble versions of the base models. The Class Activation Maps explicitly prove the ability to learn different feature concepts. The code is available at: https://github.com/2Obe/Feature-Distance-Loss.git", "authors": "Tobias Schlagenhauf, Yiwen Lin, Benjamin Noack", "first_author": "Benjamin Noack", "first_end_author": "Tobias Schlagenhauf; Benjamin Noack", "publish_time": "2022-05-23 20:01:32+00:00", "update_time": "2023-01-19 16:08:46+00:00"}, {"id": "2301.08147", "url": "http://arxiv.org/abs/2301.08147v1", "pdf_url": "http://arxiv.org/pdf/2301.08147v1", "title": "RGB-D-Based Categorical Object Pose and Shape Estimation: Methods,   Datasets, and Evaluation", "abs": "Recently, various methods for 6D pose and shape estimation of objects at a per-category level have been proposed. This work provides an overview of the field in terms of methods, datasets, and evaluation protocols. First, an overview of existing works and their commonalities and differences is provided. Second, we take a critical look at the predominant evaluation protocol, including metrics and datasets. Based on the findings, we propose a new set of metrics, contribute new annotations for the Redwood dataset, and evaluate state-of-the-art methods in a fair comparison. The results indicate that existing methods do not generalize well to unconstrained orientations and are actually heavily biased towards objects being upright. We provide an easy-to-use evaluation toolbox with well-defined metrics, methods, and dataset interfaces, which allows evaluation and comparison with various state-of-the-art approaches (https://github.com/roym899/pose_and_shape_evaluation).", "authors": "Leonard Bruns, Patric Jensfelt", "first_author": "Patric Jensfelt", "first_end_author": "Leonard Bruns; Patric Jensfelt", "publish_time": "2023-01-19 15:59:10+00:00", "update_time": "2023-01-19 15:59:10+00:00"}, {"id": "2301.08140", "url": "http://arxiv.org/abs/2301.08140v1", "pdf_url": "http://arxiv.org/pdf/2301.08140v1", "title": "Regularizing disparity estimation via multi task learning with   structured light reconstruction", "abs": "3D reconstruction is a useful tool for surgical planning and guidance. However, the lack of available medical data stunts research and development in this field, as supervised deep learning methods for accurate disparity estimation rely heavily on large datasets containing ground truth information. Alternative approaches to supervision have been explored, such as self-supervision, which can reduce or remove entirely the need for ground truth. However, no proposed alternatives have demonstrated performance capabilities close to what would be expected from a supervised setup. This work aims to alleviate this issue. In this paper, we investigate the learning of structured light projections to enhance the development of direct disparity estimation networks. We show for the first time that it is possible to accurately learn the projection of structured light on a scene, implicitly learning disparity. Secondly, we \\textcolor{black}{explore the use of a multi task learning (MTL) framework for the joint training of structured light and disparity. We present results which show that MTL with structured light improves disparity training; without increasing the number of model parameters. Our MTL setup outperformed the single task learning (STL) network in every validation test. Notably, in the medical generalisation test, the STL error was 1.4 times worse than that of the best MTL performance. The benefit of using MTL is emphasised when the training data is limited.} A dataset containing stereoscopic images, disparity maps and structured light projections on medical phantoms and ex vivo tissue was created for evaluation together with virtual scenes. This dataset will be made publicly available in the future.", "authors": "Alistair Weld, Joao Cartucho, Chi Xu, Joseph Davids, Stamatia Giannarou", "first_author": "Stamatia Giannarou", "first_end_author": "Alistair Weld; Stamatia Giannarou", "publish_time": "2023-01-19 15:54:52+00:00", "update_time": "2023-01-19 15:54:52+00:00"}, {"id": "2301.06496", "url": "http://arxiv.org/abs/2301.06496v2", "pdf_url": "http://arxiv.org/pdf/2301.06496v2", "title": "High-bandwidth Close-Range Information Transport through Light Pipes", "abs": "Image retrieval after propagation through multi-mode fibers is gaining attention due to their capacity to confine light and efficiently transport it over distances in a compact system. Here, we propose a generally applicable information-theoretic framework to transmit maximal-entropy (data) images and maximize the information transmission over sub-meter distances, a crucial capability that allows optical storage applications to scale and address different parts of storage media. To this end, we use millimeter-sized square optical waveguides to image a megapixel 8-bit spatial-light modulator. Data is thus represented as a 2D array of 8-bit values (symbols). Transmitting 100000s of symbols requires innovation beyond transmission matrix approaches. Deep neural networks have been recently utilized to retrieve images, but have been limited to small (thousands of symbols) and natural looking (low entropy) images. We maximize information transmission by combining a bandwidth-optimized homodyne detector with a differentiable hybrid neural-network consisting of a digital twin of the experiment setup and a U-Net. For the digital twin, we implement and compare a differentiable mode-based twin with a differentiable ray-based twin. Importantly, the latter can adapt to manufacturing-related setup imperfections during training which we show to be crucial. Our pipeline is trained end-to-end to recover digital input images while maximizing the achievable information page size based on a differentiable mutual-information estimator. We demonstrate retrieval of 66 kB at maximum with 1.7 bit per symbol on average with a range of 0.3 - 3.4 bit.", "authors": "Joowon Lim, Jannes Gladrow, Douglas Kelly, Greg O'Shea, Govert Verkes, Ioan Stefanovici, Sebastian Nowozin, Benn Thomsen", "first_author": "Benn Thomsen", "first_end_author": "Joowon Lim; Benn Thomsen", "publish_time": "2023-01-16 16:10:52+00:00", "update_time": "2023-01-19 15:44:27+00:00"}, {"id": "2301.08125", "url": "http://arxiv.org/abs/2301.08125v1", "pdf_url": "http://arxiv.org/pdf/2301.08125v1", "title": "Diagnose Like a Pathologist: Transformer-Enabled Hierarchical   Attention-Guided Multiple Instance Learning for Whole Slide Image   Classification", "abs": "Multiple Instance Learning (MIL) and transformers are increasingly popular in histopathology Whole Slide Image (WSI) classification. However, unlike human pathologists who selectively observe specific regions of histopathology tissues under different magnifications, most methods do not incorporate multiple resolutions of the WSIs, hierarchically and attentively, thereby leading to a loss of focus on the WSIs and information from other resolutions. To resolve this issue, we propose the Hierarchical Attention-Guided Multiple Instance Learning framework to fully exploit the WSIs, which can dynamically and attentively discover the discriminative regions across multiple resolutions of the WSIs. Within this framework, to further enhance the performance of the transformer and obtain a more holistic WSI (bag) representation, we propose an Integrated Attention Transformer, consisting of multiple Integrated Attention Modules, which is the combination of a transformer layer and an aggregation module that produces a bag representation based on every instance representation in that bag. The results of the experiments show that our method achieved state-of-the-art performances on multiple datasets, including Camelyon16, TCGA-RCC, TCGA-NSCLC, and our in-house IMGC dataset.", "authors": "Conghao Xiong, Hao Chen, Joseph Sung, Irwin King", "first_author": "Irwin King", "first_end_author": "Conghao Xiong; Irwin King", "publish_time": "2023-01-19 15:38:43+00:00", "update_time": "2023-01-19 15:38:43+00:00"}, {"id": "2301.06866", "url": "http://arxiv.org/abs/2301.06866v2", "pdf_url": "http://arxiv.org/pdf/2301.06866v2", "title": "Building Scalable Video Understanding Benchmarks through Sports", "abs": "Existing benchmarks for evaluating long video understanding falls short on multiple aspects, either lacking in scale or quality of annotations. These limitations arise from the difficulty in collecting dense annotations for long videos (e.g. actions, dialogues, etc.), which are often obtained by manually labeling many frames per second. In this work, we introduce an automated Annotation and Video Stream Alignment Pipeline (abbreviated ASAP). We demonstrate the generality of ASAP by aligning unlabeled videos of four different sports (Cricket, Football, Basketball, and American Football) with their corresponding dense annotations (i.e. commentary) freely available on the web. Our human studies indicate that ASAP can align videos and annotations with high fidelity, precision, and speed. We then leverage ASAP scalability to create LCric, a large-scale long video understanding benchmark, with over 1000 hours of densely annotated long Cricket videos (with an average sample length of 50 mins) collected at virtually zero annotation cost. We benchmark and analyze state-of-the-art video understanding models on LCric through a large set of compositional multi-choice and regression queries. We establish a human baseline that indicates significant room for new research to explore. The dataset along with the code for ASAP and baselines can be accessed here: https://asap-benchmark.github.io/.", "authors": "Aniket Agarwal, Alex Zhang, Karthik Narasimhan, Igor Gilitschenski, Vishvak Murahari, Yash Kant", "first_author": "Yash Kant", "first_end_author": "Aniket Agarwal; Yash Kant", "publish_time": "2023-01-17 13:20:21+00:00", "update_time": "2023-01-19 15:33:02+00:00"}, {"id": "2111.00770", "url": "http://arxiv.org/abs/2111.00770v3", "pdf_url": "http://arxiv.org/pdf/2111.00770v3", "title": "Dense Prediction with Attentive Feature Aggregation", "abs": "Aggregating information from features across different layers is an essential operation for dense prediction models. Despite its limited expressiveness, feature concatenation dominates the choice of aggregation operations. In this paper, we introduce Attentive Feature Aggregation (AFA) to fuse different network layers with more expressive non-linear operations. AFA exploits both spatial and channel attention to compute weighted average of the layer activations. Inspired by neural volume rendering, we extend AFA with Scale-Space Rendering (SSR) to perform late fusion of multi-scale predictions. AFA is applicable to a wide range of existing network designs. Our experiments show consistent and significant improvements on challenging semantic segmentation benchmarks, including Cityscapes, BDD100K, and Mapillary Vistas, at negligible computational and parameter overhead. In particular, AFA improves the performance of the Deep Layer Aggregation (DLA) model by nearly 6% mIoU on Cityscapes. Our experimental analyses show that AFA learns to progressively refine segmentation maps and to improve boundary details, leading to new state-of-the-art results on boundary detection benchmarks on BSDS500 and NYUDv2. Code and video resources are available at http://vis.xyz/pub/dla-afa.", "authors": "Yung-Hsu Yang, Thomas E. Huang, Min Sun, Samuel Rota Bul\u00f2, Peter Kontschieder, Fisher Yu", "first_author": "Fisher Yu", "first_end_author": "Yung-Hsu Yang; Fisher Yu", "publish_time": "2021-11-01 08:44:45+00:00", "update_time": "2023-01-19 15:25:52+00:00"}, {"id": "2301.08113", "url": "http://arxiv.org/abs/2301.08113v1", "pdf_url": "http://arxiv.org/pdf/2301.08113v1", "title": "Soft Thresholding for Visual Image Enhancement", "abs": "Thresholding converts a greyscale image into a binary image, and is thus often a necessary segmentation step in image processing. For a human viewer however, thresholding usually has a negative impact on the legibility of document images. This report describes a simple method for \"smearing out\" the threshold and transforming the greyscale image into a different greyscale image. The method is similar to fuzzy thresholding, but is discussed here in the simpler context of greyscale transformations and, unlike fuzzy thresholding, it is independent from the method for finding the threshold. A simple formula is presented for automatically determining the width of the threshold spread. The method can be used, e.g., for enhancing images for the presentation in online facsimile repositories.", "authors": "Christoph Dalitz", "first_author": "Christoph Dalitz", "first_end_author": "Christoph Dalitz; Christoph Dalitz", "publish_time": "2023-01-19 15:05:13+00:00", "update_time": "2023-01-19 15:05:13+00:00"}, {"id": "2301.08092", "url": "http://arxiv.org/abs/2301.08092v1", "pdf_url": "http://arxiv.org/pdf/2301.08092v1", "title": "RNAS-CL: Robust Neural Architecture Search by Cross-Layer Knowledge   Distillation", "abs": "Deep Neural Networks are vulnerable to adversarial attacks. Neural Architecture Search (NAS), one of the driving tools of deep neural networks, demonstrates superior performance in prediction accuracy in various machine learning applications. However, it is unclear how it performs against adversarial attacks. Given the presence of a robust teacher, it would be interesting to investigate if NAS would produce robust neural architecture by inheriting robustness from the teacher. In this paper, we propose Robust Neural Architecture Search by Cross-Layer Knowledge Distillation (RNAS-CL), a novel NAS algorithm that improves the robustness of NAS by learning from a robust teacher through cross-layer knowledge distillation. Unlike previous knowledge distillation methods that encourage close student/teacher output only in the last layer, RNAS-CL automatically searches for the best teacher layer to supervise each student layer. Experimental result evidences the effectiveness of RNAS-CL and shows that RNAS-CL produces small and robust neural architecture.", "authors": "Utkarsh Nath, Yancheng Wang, Yingzhen Yang", "first_author": "Yingzhen Yang", "first_end_author": "Utkarsh Nath; Yingzhen Yang", "publish_time": "2023-01-19 14:22:44+00:00", "update_time": "2023-01-19 14:22:44+00:00"}, {"id": "2210.11817", "url": "http://arxiv.org/abs/2210.11817v2", "pdf_url": "http://arxiv.org/pdf/2210.11817v2", "title": "Motion Matters: A Novel Motion Modeling For Cross-View Gait Feature   Learning", "abs": "As a unique biometric that can be perceived at a distance, gait has broad applications in person authentication, social security, and so on. Existing gait recognition methods suffer from changes in viewpoint and clothing and barely consider extracting diverse motion features, a fundamental characteristic in gaits, from gait sequences. This paper proposes a novel motion modeling method to extract the discriminative and robust representation. Specifically, we first extract the motion features from the encoded motion sequences in the shallow layer. Then we continuously enhance the motion feature in deep layers. This motion modeling approach is independent of mainstream work in building network architectures. As a result, one can apply this motion modeling method to any backbone to improve gait recognition performance. In this paper, we combine motion modeling with one commonly used backbone~(GaitGL) as GaitGL-M to illustrate motion modeling. Extensive experimental results on two commonly-used cross-view gait datasets demonstrate the superior performance of GaitGL-M over existing state-of-the-art methods.", "authors": "Jingqi Li, Jiaqi Gao, Yuzhen Zhang, Hongming Shan, Junping Zhang", "first_author": "Junping Zhang", "first_end_author": "Jingqi Li; Junping Zhang", "publish_time": "2022-10-21 08:42:00+00:00", "update_time": "2023-01-19 14:11:52+00:00"}, {"id": "2301.08072", "url": "http://arxiv.org/abs/2301.08072v1", "pdf_url": "http://arxiv.org/pdf/2301.08072v1", "title": "Dif-Fusion: Towards High Color Fidelity in Infrared and Visible Image   Fusion with Diffusion Models", "abs": "Color plays an important role in human visual perception, reflecting the spectrum of objects. However, the existing infrared and visible image fusion methods rarely explore how to handle multi-spectral/channel data directly and achieve high color fidelity. This paper addresses the above issue by proposing a novel method with diffusion models, termed as Dif-Fusion, to generate the distribution of the multi-channel input data, which increases the ability of multi-source information aggregation and the fidelity of colors. In specific, instead of converting multi-channel images into single-channel data in existing fusion methods, we create the multi-channel data distribution with a denoising network in a latent space with forward and reverse diffusion process. Then, we use the the denoising network to extract the multi-channel diffusion features with both visible and infrared information. Finally, we feed the multi-channel diffusion features to the multi-channel fusion module to directly generate the three-channel fused image. To retain the texture and intensity information, we propose multi-channel gradient loss and intensity loss. Along with the current evaluation metrics for measuring texture and intensity fidelity, we introduce a new evaluation metric to quantify color fidelity. Extensive experiments indicate that our method is more effective than other state-of-the-art image fusion methods, especially in color fidelity.", "authors": "Jun Yue, Leyuan Fang, Shaobo Xia, Yue Deng, Jiayi Ma", "first_author": "Jiayi Ma", "first_end_author": "Jun Yue; Jiayi Ma", "publish_time": "2023-01-19 13:37:19+00:00", "update_time": "2023-01-19 13:37:19+00:00"}, {"id": "2301.08067", "url": "http://arxiv.org/abs/2301.08067v1", "pdf_url": "http://arxiv.org/pdf/2301.08067v1", "title": "Interpreting CNN Predictions using Conditional Generative Adversarial   Networks", "abs": "We propose a novel method that trains a conditional Generative Adversarial Network (GAN) to generate visual interpretations of a Convolutional Neural Network (CNN). To comprehend a CNN, the GAN is trained with information on how the CNN processes an image when making predictions. Supplying that information has two main challenges: how to represent this information in a form that is feedable to the GANs and how to effectively feed the representation to the GAN. To address these issues, we developed a suitable representation of CNN architectures by cumulatively averaging intermediate interpretation maps. We also propose two alternative approaches to feed the representations to the GAN and to choose an effective training strategy. Our approach learned the general aspects of CNNs and was agnostic to datasets and CNN architectures. The study includes both qualitative and quantitative evaluations and compares the proposed GANs with state-of-the-art approaches. We found that the initial layers of CNNs and final layers are equally crucial for interpreting CNNs upon interpreting the proposed GAN. We believe training a GAN to interpret CNNs would open doors for improved interpretations by leveraging fast-paced deep learning advancements. The code used for experimentation is publicly available at https://github.com/Akash-guna/Explain-CNN-With-GANS", "authors": "Akash Guna R T, Raul Benitez, Sikha O K", "first_author": "Sikha O K", "first_end_author": "Akash Guna R T; Sikha O K", "publish_time": "2023-01-19 13:26:12+00:00", "update_time": "2023-01-19 13:26:12+00:00"}, {"id": "2301.08064", "url": "http://arxiv.org/abs/2301.08064v1", "pdf_url": "http://arxiv.org/pdf/2301.08064v1", "title": "Position Regression for Unsupervised Anomaly Detection", "abs": "In recent years, anomaly detection has become an essential field in medical image analysis. Most current anomaly detection methods for medical images are based on image reconstruction. In this work, we propose a novel anomaly detection approach based on coordinate regression. Our method estimates the position of patches within a volume, and is trained only on data of healthy subjects. During inference, we can detect and localize anomalies by considering the error of the position estimate of a given patch. We apply our method to 3D CT volumes and evaluate it on patients with intracranial haemorrhages and cranial fractures. The results show that our method performs well in detecting these anomalies. Furthermore, we show that our method requires less memory than comparable approaches that involve image reconstruction. This is highly relevant for processing large 3D volumes, for instance, CT or MRI scans.", "authors": "Florentin Bieder, Julia Wolleb, Robin Sandk\u00fchler, Philippe C. Cattin", "first_author": "Philippe C. Cattin", "first_end_author": "Florentin Bieder; Philippe C. Cattin", "publish_time": "2023-01-19 13:22:11+00:00", "update_time": "2023-01-19 13:22:11+00:00"}, {"id": "2301.08044", "url": "http://arxiv.org/abs/2301.08044v1", "pdf_url": "http://arxiv.org/pdf/2301.08044v1", "title": "Reference Guided Image Inpainting using Facial Attributes", "abs": "Image inpainting is a technique of completing missing pixels such as occluded region restoration, distracting objects removal, and facial completion. Among these inpainting tasks, facial completion algorithm performs face inpainting according to the user direction. Existing approaches require delicate and well controlled input by the user, thus it is difficult for an average user to provide the guidance sufficiently accurate for the algorithm to generate desired results. To overcome this limitation, we propose an alternative user-guided inpainting architecture that manipulates facial attributes using a single reference image as the guide. Our end-to-end model consists of attribute extractors for accurate reference image attribute transfer and an inpainting model to map the attributes realistically and accurately to generated images. We customize MS-SSIM loss and learnable bidirectional attention maps in which importance structures remain intact even with irregular shaped masks. Based on our evaluation using the publicly available dataset CelebA-HQ, we demonstrate that the proposed method delivers superior performance compared to some state-of-the-art methods specialized in inpainting tasks.", "authors": "Dongsik Yoon, Jeonggi Kwak, Yuanming Li, David Han, Youngsaeng Jin, Hanseok Ko", "first_author": "Hanseok Ko", "first_end_author": "Dongsik Yoon; Hanseok Ko", "publish_time": "2023-01-19 12:39:08+00:00", "update_time": "2023-01-19 12:39:08+00:00"}, {"id": "2108.06230", "url": "http://arxiv.org/abs/2108.06230v5", "pdf_url": "http://arxiv.org/pdf/2108.06230v5", "title": "Generative Zero-Shot Learning for Semantic Segmentation of 3D Point   Clouds", "abs": "While there has been a number of studies on Zero-Shot Learning (ZSL) for 2D images, its application to 3D data is still recent and scarce, with just a few methods limited to classification. We present the first generative approach for both ZSL and Generalized ZSL (GZSL) on 3D data, that can handle both classification and, for the first time, semantic segmentation. We show that it reaches or outperforms the state of the art on ModelNet40 classification for both inductive ZSL and inductive GZSL. For semantic segmentation, we created three benchmarks for evaluating this new ZSL task, using S3DIS, ScanNet and SemanticKITTI. Our experiments show that our method outperforms strong baselines, which we additionally propose for this task.", "authors": "Bj\u00f6rn Michele, Alexandre Boulch, Gilles Puy, Maxime Bucher, Renaud Marlet", "first_author": "Renaud Marlet", "first_end_author": "Bj\u00f6rn Michele; Renaud Marlet", "publish_time": "2021-08-13 13:29:27+00:00", "update_time": "2023-01-19 11:58:47+00:00"}, {"id": "2301.08252", "url": "http://arxiv.org/abs/2301.08252v1", "pdf_url": "http://arxiv.org/pdf/2301.08252v1", "title": "Evaluation of the potential of Near Infrared Hyperspectral Imaging for   monitoring the invasive brown marmorated stink bug", "abs": "The brown marmorated stink bug (BMSB), Halyomorpha halys, is an invasive insect pest of global importance that damages several crops, compromising agri-food production. Field monitoring procedures are fundamental to perform risk assessment operations, in order to promptly face crop infestations and avoid economical losses. To improve pest management, spectral cameras mounted on Unmanned Aerial Vehicles (UAVs) and other Internet of Things (IoT) devices, such as smart traps or unmanned ground vehicles, could be used as an innovative technology allowing fast, efficient and real-time monitoring of insect infestations. The present study consists in a preliminary evaluation at the laboratory level of Near Infrared Hyperspectral Imaging (NIR-HSI) as a possible technology to detect BMSB specimens on different vegetal backgrounds, overcoming the problem of BMSB mimicry. Hyperspectral images of BMSB were acquired in the 980-1660 nm range, considering different vegetal backgrounds selected to mimic a real field application scene. Classification models were obtained following two different chemometric approaches. The first approach was focused on modelling spectral information and selecting relevant spectral regions for discrimination by means of sparse-based variable selection coupled with Soft Partial Least Squares Discriminant Analysis (s-Soft PLS-DA) classification algorithm. The second approach was based on modelling spatial and spectral features contained in the hyperspectral images using Convolutional Neural Networks (CNN). Finally, to further improve BMSB detection ability, the two strategies were merged, considering only the spectral regions selected by s-Soft PLS-DA for CNN modelling.", "authors": "Veronica Ferrari, Rosalba Calvini, Bas Boom, Camilla Menozzi, Aravind Krishnaswamy Rangarajan, Lara Maistrello, Peter Offermans, Alessandro Ulrici", "first_author": "Alessandro Ulrici", "first_end_author": "Veronica Ferrari; Alessandro Ulrici", "publish_time": "2023-01-19 11:37:20+00:00", "update_time": "2023-01-19 11:37:20+00:00"}, {"id": "2301.08555", "url": "http://arxiv.org/abs/2301.08555v1", "pdf_url": "http://arxiv.org/pdf/2301.08555v1", "title": "Hybrid Open-set Segmentation with Synthetic Negative Data", "abs": "Open-set segmentation is often conceived by complementing closed-set classification with anomaly detection. Existing dense anomaly detectors operate either through generative modelling of regular training data or by discriminating with respect to negative training data. These two approaches optimize different objectives and therefore exhibit different failure modes. Consequently, we propose the first dense hybrid anomaly score that fuses generative and discriminative cues. The proposed score can be efficiently implemented by upgrading any semantic segmentation model with translation-equivariant estimates of data likelihood and dataset posterior. Our design is a remarkably good fit for efficient inference on large images due to negligible computational overhead over the closed-set baseline. The resulting dense hybrid open-set models require negative training images that can be sampled either from an auxiliary negative dataset or from a jointly trained generative model. We evaluate our contributions on benchmarks for dense anomaly detection and open-set segmentation of traffic scenes. The experiments reveal strong open-set performance in spite of negligible computational overhead.", "authors": "Matej Grci\u0107, Sini\u0161a \u0160egvi\u0107", "first_author": "Sini\u0161a \u0160egvi\u0107", "first_end_author": "Matej Grci\u0107; Sini\u0161a \u0160egvi\u0107", "publish_time": "2023-01-19 11:02:44+00:00", "update_time": "2023-01-19 11:02:44+00:00"}, {"id": "2301.00986", "url": "http://arxiv.org/abs/2301.00986v2", "pdf_url": "http://arxiv.org/pdf/2301.00986v2", "title": "Look, Listen, and Attack: Backdoor Attacks Against Video Action   Recognition", "abs": "Deep neural networks (DNNs) are vulnerable to a class of attacks called \"backdoor attacks\", which create an association between a backdoor trigger and a target label the attacker is interested in exploiting. A backdoored DNN performs well on clean test images, yet persistently predicts an attacker-defined label for any sample in the presence of the backdoor trigger. Although backdoor attacks have been extensively studied in the image domain, there are very few works that explore such attacks in the video domain, and they tend to conclude that image backdoor attacks are less effective in the video domain. In this work, we revisit the traditional backdoor threat model and incorporate additional video-related aspects to that model. We show that poisoned-label image backdoor attacks could be extended temporally in two ways, statically and dynamically, leading to highly effective attacks in the video domain. In addition, we explore natural video backdoors to highlight the seriousness of this vulnerability in the video domain. And, for the first time, we study multi-modal (audiovisual) backdoor attacks against video action recognition models, where we show that attacking a single modality is enough for achieving a high attack success rate.", "authors": "Hasan Abed Al Kader Hammoud, Shuming Liu, Mohammed Alkhrashi, Fahad AlBalawi, Bernard Ghanem", "first_author": "Bernard Ghanem", "first_end_author": "Hasan Abed Al Kader Hammoud; Bernard Ghanem", "publish_time": "2023-01-03 07:40:28+00:00", "update_time": "2023-01-19 11:02:33+00:00"}, {"id": "2212.10957", "url": "http://arxiv.org/abs/2212.10957v2", "pdf_url": "http://arxiv.org/pdf/2212.10957v2", "title": "TruFor: Leveraging all-round clues for trustworthy image forgery   detection and localization", "abs": "In this paper we present TruFor, a forensic framework that can be applied to a large variety of image manipulation methods, from classic cheapfakes to more recent manipulations based on deep learning. We rely on the extraction of both high-level and low-level traces through a transformer-based fusion architecture that combines the RGB image and a learned noise-sensitive fingerprint. The latter learns to embed the artifacts related to the camera internal and external processing by training only on real data in a self-supervised manner. Forgeries are detected as deviations from the expected regular pattern that characterizes each pristine image. Looking for anomalies makes the approach able to robustly detect a variety of local manipulations, ensuring generalization. In addition to a pixel-level localization map and a whole-image integrity score, our approach outputs a reliability map that highlights areas where localization predictions may be error-prone. This is particularly important in forensic applications in order to reduce false alarms and allow for a large scale analysis. Extensive experiments on several datasets show that our method is able to reliably detect and localize both cheapfakes and deepfakes manipulations outperforming state-of-the-art works. Code will be publicly available at https://grip-unina.github.io/TruFor/", "authors": "Fabrizio Guillaro, Davide Cozzolino, Avneesh Sud, Nicholas Dufour, Luisa Verdoliva", "first_author": "Luisa Verdoliva", "first_end_author": "Fabrizio Guillaro; Luisa Verdoliva", "publish_time": "2022-12-21 11:49:43+00:00", "update_time": "2023-01-19 10:26:38+00:00"}, {"id": "2301.07969", "url": "http://arxiv.org/abs/2301.07969v1", "pdf_url": "http://arxiv.org/pdf/2301.07969v1", "title": "Fast Inference in Denoising Diffusion Models via MMD Finetuning", "abs": "Denoising Diffusion Models (DDMs) have become a popular tool for generating high-quality samples from complex data distributions. These models are able to capture sophisticated patterns and structures in the data, and can generate samples that are highly diverse and representative of the underlying distribution. However, one of the main limitations of diffusion models is the complexity of sample generation, since a large number of inference timesteps is required to faithfully capture the data distribution. In this paper, we present MMD-DDM, a novel method for fast sampling of diffusion models. Our approach is based on the idea of using the Maximum Mean Discrepancy (MMD) to finetune the learned distribution with a given budget of timesteps. This allows the finetuned model to significantly improve the speed-quality trade-off, by substantially increasing fidelity in inference regimes with few steps or, equivalently, by reducing the required number of steps to reach a target fidelity, thus paving the way for a more practical adoption of diffusion models in a wide range of applications. We evaluate our approach on unconditional image generation with extensive experiments across the CIFAR-10, CelebA, ImageNet and LSUN-Church datasets. Our findings show that the proposed method is able to produce high-quality samples in a fraction of the time required by widely-used diffusion models, and outperforms state-of-the-art techniques for accelerated sampling. Code is available at: https://github.com/diegovalsesia/MMD-DDM.", "authors": "Emanuele Aiello, Diego Valsesia, Enrico Magli", "first_author": "Enrico Magli", "first_end_author": "Emanuele Aiello; Enrico Magli", "publish_time": "2023-01-19 09:48:07+00:00", "update_time": "2023-01-19 09:48:07+00:00"}]}]
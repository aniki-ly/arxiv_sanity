[{"date": "2023.01.24", "papers": [{"id": "2301.09637", "url": "http://arxiv.org/abs/2301.09637v1", "pdf_url": "http://arxiv.org/pdf/2301.09637v1", "title": "InfiniCity: Infinite-Scale City Synthesis", "abs": "Toward infinite-scale 3D city synthesis, we propose a novel framework, InfiniCity, which constructs and renders an unconstrainedly large and 3D-grounded environment from random noises. InfiniCity decomposes the seemingly impractical task into three feasible modules, taking advantage of both 2D and 3D data. First, an infinite-pixel image synthesis module generates arbitrary-scale 2D maps from the bird's-eye view. Next, an octree-based voxel completion module lifts the generated 2D map to 3D octrees. Finally, a voxel-based neural rendering module texturizes the voxels and renders 2D images. InfiniCity can thus synthesize arbitrary-scale and traversable 3D city environments, and allow flexible and interactive editing from users. We quantitatively and qualitatively demonstrate the efficacy of the proposed framework. Project page: https://hubert0527.github.io/infinicity/", "authors": "Chieh Hubert Lin, Hsin-Ying Lee, Willi Menapace, Menglei Chai, Aliaksandr Siarohin, Ming-Hsuan Yang, Sergey Tulyakov", "first_author": "Sergey Tulyakov", "first_end_author": "Chieh Hubert Lin; Sergey Tulyakov", "publish_time": "2023-01-23 18:59:59+00:00", "update_time": "2023-01-23 18:59:59+00:00"}, {"id": "2211.13756", "url": "http://arxiv.org/abs/2211.13756v2", "pdf_url": "http://arxiv.org/pdf/2211.13756v2", "title": "Contrastive pretraining for semantic segmentation is robust to noisy   positive pairs", "abs": "Domain-specific variants of contrastive learning can construct positive pairs from two distinct in-domain images, while traditional methods just augment the same image twice. For example, we can form a positive pair from two satellite images showing the same location at different times. Ideally, this teaches the model to ignore changes caused by seasons, weather conditions or image acquisition artifacts. However, unlike in traditional contrastive methods, this can result in undesired positive pairs, since we form them without human supervision. For example, a positive pair might consist of one image before a disaster and one after. This could teach the model to ignore the differences between intact and damaged buildings, which might be what we want to detect in the downstream task. Similar to false negative pairs, this could impede model performance. Crucially, in this setting only parts of the images differ in relevant ways, while other parts remain similar. Surprisingly, we find that downstream semantic segmentation is either robust to such badly matched pairs or even benefits from them. The experiments are conducted on the remote sensing dataset xBD, and a synthetic segmentation dataset for which we have full control over the pairing conditions. As a result, practitioners can use these domain-specific contrastive methods without having to filter their positive pairs beforehand, or might even be encouraged to purposefully include such pairs in their pretraining dataset.", "authors": "Sebastian Gerard, Josephine Sullivan", "first_author": "Josephine Sullivan", "first_end_author": "Sebastian Gerard; Josephine Sullivan", "publish_time": "2022-11-24 18:59:01+00:00", "update_time": "2023-01-23 18:59:54+00:00"}, {"id": "2301.09632", "url": "http://arxiv.org/abs/2301.09632v1", "pdf_url": "http://arxiv.org/pdf/2301.09632v1", "title": "HexPlane: A Fast Representation for Dynamic Scenes", "abs": "Modeling and re-rendering dynamic 3D scenes is a challenging task in 3D vision. Prior approaches build on NeRF and rely on implicit representations. This is slow since it requires many MLP evaluations, constraining real-world applications. We show that dynamic 3D scenes can be explicitly represented by six planes of learned features, leading to an elegant solution we call HexPlane. A HexPlane computes features for points in spacetime by fusing vectors extracted from each plane, which is highly efficient. Pairing a HexPlane with a tiny MLP to regress output colors and training via volume rendering gives impressive results for novel view synthesis on dynamic scenes, matching the image quality of prior work but reducing training time by more than $100\\times$. Extensive ablations confirm our HexPlane design and show that it is robust to different feature fusion mechanisms, coordinate systems, and decoding mechanisms. HexPlanes are a simple and effective solution for representing 4D volumes, and we hope they can broadly contribute to modeling spacetime for dynamic 3D scenes.", "authors": "Ang Cao, Justin Johnson", "first_author": "Justin Johnson", "first_end_author": "Ang Cao; Justin Johnson", "publish_time": "2023-01-23 18:59:25+00:00", "update_time": "2023-01-23 18:59:25+00:00"}, {"id": "2301.09629", "url": "http://arxiv.org/abs/2301.09629v1", "pdf_url": "http://arxiv.org/pdf/2301.09629v1", "title": "LEGO-Net: Learning Regular Rearrangements of Objects in Rooms", "abs": "Humans universally dislike the task of cleaning up a messy room. If machines were to help us with this task, they must understand human criteria for regular arrangements, such as several types of symmetry, co-linearity or co-circularity, spacing uniformity in linear or circular patterns, and further inter-object relationships that relate to style and functionality. Previous approaches for this task relied on human input to explicitly specify goal state, or synthesized scenes from scratch -- but such methods do not address the rearrangement of existing messy scenes without providing a goal state. In this paper, we present LEGO-Net, a data-driven transformer-based iterative method for learning regular rearrangement of objects in messy rooms. LEGO-Net is partly inspired by diffusion models -- it starts with an initial messy state and iteratively \"de-noises'' the position and orientation of objects to a regular state while reducing the distance traveled. Given randomly perturbed object positions and orientations in an existing dataset of professionally-arranged scenes, our method is trained to recover a regular re-arrangement. Results demonstrate that our method is able to reliably rearrange room scenes and outperform other methods. We additionally propose a metric for evaluating regularity in room arrangements using number-theoretic machinery.", "authors": "Qiuhong Anna Wei, Sijie Ding, Jeong Joon Park, Rahul Sajnani, Adrien Poulenard, Srinath Sridhar, Leonidas Guibas", "first_author": "Leonidas Guibas", "first_end_author": "Qiuhong Anna Wei; Leonidas Guibas", "publish_time": "2023-01-23 18:58:02+00:00", "update_time": "2023-01-23 18:58:02+00:00"}, {"id": "2206.08312", "url": "http://arxiv.org/abs/2206.08312v2", "pdf_url": "http://arxiv.org/pdf/2206.08312v2", "title": "SoundSpaces 2.0: A Simulation Platform for Visual-Acoustic Learning", "abs": "We introduce SoundSpaces 2.0, a platform for on-the-fly geometry-based audio rendering for 3D environments. Given a 3D mesh of a real-world environment, SoundSpaces can generate highly realistic acoustics for arbitrary sounds captured from arbitrary microphone locations. Together with existing 3D visual assets, it supports an array of audio-visual research tasks, such as audio-visual navigation, mapping, source localization and separation, and acoustic matching. Compared to existing resources, SoundSpaces 2.0 has the advantages of allowing continuous spatial sampling, generalization to novel environments, and configurable microphone and material properties. To our knowledge, this is the first geometry-based acoustic simulation that offers high fidelity and realism while also being fast enough to use for embodied learning. We showcase the simulator's properties and benchmark its performance against real-world audio measurements. In addition, we demonstrate two downstream tasks -- embodied navigation and far-field automatic speech recognition -- and highlight sim2real performance for the latter. SoundSpaces 2.0 is publicly available to facilitate wider research for perceptual systems that can both see and hear.", "authors": "Changan Chen, Carl Schissler, Sanchit Garg, Philip Kobernik, Alexander Clegg, Paul Calamia, Dhruv Batra, Philip W Robinson, Kristen Grauman", "first_author": "Kristen Grauman", "first_end_author": "Changan Chen; Kristen Grauman", "publish_time": "2022-06-16 17:17:44+00:00", "update_time": "2023-01-23 18:49:47+00:00"}, {"id": "2301.09624", "url": "http://arxiv.org/abs/2301.09624v1", "pdf_url": "http://arxiv.org/pdf/2301.09624v1", "title": "Maximum Mean Discrepancy Kernels for Predictive and Prognostic Modeling   of Whole Slide Images", "abs": "How similar are two images? In computational pathology, where Whole Slide Images (WSIs) of digitally scanned tissue samples from patients can be multi-gigapixels in size, determination of degree of similarity between two WSIs is a challenging task with a number of practical applications. In this work, we explore a novel strategy based on kernelized Maximum Mean Discrepancy (MMD) analysis for determination of pairwise similarity between WSIs. The proposed approach works by calculating MMD between two WSIs using kernels over deep features of image patches. This allows representation of an entire dataset of WSIs as a kernel matrix for WSI level clustering, weakly-supervised prediction of TP-53 mutation status in breast cancer patients from their routine WSIs as well as survival analysis with state of the art prediction performance. We believe that this work will open up further avenues for application of WSI-level kernels for predictive and prognostic tasks in computational pathology.", "authors": "Piotr Keller, Muhammad Dawood, Fayyaz ul Amir Afsar Minhas", "first_author": "Fayyaz ul Amir Afsar Minhas", "first_end_author": "Piotr Keller; Fayyaz ul Amir Afsar Minhas", "publish_time": "2023-01-23 18:47:41+00:00", "update_time": "2023-01-23 18:47:41+00:00"}, {"id": "2301.09620", "url": "http://arxiv.org/abs/2301.09620v1", "pdf_url": "http://arxiv.org/pdf/2301.09620v1", "title": "Tracking the industrial growth of modern China with high-resolution   panchromatic imagery: A sequential convolutional approach", "abs": "Due to insufficient or difficult to obtain data on development in inaccessible regions, remote sensing data is an important tool for interested stakeholders to collect information on economic growth. To date, no studies have utilized deep learning to estimate industrial growth at the level of individual sites. In this study, we harness high-resolution panchromatic imagery to estimate development over time at 419 industrial sites in the People's Republic of China using a multi-tier computer vision framework. We present two methods for approximating development: (1) structural area coverage estimated through a Mask R-CNN segmentation algorithm, and (2) imputing development directly with visible & infrared radiance from the Visible Infrared Imaging Radiometer Suite (VIIRS). Labels generated from these methods are comparatively evaluated and tested. On a dataset of 2,078 50 cm resolution images spanning 19 years, the results indicate that two dimensions of industrial development can be estimated using high-resolution daytime imagery, including (a) the total square meters of industrial development (average error of 0.021 $\\textrm{km}^2$), and (b) the radiance of lights (average error of 9.8 $\\mathrm{\\frac{nW}{cm^{2}sr}}$). Trend analysis of the techniques reveal estimates from a Mask R-CNN-labeled CNN-LSTM track ground truth measurements most closely. The Mask R-CNN estimates positive growth at every site from the oldest image to the most recent, with an average change of 4,084 $\\textrm{m}^2$.", "authors": "Ethan Brewer, Zhonghui Lv, Dan Runfola", "first_author": "Dan Runfola", "first_end_author": "Ethan Brewer; Dan Runfola", "publish_time": "2023-01-23 18:40:21+00:00", "update_time": "2023-01-23 18:40:21+00:00"}, {"id": "2301.09617", "url": "http://arxiv.org/abs/2301.09617v1", "pdf_url": "http://arxiv.org/pdf/2301.09617v1", "title": "Fully transformer-based biomarker prediction from colorectal cancer   histology: a large-scale multicentric study", "abs": "Background: Deep learning (DL) can extract predictive and prognostic biomarkers from routine pathology slides in colorectal cancer. For example, a DL test for the diagnosis of microsatellite instability (MSI) in CRC has been approved in 2022. Current approaches rely on convolutional neural networks (CNNs). Transformer networks are outperforming CNNs and are replacing them in many applications, but have not been used for biomarker prediction in cancer at a large scale. In addition, most DL approaches have been trained on small patient cohorts, which limits their clinical utility. Methods: In this study, we developed a new fully transformer-based pipeline for end-to-end biomarker prediction from pathology slides. We combine a pre-trained transformer encoder and a transformer network for patch aggregation, capable of yielding single and multi-target prediction at patient level. We train our pipeline on over 9,000 patients from 10 colorectal cancer cohorts. Results: A fully transformer-based approach massively improves the performance, generalizability, data efficiency, and interpretability as compared with current state-of-the-art algorithms. After training on a large multicenter cohort, we achieve a sensitivity of 0.97 with a negative predictive value of 0.99 for MSI prediction on surgical resection specimens. We demonstrate for the first time that resection specimen-only training reaches clinical-grade performance on endoscopic biopsy tissue, solving a long-standing diagnostic problem. Interpretation: A fully transformer-based end-to-end pipeline trained on thousands of pathology slides yields clinical-grade performance for biomarker prediction on surgical resections and biopsies. Our new methods are freely available under an open source license.", "authors": "Sophia J. Wagner, Daniel Reisenb\u00fcchler, Nicholas P. West, Jan Moritz Niehues, Gregory Patrick Veldhuizen, Philip Quirke, Heike I. Grabsch, Piet A. van den Brandt, Gordon G. A. Hutchins, Susan D. Richman, Tanwei Yuan, Rupert Langer, Josien Christina Anna Jenniskens, Kelly Offermans, Wolfram Mueller, Richard Gray, Stephen B. Gruber, Joel K. Greenson, Gad Rennert, Joseph D. Bonner, Daniel Schmolze, Jacqueline A. James, Maurice B. Loughrey, Manuel Salto-Tellez, Hermann Brenner, Michael Hoffmeister, Daniel Truhn, Julia A. Schnabel, Melanie Boxberg, Tingying Peng, Jakob Nikolas Kather", "first_author": "Jakob Nikolas Kather", "first_end_author": "Sophia J. Wagner; Jakob Nikolas Kather", "publish_time": "2023-01-23 18:33:38+00:00", "update_time": "2023-01-23 18:33:38+00:00"}, {"id": "2301.09602", "url": "http://arxiv.org/abs/2301.09602v1", "pdf_url": "http://arxiv.org/pdf/2301.09602v1", "title": "Adapting the Hypersphere Loss Function from Anomaly Detection to Anomaly   Segmentation", "abs": "We propose an incremental improvement to Fully Convolutional Data Description (FCDD), an adaptation of the one-class classification approach from anomaly detection to image anomaly segmentation (a.k.a. anomaly localization). We analyze its original loss function and propose a substitute that better resembles its predecessor, the Hypersphere Classifier (HSC). Both are compared on the MVTec Anomaly Detection Dataset (MVTec-AD) -- training images are flawless objects/textures and the goal is to segment unseen defects -- showing that consistent improvement is achieved by better designing the pixel-wise supervision.", "authors": "Joao P. C. Bertoldo, Santiago Velasco-Forero, Jesus Angulo, Etienne Decenci\u00e8re", "first_author": "Etienne Decenci\u00e8re", "first_end_author": "Joao P. C. Bertoldo; Etienne Decenci\u00e8re", "publish_time": "2023-01-23 18:06:35+00:00", "update_time": "2023-01-23 18:06:35+00:00"}, {"id": "2301.09595", "url": "http://arxiv.org/abs/2301.09595v1", "pdf_url": "http://arxiv.org/pdf/2301.09595v1", "title": "Zorro: the masked multimodal transformer", "abs": "Attention-based models are appealing for multimodal processing because inputs from multiple modalities can be concatenated and fed to a single backbone network - thus requiring very little fusion engineering. The resulting representations are however fully entangled throughout the network, which may not always be desirable: in learning, contrastive audio-visual self-supervised learning requires independent audio and visual features to operate, otherwise learning collapses; in inference, evaluation of audio-visual models should be possible on benchmarks having just audio or just video. In this paper, we introduce Zorro, a technique that uses masks to control how inputs from each modality are routed inside Transformers, keeping some parts of the representation modality-pure. We apply this technique to three popular transformer-based architectures (ViT, Swin and HiP) and show that with contrastive pre-training Zorro achieves state-of-the-art results on most relevant benchmarks for multimodal tasks (AudioSet and VGGSound). Furthermore, the resulting models are able to perform unimodal inference on both video and audio benchmarks such as Kinetics-400 or ESC-50.", "authors": "Adri\u00e0 Recasens, Jason Lin, Jo\u0101o Carreira, Drew Jaegle, Luyu Wang, Jean-baptiste Alayrac, Pauline Luc, Antoine Miech, Lucas Smaira, Ross Hemsley, Andrew Zisserman", "first_author": "Andrew Zisserman", "first_end_author": "Adri\u00e0 Recasens; Andrew Zisserman", "publish_time": "2023-01-23 17:51:39+00:00", "update_time": "2023-01-23 17:51:39+00:00"}, {"id": "2301.08874", "url": "http://arxiv.org/abs/2301.08874v1", "pdf_url": "http://arxiv.org/pdf/2301.08874v1", "title": "Improving Accuracy of Zero-Shot Action Recognition with Handcrafted   Features", "abs": "With the development of machine learning, datasets for models are getting increasingly larger. This leads to increased data annotation costs and training time, which undoubtedly hinders the development of machine learning. To solve this problem, zero-shot learning is gaining considerable attention. With zero-shot learning, objects can be recognized or classified, even without having been seen before. Nevertheless, the accuracy of this method is still low, thus limiting its practical application. To solve this problem, we propose a video-text matching model, which can learn from handcrafted features. Our model can be used alone to predict the action classes and can also be added to any other model to improve its accuracy. Moreover, our model can be continuously optimized to improve its accuracy. We only need to manually annotate some features, which incurs some labor costs; in many situations, the costs are worth it. The results with UCF101 and HMDB51 show that our model achieves the best accuracy and also improves the accuracies of other models.", "authors": "Nan Wu, Hiroshi Kera, Kazuhiko Kawamoto", "first_author": "Kazuhiko Kawamoto", "first_end_author": "Nan Wu; Kazuhiko Kawamoto", "publish_time": "2023-01-21 03:41:07+00:00", "update_time": "2023-01-21 03:41:07+00:00"}, {"id": "2301.08868", "url": "http://arxiv.org/abs/2301.08868v1", "pdf_url": "http://arxiv.org/pdf/2301.08868v1", "title": "Dynamic MLP for MRI Reconstruction", "abs": "As convolutional neural networks (CNN) become the most successful reconstruction technique for accelerated Magnetic Resonance Imaging (MRI), CNN reaches its limit on image quality especially in sharpness. Further improvement on image quality often comes at massive computational costs, hindering their practicability in the clinic setting. MRI reconstruction is essentially a deconvolution problem, which demands long-distance information that is difficult to be captured by CNNs with small convolution kernels. The multi-layer perceptron (MLP) is able to model such long-distance information, but it restricts a fixed input size while the reconstruction of images in flexible resolutions is required in the clinic setting. In this paper, we proposed a hybrid CNN and MLP reconstruction strategy, featured by dynamic MLP (dMLP) that accepts arbitrary image sizes. Experiments were conducted using 3D multi-coil MRI. Our results suggested the proposed dMLP can improve image sharpness compared to its pure CNN counterpart, while costing minor additional GPU memory and computation time. We further compared the proposed dMLP with CNNs using large kernels and studied pure MLP-based reconstruction using a stack of 1D dMLPs, as well as its CNN counterpart using only 1D convolutions. We observed the enlarged receptive field has noticeably improved image quality, while simply using CNN with a large kernel leads to difficulties in training. Noticeably, the pure MLP-based method has been outperformed by CNN-involved methods, which matches the observations in other computer vision tasks for natural images.", "authors": "Chi Zhang, Eric Z. Chen, Xiao Chen, Yikang Liu, Terrence Chen, Shanhui Sun", "first_author": "Shanhui Sun", "first_end_author": "Chi Zhang; Shanhui Sun", "publish_time": "2023-01-21 02:58:51+00:00", "update_time": "2023-01-21 02:58:51+00:00"}, {"id": "2210.05861", "url": "http://arxiv.org/abs/2210.05861v2", "pdf_url": "http://arxiv.org/pdf/2210.05861v2", "title": "SlotFormer: Unsupervised Visual Dynamics Simulation with Object-Centric   Models", "abs": "Understanding dynamics from visual observations is a challenging problem that requires disentangling individual objects from the scene and learning their interactions. While recent object-centric models can successfully decompose a scene into objects, modeling their dynamics effectively still remains a challenge. We address this problem by introducing SlotFormer -- a Transformer-based autoregressive model operating on learned object-centric representations. Given a video clip, our approach reasons over object features to model spatio-temporal relationships and predicts accurate future object states. In this paper, we successfully apply SlotFormer to perform video prediction on datasets with complex object interactions. Moreover, the unsupervised SlotFormer's dynamics model can be used to improve the performance on supervised downstream tasks, such as Visual Question Answering (VQA), and goal-conditioned planning. Compared to past works on dynamics modeling, our method achieves significantly better long-term synthesis of object dynamics, while retaining high quality visual generation. Besides, SlotFormer enables VQA models to reason about the future without object-level labels, even outperforming counterparts that use ground-truth annotations. Finally, we show its ability to serve as a world model for model-based planning, which is competitive with methods designed specifically for such tasks.", "authors": "Ziyi Wu, Nikita Dvornik, Klaus Greff, Thomas Kipf, Animesh Garg", "first_author": "Animesh Garg", "first_end_author": "Ziyi Wu; Animesh Garg", "publish_time": "2022-10-12 01:53:58+00:00", "update_time": "2023-01-21 02:46:41+00:00"}, {"id": "2301.08849", "url": "http://arxiv.org/abs/2301.08849v1", "pdf_url": "http://arxiv.org/pdf/2301.08849v1", "title": "CADA-GAN: Context-Aware GAN with Data Augmentation", "abs": "Current child face generators are restricted by the limited size of the available datasets. In addition, feature selection can prove to be a significant challenge, especially due to the large amount of features that need to be trained for. To manage these problems, we proposed CADA-GAN, a \\textbf{C}ontext-\\textbf{A}ware GAN that allows optimal feature extraction, with added robustness from additional \\textbf{D}ata \\textbf{A}ugmentation. CADA-GAN is adapted from the popular StyleGAN2-Ada model, with attention on augmentation and segmentation of the parent images. The model has the lowest \\textit{Mean Squared Error Loss} (MSEloss) on latent feature representations and the generated child image is robust compared with the one that generated from baseline models.", "authors": "Sofie Daniels, Jiugeng Sun, Jiaqing Xie", "first_author": "Jiaqing Xie", "first_end_author": "Sofie Daniels; Jiaqing Xie", "publish_time": "2023-01-21 01:52:17+00:00", "update_time": "2023-01-21 01:52:17+00:00"}, {"id": "2301.08846", "url": "http://arxiv.org/abs/2301.08846v1", "pdf_url": "http://arxiv.org/pdf/2301.08846v1", "title": "Regeneration Learning: A Learning Paradigm for Data Generation", "abs": "Machine learning methods for conditional data generation usually build a mapping from source conditional data X to target data Y. The target Y (e.g., text, speech, music, image, video) is usually high-dimensional and complex, and contains information that does not exist in source data, which hinders effective and efficient learning on the source-target mapping. In this paper, we present a learning paradigm called regeneration learning for data generation, which first generates Y' (an abstraction/representation of Y) from X and then generates Y from Y'. During training, Y' is obtained from Y through either handcrafted rules or self-supervised learning and is used to learn X-->Y' and Y'-->Y. Regeneration learning extends the concept of representation learning to data generation tasks, and can be regarded as a counterpart of traditional representation learning, since 1) regeneration learning handles the abstraction (Y') of the target data Y for data generation while traditional representation learning handles the abstraction (X') of source data X for data understanding; 2) both the processes of Y'-->Y in regeneration learning and X-->X' in representation learning can be learned in a self-supervised way (e.g., pre-training); 3) both the mappings from X to Y' in regeneration learning and from X' to Y in representation learning are simpler than the direct mapping from X to Y. We show that regeneration learning can be a widely-used paradigm for data generation (e.g., text generation, speech recognition, speech synthesis, music composition, image generation, and video generation) and can provide valuable insights into developing data generation methods.", "authors": "Xu Tan, Tao Qin, Jiang Bian, Tie-Yan Liu, Yoshua Bengio", "first_author": "Yoshua Bengio", "first_end_author": "Xu Tan; Yoshua Bengio", "publish_time": "2023-01-21 01:33:34+00:00", "update_time": "2023-01-21 01:33:34+00:00"}, {"id": "2301.04746", "url": "http://arxiv.org/abs/2301.04746v3", "pdf_url": "http://arxiv.org/pdf/2301.04746v3", "title": "Switchable Lightweight Anti-symmetric Processing (SLAP) with CNN   Outspeeds Data Augmentation by Smaller Sample -- Application in Gomoku   Reinforcement Learning", "abs": "To replace data augmentation, this paper proposed a method called SLAP to intensify experience to speed up machine learning and reduce the sample size. SLAP is a model-independent protocol/function to produce the same output given different transformation variants. SLAP improved the convergence speed of convolutional neural network learning by 83% in the experiments with Gomoku game states, with only one eighth of the sample size compared with data augmentation. In reinforcement learning for Gomoku, using AlphaGo Zero/AlphaZero algorithm with data augmentation as baseline, SLAP reduced the number of training samples by a factor of 8 and achieved similar winning rate against the same evaluator, but it was not yet evident that it could speed up reinforcement learning. The benefits should at least apply to domains that are invariant to symmetry or certain transformations. As future work, SLAP may aid more explainable learning and transfer learning for domains that are not invariant to symmetry, as a small step towards artificial general intelligence.", "authors": "Chi-Hang Suen", "first_author": "Chi-Hang Suen", "first_end_author": "Chi-Hang Suen; Chi-Hang Suen", "publish_time": "2023-01-11 22:55:05+00:00", "update_time": "2023-01-21 01:06:34+00:00"}, {"id": "2301.07836", "url": "http://arxiv.org/abs/2301.07836v2", "pdf_url": "http://arxiv.org/pdf/2301.07836v2", "title": "Self Supervision Does Not Help Natural Language Supervision at Scale", "abs": "Self supervision and natural language supervision have emerged as two exciting ways to train general purpose image encoders which excel at a variety of downstream tasks. Recent works such as M3AE and SLIP have suggested that these approaches can be effectively combined, but most notably their results use small pre-training datasets (<50M samples) and don't effectively reflect the large-scale regime (>100M examples) that is commonly used for these approaches. Here we investigate whether a similar approach can be effective when trained with a much larger amount of data. We find that a combination of two state of the art approaches: masked auto-encoders, MAE and contrastive language image pre-training, CLIP provides a benefit over CLIP when trained on a corpus of 11.3M image-text pairs, but little to no benefit (as evaluated on a suite of common vision tasks) over CLIP when trained on a large corpus of 1.4B images. Our work provides some much needed clarity into the effectiveness (or lack thereof) of self supervision for large-scale image-text training.", "authors": "Floris Weers, Vaishaal Shankar, Angelos Katharopoulos, Yinfei Yang, Tom Gunter", "first_author": "Tom Gunter", "first_end_author": "Floris Weers; Tom Gunter", "publish_time": "2023-01-19 01:05:18+00:00", "update_time": "2023-01-20 22:26:21+00:00"}, {"id": "2301.08815", "url": "http://arxiv.org/abs/2301.08815v1", "pdf_url": "http://arxiv.org/pdf/2301.08815v1", "title": "DiffusionCT: Latent Diffusion Model for CT Image Standardization", "abs": "Computed tomography (CT) imaging is a widely used modality for early lung cancer diagnosis, treatment, and prognosis. Features extracted from CT images are now accepted to quantify spatial and temporal variations in tumor architecture and function. However, CT images are often acquired using scanners from different vendors with customized acquisition standards, resulting in significantly different texture features even for the same patient, posing a fundamental challenge to downstream studies. Existing CT image harmonization models rely on supervised or semi-supervised techniques, with limited performance. In this paper, we have proposed a diffusion-based CT image standardization model called DiffusionCT which works on latent space by mapping latent distribution into a standard distribution. DiffusionCT incorporates an Unet-based encoder-decoder and a diffusion model embedded in its bottleneck part. The Unet first trained without the diffusion model to learn the latent representation of the input data. The diffusion model is trained in the next training phase. All the trained models work together on image standardization. The encoded representation outputted from the Unet encoder passes through the diffusion model, and the diffusion model maps the distribution in to target standard image domain. Finally, the decode takes that transformed latent representation to synthesize a standardized image. The experimental results show that DiffusionCT significantly improves the performance of the standardization task.", "authors": "Md Selim, Jie Zhang, Michael A. Brooks, Ge Wang, Jin Chen", "first_author": "Jin Chen", "first_end_author": "Md Selim; Jin Chen", "publish_time": "2023-01-20 22:13:48+00:00", "update_time": "2023-01-20 22:13:48+00:00"}, {"id": "2212.01548", "url": "http://arxiv.org/abs/2212.01548v2", "pdf_url": "http://arxiv.org/pdf/2212.01548v2", "title": "FedRolex: Model-Heterogeneous Federated Learning with Rolling Sub-Model   Extraction", "abs": "Most cross-device federated learning (FL) studies focus on the model-homogeneous setting where the global server model and local client models are identical. However, such constraint not only excludes low-end clients who would otherwise make unique contributions to model training but also restrains clients from training large models due to on-device resource bottlenecks. In this work, we propose FedRolex, a partial training (PT)-based approach that enables model-heterogeneous FL and can train a global server model larger than the largest client model. At its core, FedRolex employs a rolling sub-model extraction scheme that allows different parts of the global server model to be evenly trained, which mitigates the client drift induced by the inconsistency between individual client models and server model architectures. We show that FedRolex outperforms state-of-the-art PT-based model-heterogeneous FL methods (e.g. Federated Dropout) and reduces the gap between model-heterogeneous and model-homogeneous FL, especially under the large-model large-dataset regime. In addition, we provide theoretical statistical analysis on its advantage over Federated Dropout and evaluate FedRolex on an emulated real-world device distribution to show that FedRolex can enhance the inclusiveness of FL and boost the performance of low-end devices that would otherwise not benefit from FL. Our code is available at: https://github.com/AIoT-MLSys-Lab/FedRolex", "authors": "Samiul Alam, Luyang Liu, Ming Yan, Mi Zhang", "first_author": "Mi Zhang", "first_end_author": "Samiul Alam; Mi Zhang", "publish_time": "2022-12-03 06:04:11+00:00", "update_time": "2023-01-20 21:53:11+00:00"}, {"id": "2208.11050", "url": "http://arxiv.org/abs/2208.11050v2", "pdf_url": "http://arxiv.org/pdf/2208.11050v2", "title": "Self-Trained Proposal Networks for the Open World", "abs": "Current state-of-the-art object proposal networks are trained with a closed-world assumption, meaning they learn to only detect objects of the training classes. These models fail to provide high recall in open-world environments where important novel objects may be encountered. While a handful of recent works attempt to tackle this problem, they fail to consider that the optimal behavior of a proposal network can vary significantly depending on the data and application. Our goal is to provide a flexible proposal solution that can be easily tuned to suit a variety of open-world settings. To this end, we design a Self-Trained Proposal Network (STPN) that leverages an adjustable hybrid architecture, a novel self-training procedure, and dynamic loss components to optimize the tradeoff between known and unknown object detection performance. To thoroughly evaluate our method, we devise several new challenges which invoke varying degrees of label bias by altering known class diversity and label count. We find that in every task, STPN easily outperforms existing baselines (e.g., RPN, OLN). Our method is also highly data efficient, surpassing baseline recall with a fraction of the labeled data.", "authors": "Matthew Inkawhich, Nathan Inkawhich, Hai Li, Yiran Chen", "first_author": "Yiran Chen", "first_end_author": "Matthew Inkawhich; Yiran Chen", "publish_time": "2022-08-23 15:57:19+00:00", "update_time": "2023-01-20 21:17:20+00:00"}, {"id": "2301.08802", "url": "http://arxiv.org/abs/2301.08802v1", "pdf_url": "http://arxiv.org/pdf/2301.08802v1", "title": "Impact of PCA-based preprocessing and different CNN structures on   deformable registration of sonograms", "abs": "Central venous catheters (CVC) are commonly inserted into the large veins of the neck, e.g. the internal jugular vein (IJV). CVC insertion may cause serious complications like misplacement into an artery or perforation of cervical vessels. Placing a CVC under sonographic guidance is an appropriate method to reduce such adverse events, if anatomical landmarks like venous and arterial vessels can be detected reliably. This task shall be solved by registration of patient individual images vs. an anatomically labelled reference image. In this work, a linear, affine transformation is performed on cervical sonograms, followed by a non-linear transformation to achieve a more precise registration. Voxelmorph (VM), a learning-based library for deformable image registration using a convolutional neural network (CNN) with U-Net structure was used for non-linear transformation. The impact of principal component analysis (PCA)-based pre-denoising of patient individual images, as well as the impact of modified net structures with differing complexities on registration results were examined visually and quantitatively, the latter using metrics for deformation and image similarity. Using the PCA-approximated cervical sonograms resulted in decreased mean deformation lengths between 18% and 66% compared to their original image counterparts, depending on net structure. In addition, reducing the number of convolutional layers led to improved image similarity with PCA images, while worsening in original images. Despite a large reduction of network parameters, no overall decrease in registration quality was observed, leading to the conclusion that the original net structure is oversized for the task at hand.", "authors": "Christian Schmidt, Heinrich Martin Overhoff", "first_author": "Heinrich Martin Overhoff", "first_end_author": "Christian Schmidt; Heinrich Martin Overhoff", "publish_time": "2023-01-20 21:01:39+00:00", "update_time": "2023-01-20 21:01:39+00:00"}, {"id": "2301.08800", "url": "http://arxiv.org/abs/2301.08800v1", "pdf_url": "http://arxiv.org/pdf/2301.08800v1", "title": "In-situ Water quality monitoring in Oil and Gas operations", "abs": "From agriculture to mining, to energy, surface water quality monitoring is an essential task. As oil and gas operators work to reduce the consumption of freshwater, it is increasingly important to actively manage fresh and non-fresh water resources over the long term. For large-scale monitoring, manual sampling at many sites has become too time-consuming and unsustainable, given the sheer number of dispersed ponds, small lakes, playas, and wetlands over a large area. Therefore, satellite-based environmental monitoring presents great potential. Many existing satellite-based monitoring studies utilize index-based methods to monitor large water bodies such as rivers and oceans. However, these existing methods fail when monitoring small ponds-the reflectance signal received from small water bodies is too weak to detect. To address this challenge, we propose a new Water Quality Enhanced Index (WQEI) Model, which is designed to enable users to determine contamination levels in water bodies with weak reflectance patterns. Our results show that 1) WQEI is a good indicator of water turbidity validated with 1200 water samples measured in the laboratory, and 2) by applying our method to commonly available satellite data (e.g. LandSat8), one can achieve high accuracy water quality monitoring efficiently in large regions. This provides a tool for operators to optimize the quality of water stored within surface storage ponds and increasing the readiness and availability of non-fresh water.", "authors": "Satish Kumar, Rui Kou, Henry Hill, Jake Lempges, Eric Qian, Vikram Jayaram", "first_author": "Vikram Jayaram", "first_end_author": "Satish Kumar; Vikram Jayaram", "publish_time": "2023-01-20 20:56:52+00:00", "update_time": "2023-01-20 20:56:52+00:00"}, {"id": "2301.08798", "url": "http://arxiv.org/abs/2301.08798v1", "pdf_url": "http://arxiv.org/pdf/2301.08798v1", "title": "DeepCOVID-Fuse: A Multi-modality Deep Learning Model Fusing Chest   X-Radiographs and Clinical Variables to Predict COVID-19 Risk Levels", "abs": "Propose: To present DeepCOVID-Fuse, a deep learning fusion model to predict risk levels in patients with confirmed coronavirus disease 2019 (COVID-19) and to evaluate the performance of pre-trained fusion models on full or partial combination of chest x-ray (CXRs) or chest radiograph and clinical variables.   Materials and Methods: The initial CXRs, clinical variables and outcomes (i.e., mortality, intubation, hospital length of stay, ICU admission) were collected from February 2020 to April 2020 with reverse-transcription polymerase chain reaction (RT-PCR) test results as the reference standard. The risk level was determined by the outcome. The fusion model was trained on 1657 patients (Age: 58.30 +/- 17.74; Female: 807) and validated on 428 patients (56.41 +/- 17.03; 190) from Northwestern Memorial HealthCare system and was tested on 439 patients (56.51 +/- 17.78; 205) from a single holdout hospital. Performance of pre-trained fusion models on full or partial modalities were compared on the test set using the DeLong test for the area under the receiver operating characteristic curve (AUC) and the McNemar test for accuracy, precision, recall and F1.   Results: The accuracy of DeepCOVID-Fuse trained on CXRs and clinical variables is 0.658, with an AUC of 0.842, which significantly outperformed (p < 0.05) models trained only on CXRs with an accuracy of 0.621 and AUC of 0.807 and only on clinical variables with an accuracy of 0.440 and AUC of 0.502. The pre-trained fusion model with only CXRs as input increases accuracy to 0.632 and AUC to 0.813 and with only clinical variables as input increases accuracy to 0.539 and AUC to 0.733.   Conclusion: The fusion model learns better feature representations across different modalities during training and achieves good outcome predictions even when only some of the modalities are used in testing.", "authors": "Yunan Wu, Amil Dravid, Ramsey Michael Wehbe, Aggelos K. Katsaggelos", "first_author": "Aggelos K. Katsaggelos", "first_end_author": "Yunan Wu; Aggelos K. Katsaggelos", "publish_time": "2023-01-20 20:54:25+00:00", "update_time": "2023-01-20 20:54:25+00:00"}, {"id": "2301.08794", "url": "http://arxiv.org/abs/2301.08794v1", "pdf_url": "http://arxiv.org/pdf/2301.08794v1", "title": "Robot Skill Learning Via Classical Robotics-Based Generated Datasets:   Advantages, Disadvantages, and Future Improvement", "abs": "Why do we not profit from our long-existing classical robotics knowledge and look for some alternative way for data collection? The situation ignoring all existing methods might be such a waste. This article argues that a dataset created using a classical robotics algorithm is a crucial part of future development. This developed classic algorithm has a perfect domain adaptation and generalization property, and most importantly, collecting datasets based on them is quite easy. It is well known that current robot skill-learning approaches perform exceptionally badly in the unseen domain, and their performance against adversarial attacks is quite limited as long as they do not have a very exclusive big dataset. Our experiment is the initial steps of using a dataset created by classical robotics codes. Our experiment investigated possible trajectory collection based on classical robotics. It addressed some advantages and disadvantages and pointed out other future development ideas.", "authors": "Batu Kaan Oezen", "first_author": "Batu Kaan Oezen", "first_end_author": "Batu Kaan Oezen; Batu Kaan Oezen", "publish_time": "2023-01-20 20:37:46+00:00", "update_time": "2023-01-20 20:37:46+00:00"}, {"id": "2301.08784", "url": "http://arxiv.org/abs/2301.08784v1", "pdf_url": "http://arxiv.org/pdf/2301.08784v1", "title": "Visual Semantic Relatedness Dataset for Image Captioning", "abs": "Modern image captioning system relies heavily on extracting knowledge from images to capture the concept of a static story. In this paper, we propose a textual visual context dataset for captioning, in which the publicly available dataset COCO Captions (Lin et al., 2014) has been extended with information about the scene (such as objects in the image). Since this information has a textual form, it can be used to leverage any NLP task, such as text similarity or semantic relation methods, into captioning systems, either as an end-to-end training strategy or a post-processing based approach.", "authors": "Ahmed Sabir, Francesc Moreno-Noguer, Llu\u00eds Padr\u00f3", "first_author": "Llu\u00eds Padr\u00f3", "first_end_author": "Ahmed Sabir; Llu\u00eds Padr\u00f3", "publish_time": "2023-01-20 20:04:35+00:00", "update_time": "2023-01-20 20:04:35+00:00"}, {"id": "2301.08783", "url": "http://arxiv.org/abs/2301.08783v1", "pdf_url": "http://arxiv.org/pdf/2301.08783v1", "title": "An Asynchronous Intensity Representation for Framed and Event Video   Sources", "abs": "Neuromorphic \"event\" cameras, designed to mimic the human vision system with asynchronous sensing, unlock a new realm of high-speed and high dynamic range applications. However, researchers often either revert to a framed representation of event data for applications, or build bespoke applications for a particular camera's event data type. To usher in the next era of video systems, accommodate new event camera designs, and explore the benefits to asynchronous video in classical applications, we argue that there is a need for an asynchronous, source-agnostic video representation. In this paper, we introduce a novel, asynchronous intensity representation for both framed and non-framed data sources. We show that our representation can increase intensity precision and greatly reduce the number of samples per pixel compared to grid-based representations. With framed sources, we demonstrate that by permitting a small amount of loss through the temporal averaging of similar pixel values, we can reduce our representational sample rate by more than half, while incurring a drop in VMAF quality score of only 4.5. We also demonstrate lower latency than the state-of-the-art method for fusing and transcoding framed and event camera data to an intensity representation, while maintaining $2000\\times$ the temporal resolution. We argue that our method provides the computational efficiency and temporal granularity necessary to build real-time intensity-based applications for event cameras.", "authors": "Andrew C. Freeman, Montek Singh, Ketan Mayer-Patel", "first_author": "Ketan Mayer-Patel", "first_end_author": "Andrew C. Freeman; Ketan Mayer-Patel", "publish_time": "2023-01-20 19:46:23+00:00", "update_time": "2023-01-20 19:46:23+00:00"}, {"id": "2301.08782", "url": "http://arxiv.org/abs/2301.08782v1", "pdf_url": "http://arxiv.org/pdf/2301.08782v1", "title": "Estimation of mitral valve hinge point coordinates -- deep neural net   for echocardiogram segmentation", "abs": "Cardiac image segmentation is a powerful tool in regard to diagnostics and treatment of cardiovascular diseases. Purely feature-based detection of anatomical structures like the mitral valve is a laborious task due to specifically required feature engineering and is especially challenging in echocardiograms, because of their inherently low contrast and blurry boundaries between some anatomical structures. With the publication of further annotated medical datasets and the increase in GPU processing power, deep learning-based methods in medical image segmentation became more feasible in the past years. We propose a fully automatic detection method for mitral valve hinge points, which uses a U-Net based deep neural net to segment cardiac chambers in echocardiograms in a first step, and subsequently extracts the mitral valve hinge points from the resulting segmentations in a second step. Results measured with this automatic detection method were compared to reference coordinate values, which with median absolute hinge point coordinate errors of 1.35 mm for the x- (15-85 percentile range: [0.3 mm; 3.15 mm]) and 0.75 mm for the y- coordinate (15-85 percentile range: [0.15 mm; 1.88 mm]).", "authors": "Christian Schmidt, Heinrich Martin Overhoff", "first_author": "Heinrich Martin Overhoff", "first_end_author": "Christian Schmidt; Heinrich Martin Overhoff", "publish_time": "2023-01-20 19:46:16+00:00", "update_time": "2023-01-20 19:46:16+00:00"}, {"id": "2301.08739", "url": "http://arxiv.org/abs/2301.08739v1", "pdf_url": "http://arxiv.org/pdf/2301.08739v1", "title": "FlatFormer: Flattened Window Attention for Efficient Point Cloud   Transformer", "abs": "Transformer, as an alternative to CNN, has been proven effective in many modalities (e.g., texts and images). For 3D point cloud transformers, existing efforts focus primarily on pushing their accuracy to the state-of-the-art level. However, their latency lags behind sparse convolution-based models (3x slower), hindering their usage in resource-constrained, latency-sensitive applications (such as autonomous driving). This inefficiency comes from point clouds' sparse and irregular nature, whereas transformers are designed for dense, regular workloads. This paper presents FlatFormer to close this latency gap by trading spatial proximity for better computational regularity. We first flatten the point cloud with window-based sorting and partition points into groups of equal sizes rather than windows of equal shapes. This effectively avoids expensive structuring and padding overheads. We then apply self-attention within groups to extract local features, alternate sorting axis to gather features from different directions, and shift windows to exchange features across groups. FlatFormer delivers state-of-the-art accuracy on Waymo Open Dataset with 4.6x speedup over (transformer-based) SST and 1.4x speedup over (sparse convolutional) CenterPoint. This is the first point cloud transformer that achieves real-time performance on edge GPUs and is faster than sparse convolutional methods while achieving on-par or even superior accuracy on large-scale benchmarks. Code to reproduce our results will be made publicly available.", "authors": "Zhijian Liu, Xinyu Yang, Haotian Tang, Shang Yang, Song Han", "first_author": "Song Han", "first_end_author": "Zhijian Liu; Song Han", "publish_time": "2023-01-20 18:59:57+00:00", "update_time": "2023-01-20 18:59:57+00:00"}, {"id": "2208.05834", "url": "http://arxiv.org/abs/2208.05834v2", "pdf_url": "http://arxiv.org/pdf/2208.05834v2", "title": "Joint reconstruction-segmentation on graphs", "abs": "Practical image segmentation tasks concern images which must be reconstructed from noisy, distorted, and/or incomplete observations. A recent approach for solving such tasks is to perform this reconstruction jointly with the segmentation, using each to guide the other. However, this work has so far employed relatively simple segmentation methods, such as the Chan--Vese algorithm. In this paper, we present a method for joint reconstruction-segmentation using graph-based segmentation methods, which have been seeing increasing recent interest. Complications arise due to the large size of the matrices involved, and we show how these complications can be managed. We then analyse the convergence properties of our scheme. Finally, we apply this scheme to distorted versions of ``two cows'' images familiar from previous graph-based segmentation literature, first to a highly noised version and second to a blurred version, achieving highly accurate segmentations in both cases. We compare these results to those obtained by sequential reconstruction-segmentation approaches, finding that our method competes with, or even outperforms, those approaches in terms of reconstruction and segmentation accuracy.", "authors": "Jeremy Budd, Yves van Gennip, Jonas Latz, Simone Parisotto, Carola-Bibiane Sch\u00f6nlieb", "first_author": "Carola-Bibiane Sch\u00f6nlieb", "first_end_author": "Jeremy Budd; Carola-Bibiane Sch\u00f6nlieb", "publish_time": "2022-08-11 14:01:38+00:00", "update_time": "2023-01-20 17:58:02+00:00"}, {"id": "2212.10772", "url": "http://arxiv.org/abs/2212.10772v3", "pdf_url": "http://arxiv.org/pdf/2212.10772v3", "title": "Low-Light Image and Video Enhancement: A Comprehensive Survey and Beyond", "abs": "This paper presents a comprehensive survey of low-light image and video enhancement. We begin with the challenging mixed over-/under-exposed images, which are under-performed by existing methods. To this end, we propose two variants of the SICE dataset named SICE_Grad and SICE_Mix. Next, we introduce Night Wenzhou, a large-scale, high-resolution video dataset, to address the issue of the lack of a low-light video dataset that discount the use of low-light image enhancement (LLIE) to videos. Our Night Wenzhou dataset is challenging since it consists of fast-moving aerial scenes and streetscapes with varying illuminations and degradation. We conduct extensive key technique analysis and experimental comparisons for representative LLIE approaches using these newly proposed datasets and the current benchmark datasets. Finally, we address unresolved issues and propose future research topics for the LLIE community. Our datasets are available at https://github.com/ShenZheng2000/LLIE_Survey.", "authors": "Shen Zheng, Yiling Ma, Jinqian Pan, Changjie Lu, Gaurav Gupta", "first_author": "Gaurav Gupta", "first_end_author": "Shen Zheng; Gaurav Gupta", "publish_time": "2022-12-21 05:08:37+00:00", "update_time": "2023-01-20 17:10:15+00:00"}, {"id": "2301.08669", "url": "http://arxiv.org/abs/2301.08669v1", "pdf_url": "http://arxiv.org/pdf/2301.08669v1", "title": "Holistically Explainable Vision Transformers", "abs": "Transformers increasingly dominate the machine learning landscape across many tasks and domains, which increases the importance for understanding their outputs. While their attention modules provide partial insight into their inner workings, the attention scores have been shown to be insufficient for explaining the models as a whole. To address this, we propose B-cos transformers, which inherently provide holistic explanations for their decisions. Specifically, we formulate each model component - such as the multi-layer perceptrons, attention layers, and the tokenisation module - to be dynamic linear, which allows us to faithfully summarise the entire transformer via a single linear transform. We apply our proposed design to Vision Transformers (ViTs) and show that the resulting models, dubbed Bcos-ViTs, are highly interpretable and perform competitively to baseline ViTs on ImageNet. Code will be made available soon.", "authors": "Moritz B\u00f6hle, Mario Fritz, Bernt Schiele", "first_author": "Bernt Schiele", "first_end_author": "Moritz B\u00f6hle; Bernt Schiele", "publish_time": "2023-01-20 16:45:34+00:00", "update_time": "2023-01-20 16:45:34+00:00"}, {"id": "2301.08664", "url": "http://arxiv.org/abs/2301.08664v1", "pdf_url": "http://arxiv.org/pdf/2301.08664v1", "title": "AccDecoder: Accelerated Decoding for Neural-enhanced Video Analytics", "abs": "The quality of the video stream is key to neural network-based video analytics. However, low-quality video is inevitably collected by existing surveillance systems because of poor quality cameras or over-compressed/pruned video streaming protocols, e.g., as a result of upstream bandwidth limit. To address this issue, existing studies use quality enhancers (e.g., neural super-resolution) to improve the quality of videos (e.g., resolution) and eventually ensure inference accuracy. Nevertheless, directly applying quality enhancers does not work in practice because it will introduce unacceptable latency. In this paper, we present AccDecoder, a novel accelerated decoder for real-time and neural-enhanced video analytics. AccDecoder can select a few frames adaptively via Deep Reinforcement Learning (DRL) to enhance the quality by neural super-resolution and then up-scale the unselected frames that reference them, which leads to 6-21% accuracy improvement. AccDecoder provides efficient inference capability via filtering important frames using DRL for DNN-based inference and reusing the results for the other frames via extracting the reference relationship among frames and blocks, which results in a latency reduction of 20-80% than baselines.", "authors": "Tingting Yuan, Liang Mi, Weijun Wang, Haipeng Dai, Xiaoming Fu", "first_author": "Xiaoming Fu", "first_end_author": "Tingting Yuan; Xiaoming Fu", "publish_time": "2023-01-20 16:30:44+00:00", "update_time": "2023-01-20 16:30:44+00:00"}, {"id": "2301.08654", "url": "http://arxiv.org/abs/2301.08654v1", "pdf_url": "http://arxiv.org/pdf/2301.08654v1", "title": "Automated extraction of capacitive coupling for quantum dot systems", "abs": "Gate-defined quantum dots (QDs) have appealing attributes as a quantum computing platform, however, near-term devices possess a range of possible imperfections that need to be accounted for during the tuning and operation of QD devices. One such problem is the capacitive cross-talk between the metallic gates that define and control QD qubits. A way to compensate for the capacitive cross-talk and enable targeted control of specific QDs independent of coupling is by the use of virtual gates. Here, we demonstrate a reliable automated capacitive coupling identification method that combines machine learning with traditional fitting to take advantage of the desirable properties of each. We also show how the cross-capacitance measurement may be used for the identification of spurious QDs sometimes formed during tuning experimental devices. Our systems can autonomously flag devices with spurious dots near the operating regime which is crucial information for reliable tuning to a regime suitable for qubit operations.", "authors": "Joshua Ziegler, Florian Luthi, Mick Ramsey, Felix Borjans, Guoji Zheng, Justyna P. Zwolak", "first_author": "Justyna P. Zwolak", "first_end_author": "Joshua Ziegler; Justyna P. Zwolak", "publish_time": "2023-01-20 16:03:30+00:00", "update_time": "2023-01-20 16:03:30+00:00"}, {"id": "2301.08647", "url": "http://arxiv.org/abs/2301.08647v1", "pdf_url": "http://arxiv.org/pdf/2301.08647v1", "title": "Image Memorability Prediction with Vision Transformers", "abs": "Behavioral studies have shown that the memorability of images is similar across groups of people, suggesting that memorability is a function of the intrinsic properties of images, and is unrelated to people's individual experiences and traits. Deep learning networks can be trained on such properties and be used to predict memorability in new data sets. Convolutional neural networks (CNN) have pioneered image memorability prediction, but more recently developed vision transformer (ViT) models may have the potential to yield even better predictions. In this paper, we present the ViTMem, a new memorability model based on ViT, and evaluate memorability predictions obtained by it with state-of-the-art CNN-derived models. Results showed that ViTMem performed equal to or better than state-of-the-art models on all data sets. Additional semantic level analyses revealed that ViTMem is particularly sensitive to the semantic content that drives memorability in images. We conclude that ViTMem provides a new step forward, and propose that ViT-derived models can replace CNNs for computational prediction of image memorability. Researchers, educators, advertisers, visual designers and other interested parties can leverage the model to improve the memorability of their image material.", "authors": "Thomas Hagen, Thomas Espeseth", "first_author": "Thomas Espeseth", "first_end_author": "Thomas Hagen; Thomas Espeseth", "publish_time": "2023-01-20 15:55:35+00:00", "update_time": "2023-01-20 15:55:35+00:00"}, {"id": "2203.14645", "url": "http://arxiv.org/abs/2203.14645v2", "pdf_url": "http://arxiv.org/pdf/2203.14645v2", "title": "REx: Data-Free Residual Quantization Error Expansion", "abs": "Deep neural networks (DNNs) are ubiquitous in computer vision and natural language processing, but suffer from high inference cost. This problem can be addressed by quantization, which consists in converting floating point operations into a lower bit-width format. With the growing concerns on privacy rights, we focus our efforts on data-free methods. However, such techniques suffer from their lack of adaptability to the target devices, as a hardware typically only support specific bit widths. Thus, to adapt to a variety of devices, a quantization method shall be flexible enough to find good accuracy v.s. speed trade-offs for every bit width and target device. To achieve this, we propose REx, a quantization method that leverages residual error expansion, along with group sparsity and an ensemble approximation for better parallelization. REx is backed off by strong theoretical guarantees and achieves superior performance on every benchmarked application (from vision to NLP tasks), architecture (ConvNets, transformers) and bit-width (from int8 to ternary quantization).", "authors": "Edouard Yvinec, Arnaud Dapgony, Matthieu Cord, Kevin Bailly", "first_author": "Kevin Bailly", "first_end_author": "Edouard Yvinec; Kevin Bailly", "publish_time": "2022-03-28 11:04:45+00:00", "update_time": "2023-01-20 15:03:41+00:00"}, {"id": "2206.06484", "url": "http://arxiv.org/abs/2206.06484v3", "pdf_url": "http://arxiv.org/pdf/2206.06484v3", "title": "On Image Segmentation With Noisy Labels: Characterization and Volume   Properties of the Optimal Solutions to Accuracy and Dice", "abs": "We study two of the most popular performance metrics in medical image segmentation, Accuracy and Dice, when the target labels are noisy. For both metrics, several statements related to characterization and volume properties of the set of optimal segmentations are proved, and associated experiments are provided. Our main insights are: (i) the volume of the solutions to both metrics may deviate significantly from the expected volume of the target, (ii) the volume of a solution to Accuracy is always less than or equal to the volume of a solution to Dice and (iii) the optimal solutions to both of these metrics coincide when the set of feasible segmentations is constrained to the set of segmentations with the volume equal to the expected volume of the target.", "authors": "Marcus Nordstr\u00f6m, Henrik Hult, Jonas S\u00f6derberg, Fredrik L\u00f6fman", "first_author": "Fredrik L\u00f6fman", "first_end_author": "Marcus Nordstr\u00f6m; Fredrik L\u00f6fman", "publish_time": "2022-06-13 21:30:29+00:00", "update_time": "2023-01-20 15:02:33+00:00"}, {"id": "2301.08605", "url": "http://arxiv.org/abs/2301.08605v1", "pdf_url": "http://arxiv.org/pdf/2301.08605v1", "title": "A Deep Learning Approach for SAR Tomographic Imaging of Forested Areas", "abs": "Synthetic aperture radar tomographic imaging reconstructs the three-dimensional reflectivity of a scene from a set of coherent acquisitions performed in an interferometric configuration. In forest areas, a large number of elements backscatter the radar signal within each resolution cell. To reconstruct the vertical reflectivity profile, state-of-the-art techniques perform a regularized inversion implemented in the form of iterative minimization algorithms. We show that light-weight neural networks can be trained to perform the tomographic inversion with a single feed-forward pass, leading to fast reconstructions that could better scale to the amount of data provided by the future BIOMASS mission. We train our encoder-decoder network using simulated data and validate our technique on real L-band and P-band data.", "authors": "Zo\u00e9 Berenger, Lo\u00efc Denis, Florence Tupin, Laurent Ferro-Famil, Yue Huang", "first_author": "Yue Huang", "first_end_author": "Zo\u00e9 Berenger; Yue Huang", "publish_time": "2023-01-20 14:34:03+00:00", "update_time": "2023-01-20 14:34:03+00:00"}, {"id": "2208.07360", "url": "http://arxiv.org/abs/2208.07360v2", "pdf_url": "http://arxiv.org/pdf/2208.07360v2", "title": "Evaluating the Evaluators: Which UDA validation methods are most   effective? Can they be improved?", "abs": "This paper compares and ranks 8 UDA validation methods. Validators estimate model accuracy, which makes them an essential component of any UDA train-test pipeline. We rank these validators to indicate which of them are most useful for the purpose of selecting optimal model checkpoints and hyperparameters. To the best of our knowledge, this large-scale benchmark study is the first of its kind in the UDA field. In addition, we propose three new validators that outperform all the existing checkpoint-based validators that we were able to find in the existing literature. Code is available at https://www.github.com/KevinMusgrave/powerful-benchmarker.", "authors": "Kevin Musgrave, Serge Belongie, Ser-Nam Lim", "first_author": "Ser-Nam Lim", "first_end_author": "Kevin Musgrave; Ser-Nam Lim", "publish_time": "2022-08-15 17:55:26+00:00", "update_time": "2023-01-20 14:13:08+00:00"}, {"id": "2301.08590", "url": "http://arxiv.org/abs/2301.08590v1", "pdf_url": "http://arxiv.org/pdf/2301.08590v1", "title": "Improving Sketch Colorization using Adversarial Segmentation Consistency", "abs": "We propose a new method for producing color images from sketches. Current solutions in sketch colorization either necessitate additional user instruction or are restricted to the \"paired\" translation strategy. We leverage semantic image segmentation from a general-purpose panoptic segmentation network to generate an additional adversarial loss function. The proposed loss function is compatible with any GAN model. Our method is not restricted to datasets with segmentation labels and can be applied to unpaired translation tasks as well. Using qualitative, and quantitative analysis, and based on a user study, we demonstrate the efficacy of our method on four distinct image datasets. On the FID metric, our model improves the baseline by up to 35 points. Our code, pretrained models, scripts to produce newly introduced datasets and corresponding sketch images are available at https://github.com/giddyyupp/AdvSegLoss.", "authors": "Samet Hicsonmez, Nermin Samet, Emre Akbas, Pinar Duygulu", "first_author": "Pinar Duygulu", "first_end_author": "Samet Hicsonmez; Pinar Duygulu", "publish_time": "2023-01-20 14:07:30+00:00", "update_time": "2023-01-20 14:07:30+00:00"}, {"id": "2205.13268", "url": "http://arxiv.org/abs/2205.13268v2", "pdf_url": "http://arxiv.org/pdf/2205.13268v2", "title": "MemeTector: Enforcing deep focus for meme detection", "abs": "Image memes and specifically their widely-known variation image macros, is a special new media type that combines text with images and is used in social media to playfully or subtly express humour, irony, sarcasm and even hate. It is important to accurately retrieve image memes from social media to better capture the cultural and social aspects of online phenomena and detect potential issues (hate-speech, disinformation). Essentially, the background image of an image macro is a regular image easily recognized as such by humans but cumbersome for the machine to do so due to feature map similarity with the complete image macro. Hence, accumulating suitable feature maps in such cases can lead to deep understanding of the notion of image memes. To this end, we propose a methodology, called Visual Part Utilization, that utilizes the visual part of image memes as instances of the regular image class and the initial image memes as instances of the image meme class to force the model to concentrate on the critical parts that characterize an image meme. Additionally, we employ a trainable attention mechanism on top of a standard ViT architecture to enhance the model's ability to focus on these critical parts and make the predictions interpretable. Several training and test scenarios involving web-scraped regular images of controlled text presence are considered for evaluating the model in terms of robustness and accuracy. The findings indicate that light visual part utilization combined with sufficient text presence during training provides the best and most robust model, surpassing state of the art. Source code and dataset are available at https://github.com/mever-team/memetector.", "authors": "Christos Koutlis, Manos Schinas, Symeon Papadopoulos", "first_author": "Symeon Papadopoulos", "first_end_author": "Christos Koutlis; Symeon Papadopoulos", "publish_time": "2022-05-26 10:50:29+00:00", "update_time": "2023-01-20 14:00:26+00:00"}, {"id": "2211.16191", "url": "http://arxiv.org/abs/2211.16191v2", "pdf_url": "http://arxiv.org/pdf/2211.16191v2", "title": "SgVA-CLIP: Semantic-guided Visual Adapting of Vision-Language Models for   Few-shot Image Classification", "abs": "Although significant progress has been made in few-shot learning, most of existing few-shot image classification methods require supervised pre-training on a large amount of samples of base classes, which limits their generalization ability in real world application. Recently, large-scale Vision-Language Pre-trained models (VLPs) have been gaining increasing attention in few-shot learning because they can provide a new paradigm for transferable visual representation learning with easily available text on the Web. However, the VLPs may neglect detailed visual information that is difficult to describe by language sentences, but important for learning an effective classifier to distinguish different images. To address the above problem, we propose a new framework, named Semantic-guided Visual Adapting (SgVA), which can effectively extend vision-language pre-trained models to produce discriminative adapted visual features by comprehensively using an implicit knowledge distillation, a vision-specific contrastive loss, and a cross-modal contrastive loss. The implicit knowledge distillation is designed to transfer the fine-grained cross-modal knowledge to guide the updating of the vision adapter. State-of-the-art results on 13 datasets demonstrate that the adapted visual features can well complement the cross-modal features to improve few-shot image classification.", "authors": "Fang Peng, Xiaoshan Yang, Linhui Xiao, Yaowei Wang, Changsheng Xu", "first_author": "Changsheng Xu", "first_end_author": "Fang Peng; Changsheng Xu", "publish_time": "2022-11-28 14:58:15+00:00", "update_time": "2023-01-20 13:56:39+00:00"}, {"id": "2301.08571", "url": "http://arxiv.org/abs/2301.08571v1", "pdf_url": "http://arxiv.org/pdf/2301.08571v1", "title": "Visual Writing Prompts: Character-Grounded Story Generation with Curated   Image Sequences", "abs": "Current work on image-based story generation suffers from the fact that the existing image sequence collections do not have coherent plots behind them. We improve visual story generation by producing a new image-grounded dataset, Visual Writing Prompts (VWP). VWP contains almost 2K selected sequences of movie shots, each including 5-10 images. The image sequences are aligned with a total of 12K stories which were collected via crowdsourcing given the image sequences and a set of grounded characters from the corresponding image sequence. Our new image sequence collection and filtering process has allowed us to obtain stories that are more coherent and have more narrativity compared to previous work. We also propose a character-based story generation model driven by coherence as a strong baseline. Evaluations show that our generated stories are more coherent, visually grounded, and have more narrativity than stories generated with the current state-of-the-art model.", "authors": "Xudong Hong, Asad Sayeed, Khushboo Mehra, Vera Demberg, Bernt Schiele", "first_author": "Bernt Schiele", "first_end_author": "Xudong Hong; Bernt Schiele", "publish_time": "2023-01-20 13:38:24+00:00", "update_time": "2023-01-20 13:38:24+00:00"}, {"id": "2212.11614", "url": "http://arxiv.org/abs/2212.11614v2", "pdf_url": "http://arxiv.org/pdf/2212.11614v2", "title": "Hybrid Quantum-Classical Generative Adversarial Network for High   Resolution Image Generation", "abs": "Quantum machine learning (QML) has received increasing attention due to its potential to outperform classical machine learning methods in problems pertaining classification and identification tasks. A subclass of QML methods is quantum generative adversarial networks (QGANs) which have been studied as a quantum counterpart of classical GANs widely used in image manipulation and generation tasks. The existing work on QGANs is still limited to small-scale proof-of-concept examples based on images with significant downscaling. Here we integrate classical and quantum techniques to propose a new hybrid quantum-classical GAN framework. We demonstrate its superior learning capabilities by generating $28 \\times 28$ pixels grey-scale images without dimensionality reduction or classical pre/post-processing on multiple classes of the standard MNIST and Fashion MNIST datasets, which achieves comparable results to classical frameworks with three orders of magnitude less trainable generator parameters. To gain further insight into the working of our hybrid approach, we systematically explore the impact of its parameter space by varying the number of qubits, the size of image patches, the number of layers in the generator, the shape of the patches and the choice of prior distribution. Our results show that increasing the quantum generator size generally improves the learning capability of the network. The developed framework provides a foundation for future design of QGANs with optimal parameter set tailored for complex image generation tasks.", "authors": "Shu Lok Tsang, Maxwell T. West, Sarah M. Erfani, Muhammad Usman", "first_author": "Muhammad Usman", "first_end_author": "Shu Lok Tsang; Muhammad Usman", "publish_time": "2022-12-22 11:18:35+00:00", "update_time": "2023-01-20 12:38:30+00:00"}, {"id": "2301.08534", "url": "http://arxiv.org/abs/2301.08534v1", "pdf_url": "http://arxiv.org/pdf/2301.08534v1", "title": "Prodromal Diagnosis of Lewy Body Diseases Based on the Assessment of   Graphomotor and Handwriting Difficulties", "abs": "To this date, studies focusing on the prodromal diagnosis of Lewy body diseases (LBDs) based on quantitative analysis of graphomotor and handwriting difficulties are missing. In this work, we enrolled 18 subjects diagnosed with possible or probable mild cognitive impairment with Lewy bodies (MCI-LB), 7 subjects having more than 50% probability of developing Parkinson's disease (PD), 21 subjects with both possible/probable MCI-LB and probability of PD > 50%, and 37 age- and gender-matched healthy controls (HC). Each participant performed three tasks: Archimedean spiral drawing (to quantify graphomotor difficulties), sentence writing task (to quantify handwriting difficulties), and pentagon copying test (to quantify cognitive decline). Next, we parameterized the acquired data by various temporal, kinematic, dynamic, spatial, and task-specific features. And finally, we trained classification models for each task separately as well as a model for their combination to estimate the predictive power of the features for the identification of LBDs. Using this approach we were able to identify prodromal LBDs with 74% accuracy and showed the promising potential of computerized objective and non-invasive diagnosis of LBDs based on the assessment of graphomotor and handwriting difficulties.", "authors": "Zoltan Galaz, Jiri Mekyska, Jan Mucha, Vojtech Zvoncak, Zdenek Smekal, Marcos Faundez-Zanuy, Lubos Brabenec, Ivona Moravkova, Irena Rektorova", "first_author": "Irena Rektorova", "first_end_author": "Zoltan Galaz; Irena Rektorova", "publish_time": "2023-01-20 12:30:28+00:00", "update_time": "2023-01-20 12:30:28+00:00"}, {"id": "2301.08529", "url": "http://arxiv.org/abs/2301.08529v1", "pdf_url": "http://arxiv.org/pdf/2301.08529v1", "title": "Exploration of Various Fractional Order Derivatives in Parkinson's   Disease Dysgraphia Analysis", "abs": "Parkinson's disease (PD) is a common neurodegenerative disorder with a prevalence rate estimated to 2.0% for people aged over 65 years. Cardinal motor symptoms of PD such as rigidity and bradykinesia affect the muscles involved in the handwriting process resulting in handwriting abnormalities called PD dysgraphia. Nowadays, online handwritten signal (signal with temporal information) acquired by the digitizing tablets is the most advanced approach of graphomotor difficulties analysis. Although the basic kinematic features were proved to effectively quantify the symptoms of PD dysgraphia, a recent research identified that the theory of fractional calculus can be used to improve the graphomotor difficulties analysis. Therefore, in this study, we follow up on our previous research, and we aim to explore the utilization of various approaches of fractional order derivative (FD) in the analysis of PD dysgraphia. For this purpose, we used the repetitive loops task from the Parkinson's disease handwriting database (PaHaW). Handwritten signals were parametrized by the kinematic features employing three FD approximations: Gr\\\"unwald-Letnikov's, Riemann-Liouville's, and Caputo's. Results of the correlation analysis revealed a significant relationship between the clinical state and the handwriting features based on the velocity. The extracted features by Caputo's FD approximation outperformed the rest of the analyzed FD approaches. This was also confirmed by the results of the classification analysis, where the best model trained by Caputo's handwriting features resulted in a balanced accuracy of 79.73% with a sensitivity of 83.78% and a specificity of 75.68%.", "authors": "Jan Mucha, Zoltan Galaz, Jiri Mekyska, Marcos Faundez-Zanuy, Vojtech Zvoncak, Zdenek Smekal, Lubos Brabenec, Irena Rektorova", "first_author": "Irena Rektorova", "first_end_author": "Jan Mucha; Irena Rektorova", "publish_time": "2023-01-20 12:18:05+00:00", "update_time": "2023-01-20 12:18:05+00:00"}, {"id": "2106.07075", "url": "http://arxiv.org/abs/2106.07075v5", "pdf_url": "http://arxiv.org/pdf/2106.07075v5", "title": "Revisiting consistency for semi-supervised semantic segmentation", "abs": "Semi-supervised learning an attractive technique in practical deployments of deep models since it relaxes the dependence on labeled data. It is especially important in the scope of dense prediction because pixel-level annotation requires significant effort. This paper considers semi-supervised algorithms that enforce consistent predictions over perturbed unlabeled inputs. We study the advantages of perturbing only one of the two model instances and preventing the backward pass through the unperturbed instance. We also propose a competitive perturbation model as a composition of geometric warp and photometric jittering. We experiment with efficient models due to their importance for real-time and low-power applications. Our experiments show clear advantages of (1) one-way consistency, (2) perturbing only the student branch, and (3) strong photometric and geometric perturbations. Our perturbation model outperforms recent work and most of the contribution comes from photometric component. Experiments with additional data from the large coarsely annotated subset of Cityscapes suggest that semi-supervised training can outperform supervised training with the coarse labels.", "authors": "Ivan Grubi\u0161i\u0107, Marin Or\u0161i\u0107, Sini\u0161a \u0160egvi\u0107", "first_author": "Sini\u0161a \u0160egvi\u0107", "first_end_author": "Ivan Grubi\u0161i\u0107; Sini\u0161a \u0160egvi\u0107", "publish_time": "2021-06-13 19:31:59+00:00", "update_time": "2023-01-20 10:52:30+00:00"}, {"id": "2208.08241", "url": "http://arxiv.org/abs/2208.08241v3", "pdf_url": "http://arxiv.org/pdf/2208.08241v3", "title": "ILLUME: Rationalizing Vision-Language Models through Human Interactions", "abs": "Bootstrapping from pre-trained language models has been proven to be an efficient approach for building vision-language models (VLM) for tasks such as image captioning or visual question answering. However, outputs of these models rarely align with user's rationales for specific answers. In order to improve this alignment and reinforce commonsense reasons, we propose a tuning paradigm based on human interactions with machine generated data. Our ILLUME executes the following loop: Given an image-question-answer prompt, the VLM samples multiple candidate rationales, and a human critic provides minimal feedback via preference selection, used for fine-tuning. This loop increases the training data and gradually carves out the VLM's rationalization capabilities that are aligned with human intend. Our exhaustive experiments demonstrate that ILLUME is competitive with standard supervised fine-tuning while using significantly fewer training data and only requiring minimal feedback.", "authors": "Manuel Brack, Patrick Schramowski, Bj\u00f6rn Deiseroth, Kristian Kersting", "first_author": "Kristian Kersting", "first_end_author": "Manuel Brack; Kristian Kersting", "publish_time": "2022-08-17 11:41:43+00:00", "update_time": "2023-01-20 10:38:03+00:00"}, {"id": "2212.14447", "url": "http://arxiv.org/abs/2212.14447v2", "pdf_url": "http://arxiv.org/pdf/2212.14447v2", "title": "A Theoretical Framework for AI Models Explainability", "abs": "EXplainable Artificial Intelligence (XAI) is a vibrant research topic in the artificial intelligence community, with growing interest across methods and domains. Much has been written about the subject, yet XAI still lacks shared terminology and a framework capable of providing structural soundness to explanations. In our work, we address these issues by proposing a novel definition of explanation that is a synthesis of what can be found in the literature. We recognize that explanations are not atomic but the combination of evidence stemming from the model and its input-output mapping, and the human interpretation of this evidence. Furthermore, we fit explanations into the properties of faithfulness (i.e., the explanation being a true description of the model's inner workings and decision-making process) and plausibility (i.e., how much the explanation looks convincing to the user). Using our proposed theoretical framework simplifies how these properties are operationalized and it provides new insight into common explanation methods that we analyze as case studies.", "authors": "Matteo Rizzo, Alberto Veneri, Andrea Albarelli, Claudio Lucchese, Cristina Conati", "first_author": "Cristina Conati", "first_end_author": "Matteo Rizzo; Cristina Conati", "publish_time": "2022-12-29 20:05:26+00:00", "update_time": "2023-01-20 10:01:31+00:00"}, {"id": "2301.08479", "url": "http://arxiv.org/abs/2301.08479v1", "pdf_url": "http://arxiv.org/pdf/2301.08479v1", "title": "Pneumonia Detection in Chest X-Ray Images : Handling Class Imbalance", "abs": "People all over the globe are affected by pneumonia but deaths due to it are highest in Sub-Saharan Asia and South Asia. In recent years, the overall incidence and mortality rate of pneumonia regardless of the utilization of effective vaccines and compelling antibiotics has escalated. Thus, pneumonia remains a disease that needs spry prevention and treatment. The widespread prevalence of pneumonia has caused the research community to come up with a framework that helps detect, diagnose and analyze diseases accurately and promptly. One of the major hurdles faced by the Artificial Intelligence (AI) research community is the lack of publicly available datasets for chest diseases, including pneumonia . Secondly, few of the available datasets are highly imbalanced (normal examples are over sampled, while samples with ailment are in severe minority) making the problem even more challenging. In this article we present a novel framework for the detection of pneumonia. The novelty of the proposed methodology lies in the tackling of class imbalance problem. The Generative Adversarial Network (GAN), specifically a combination of Deep Convolutional Generative Adversarial Network (DCGAN) and Wasserstein GAN gradient penalty (WGAN-GP) was applied on the minority class ``Pneumonia'' for augmentation, whereas Random Under-Sampling (RUS) was done on the majority class ``No Findings'' to deal with the imbalance problem. The ChestX-Ray8 dataset, one of the biggest datasets, is used to validate the performance of the proposed framework. The learning phase is completed using transfer learning on state-of-the-art deep learning models i.e. ResNet-50, Xception, and VGG-16. Results obtained exceed state-of-the-art.", "authors": "Wardah Ali, Eesha Qureshi, Omama Ahmed Farooqi, Rizwan Ahmed Khan", "first_author": "Rizwan Ahmed Khan", "first_end_author": "Wardah Ali; Rizwan Ahmed Khan", "publish_time": "2023-01-20 09:17:39+00:00", "update_time": "2023-01-20 09:17:39+00:00"}, {"id": "2108.02104", "url": "http://arxiv.org/abs/2108.02104v3", "pdf_url": "http://arxiv.org/pdf/2108.02104v3", "title": "Point Discriminative Learning for Data-efficient 3D Point Cloud Analysis", "abs": "3D point cloud analysis has drawn a lot of research attention due to its wide applications. However, collecting massive labelled 3D point cloud data is both time-consuming and labor-intensive. This calls for data-efficient learning methods. In this work we propose PointDisc, a point discriminative learning method to leverage self-supervisions for data-efficient 3D point cloud classification and segmentation. PointDisc imposes a novel point discrimination loss on the middle and global level features produced by the backbone network. This point discrimination loss enforces learned features to be consistent with points belonging to the corresponding local shape region and inconsistent with randomly sampled noisy points. We conduct extensive experiments on 3D object classification, 3D semantic and part segmentation, showing the benefits of PointDisc for data-efficient learning. Detailed analysis demonstrate that PointDisc learns unsupervised features that well capture local and global geometry.", "authors": "Fayao Liu, Guosheng Lin, Chuan-Sheng Foo, Chaitanya K. Joshi, Jie Lin", "first_author": "Jie Lin", "first_end_author": "Fayao Liu; Jie Lin", "publish_time": "2021-08-04 15:11:48+00:00", "update_time": "2023-01-20 08:46:35+00:00"}, {"id": "2211.09590", "url": "http://arxiv.org/abs/2211.09590v2", "pdf_url": "http://arxiv.org/pdf/2211.09590v2", "title": "Hypergraph Transformer for Skeleton-based Action Recognition", "abs": "Skeleton-based action recognition aims to predict human actions given human joint coordinates with skeletal interconnections. To model such off-grid data points and their co-occurrences, Transformer-based formulations would be a natural choice. However, Transformers still lag behind state-of-the-art methods using graph convolutional networks (GCNs). Transformers assume that the input is permutation-invariant and homogeneous (partially alleviated by positional encoding), which ignores an important characteristic of skeleton data, i.e., bone connectivity. Furthermore, each type of body joint has a clear physical meaning in human motion, i.e., motion retains an intrinsic relationship regardless of the joint coordinates, which is not explored in Transformers. In fact, certain re-occurring groups of body joints are often involved in specific actions, such as the subconscious hand movement for keeping balance. Vanilla attention is incapable of describing such underlying relations that are persistent and beyond pair-wise. In this work, we aim to exploit these unique aspects of skeleton data to close the performance gap between Transformers and GCNs. Specifically, we propose a new self-attention (SA) extension, named Hypergraph Self-Attention (HyperSA), to incorporate inherently higher-order relations into the model. The K-hop relative positional embeddings are also employed to take bone connectivity into account. We name the resulting model Hyperformer, and it achieves comparable or better performance w.r.t. accuracy and efficiency than state-of-the-art GCN architectures on NTU RGB+D, NTU RGB+D 120, and Northwestern-UCLA datasets. On the largest NTU RGB+D 120 dataset, the significantly improved performance reached by our Hyperformer demonstrates the underestimated potential of Transformer models in this field.", "authors": "Yuxuan Zhou, Chao Li, Zhi-Qi Cheng, Yifeng Geng, Xuansong Xie, Margret Keuper", "first_author": "Margret Keuper", "first_end_author": "Yuxuan Zhou; Margret Keuper", "publish_time": "2022-11-17 15:36:48+00:00", "update_time": "2023-01-20 07:38:35+00:00"}, {"id": "2301.08455", "url": "http://arxiv.org/abs/2301.08455v1", "pdf_url": "http://arxiv.org/pdf/2301.08455v1", "title": "Spatial Steerability of GANs via Self-Supervision from Discriminator", "abs": "Generative models make huge progress to the photorealistic image synthesis in recent years. To enable human to steer the image generation process and customize the output, many works explore the interpretable dimensions of the latent space in GANs. Existing methods edit the attributes of the output image such as orientation or color scheme by varying the latent code along certain directions. However, these methods usually require additional human annotations for each pretrained model, and they mostly focus on editing global attributes. In this work, we propose a self-supervised approach to improve the spatial steerability of GANs without searching for steerable directions in the latent space or requiring extra annotations. Specifically, we design randomly sampled Gaussian heatmaps to be encoded into the intermediate layers of generative models as spatial inductive bias. Along with training the GAN model from scratch, these heatmaps are being aligned with the emerging attention of the GAN's discriminator in a self-supervised learning manner. During inference, human users can intuitively interact with the spatial heatmaps to edit the output image, such as varying the scene layout or moving objects in the scene. Extensive experiments show that the proposed method not only enables spatial editing over human faces, animal faces, outdoor scenes, and complicated indoor scenes, but also brings improvement in synthesis quality.", "authors": "Jianyuan Wang, Ceyuan Yang, Yinghao Xu, Yujun Shen, Hongdong Li, Bolei Zhou", "first_author": "Bolei Zhou", "first_end_author": "Jianyuan Wang; Bolei Zhou", "publish_time": "2023-01-20 07:36:29+00:00", "update_time": "2023-01-20 07:36:29+00:00"}, {"id": "2204.10419", "url": "http://arxiv.org/abs/2204.10419v2", "pdf_url": "http://arxiv.org/pdf/2204.10419v2", "title": "Learning Sequential Latent Variable Models from Multimodal Time Series   Data", "abs": "Sequential modelling of high-dimensional data is an important problem that appears in many domains including model-based reinforcement learning and dynamics identification for control. Latent variable models applied to sequential data (i.e., latent dynamics models) have been shown to be a particularly effective probabilistic approach to solve this problem, especially when dealing with images. However, in many application areas (e.g., robotics), information from multiple sensing modalities is available -- existing latent dynamics methods have not yet been extended to effectively make use of such multimodal sequential data. Multimodal sensor streams can be correlated in a useful manner and often contain complementary information across modalities. In this work, we present a self-supervised generative modelling framework to jointly learn a probabilistic latent state representation of multimodal data and the respective dynamics. Using synthetic and real-world datasets from a multimodal robotic planar pushing task, we demonstrate that our approach leads to significant improvements in prediction and representation quality. Furthermore, we compare to the common learning baseline of concatenating each modality in the latent space and show that our principled probabilistic formulation performs better. Finally, despite being fully self-supervised, we demonstrate that our method is nearly as effective as an existing supervised approach that relies on ground truth labels.", "authors": "Oliver Limoyo, Trevor Ablett, Jonathan Kelly", "first_author": "Jonathan Kelly", "first_end_author": "Oliver Limoyo; Jonathan Kelly", "publish_time": "2022-04-21 21:59:24+00:00", "update_time": "2023-01-20 07:11:44+00:00"}, {"id": "2204.04457", "url": "http://arxiv.org/abs/2204.04457v3", "pdf_url": "http://arxiv.org/pdf/2204.04457v3", "title": "Refining time-space traffic diagrams: A multiple linear regression model", "abs": "A time-space traffic (TS) diagram, which presents traffic states in time-space cells with color, is an important traffic analysis and visualization tool. Despite its importance for transportation research and engineering, most TS diagrams that have already existed or are being produced are too coarse to exhibit detailed traffic dynamics due to the limitations of existing information technology and traffic infrastructure investment. To increase the resolution of a TS diagram and enable it to present ample traffic details, this paper introduces the TS diagram refinement problem and proposes a multiple linear regression-based model to solve the problem. Two tests, which attempt to increase the resolution of a TS diagram 4 and 16 times, are carried out to evaluate the performance of the proposed model. Data collected at different times, in different locations and even in different countries are employed to thoroughly evaluate the accuracy and transferability of the proposed model. Strict tests with diverse data show that the proposed model, despite its simplicity, is able to refine a TS diagram with promising accuracy and reliable transferability. The proposed refinement model will \"save\" widely existing TS diagrams from their blurry \"faces\" and enable TS diagrams to show more traffic details.", "authors": "Zhengbing He", "first_author": "Zhengbing He", "first_end_author": "Zhengbing He; Zhengbing He", "publish_time": "2022-04-09 12:02:50+00:00", "update_time": "2023-01-20 07:05:59+00:00"}, {"id": "2301.08448", "url": "http://arxiv.org/abs/2301.08448v1", "pdf_url": "http://arxiv.org/pdf/2301.08448v1", "title": "Source-free Subject Adaptation for EEG-based Visual Recognition", "abs": "This paper focuses on subject adaptation for EEG-based visual recognition. It aims at building a visual stimuli recognition system customized for the target subject whose EEG samples are limited, by transferring knowledge from abundant data of source subjects. Existing approaches consider the scenario that samples of source subjects are accessible during training. However, it is often infeasible and problematic to access personal biological data like EEG signals due to privacy issues. In this paper, we introduce a novel and practical problem setup, namely source-free subject adaptation, where the source subject data are unavailable and only the pre-trained model parameters are provided for subject adaptation. To tackle this challenging problem, we propose classifier-based data generation to simulate EEG samples from source subjects using classifier responses. Using the generated samples and target subject data, we perform subject-independent feature learning to exploit the common knowledge shared across different subjects. Notably, our framework is generalizable and can adopt any subject-independent learning method. In the experiments on the EEG-ImageNet40 benchmark, our model brings consistent improvements regardless of the choice of subject-independent learning. Also, our method shows promising performance, recording top-1 test accuracy of 74.6% under the 5-shot setting even without relying on source data. Our code can be found at https://github.com/DeepBCI/Deep-BCI/tree/master/1_Intelligent_BCI/Source_Free_Subject_Adaptation_for_EEG.", "authors": "Pilhyeon Lee, Seogkyu Jeon, Sunhee Hwang, Minjung Shin, Hyeran Byun", "first_author": "Hyeran Byun", "first_end_author": "Pilhyeon Lee; Hyeran Byun", "publish_time": "2023-01-20 07:01:01+00:00", "update_time": "2023-01-20 07:01:01+00:00"}, {"id": "2301.08443", "url": "http://arxiv.org/abs/2301.08443v1", "pdf_url": "http://arxiv.org/pdf/2301.08443v1", "title": "DIFAI: Diverse Facial Inpainting using StyleGAN Inversion", "abs": "Image inpainting is an old problem in computer vision that restores occluded regions and completes damaged images. In the case of facial image inpainting, most of the methods generate only one result for each masked image, even though there are other reasonable possibilities. To prevent any potential biases and unnatural constraints stemming from generating only one image, we propose a novel framework for diverse facial inpainting exploiting the embedding space of StyleGAN. Our framework employs pSp encoder and SeFa algorithm to identify semantic components of the StyleGAN embeddings and feed them into our proposed SPARN decoder that adopts region normalization for plausible inpainting. We demonstrate that our proposed method outperforms several state-of-the-art methods.", "authors": "Dongsik Yoon, Jeong-gi Kwak, Yuanming Li, David Han, Hanseok Ko", "first_author": "Hanseok Ko", "first_end_author": "Dongsik Yoon; Hanseok Ko", "publish_time": "2023-01-20 06:51:34+00:00", "update_time": "2023-01-20 06:51:34+00:00"}, {"id": "2301.08433", "url": "http://arxiv.org/abs/2301.08433v1", "pdf_url": "http://arxiv.org/pdf/2301.08433v1", "title": "Unsupervised Light Field Depth Estimation via Multi-view Feature   Matching with Occlusion Prediction", "abs": "Depth estimation from light field (LF) images is a fundamental step for some applications. Recently, learning-based methods have achieved higher accuracy and efficiency than the traditional methods. However, it is costly to obtain sufficient depth labels for supervised training. In this paper, we propose an unsupervised framework to estimate depth from LF images. First, we design a disparity estimation network (DispNet) with a coarse-to-fine structure to predict disparity maps from different view combinations by performing multi-view feature matching to learn the correspondences more effectively. As occlusions may cause the violation of photo-consistency, we design an occlusion prediction network (OccNet) to predict the occlusion maps, which are used as the element-wise weights of photometric loss to solve the occlusion issue and assist the disparity learning. With the disparity maps estimated by multiple input combinations, we propose a disparity fusion strategy based on the estimated errors with effective occlusion handling to obtain the final disparity map. Experimental results demonstrate that our method achieves superior performance on both the dense and sparse LF images, and also has better generalization ability to the real-world LF images.", "authors": "Shansi Zhang, Nan Meng, Edmund Y. Lam", "first_author": "Edmund Y. Lam", "first_end_author": "Shansi Zhang; Edmund Y. Lam", "publish_time": "2023-01-20 06:11:17+00:00", "update_time": "2023-01-20 06:11:17+00:00"}, {"id": "2204.14030", "url": "http://arxiv.org/abs/2204.14030v4", "pdf_url": "http://arxiv.org/pdf/2204.14030v4", "title": "Neural Implicit Representations for Physical Parameter Inference from a   Single Video", "abs": "Neural networks have recently been used to analyze diverse physical systems and to identify the underlying dynamics. While existing methods achieve impressive results, they are limited by their strong demand for training data and their weak generalization abilities to out-of-distribution data. To overcome these limitations, in this work we propose to combine neural implicit representations for appearance modeling with neural ordinary differential equations (ODEs) for modelling physical phenomena to obtain a dynamic scene representation that can be identified directly from visual observations. Our proposed model combines several unique advantages: (i) Contrary to existing approaches that require large training datasets, we are able to identify physical parameters from only a single video. (ii) The use of neural implicit representations enables the processing of high-resolution videos and the synthesis of photo-realistic images. (iii) The embedded neural ODE has a known parametric form that allows for the identification of interpretable physical parameters, and (iv) long-term prediction in state space. (v) Furthermore, the photo-realistic rendering of novel scenes with modified physical parameters becomes possible.", "authors": "Florian Hofherr, Lukas Koestler, Florian Bernard, Daniel Cremers", "first_author": "Daniel Cremers", "first_end_author": "Florian Hofherr; Daniel Cremers", "publish_time": "2022-04-29 11:55:35+00:00", "update_time": "2023-01-20 05:47:22+00:00"}, {"id": "2301.09416", "url": "http://arxiv.org/abs/2301.09416v1", "pdf_url": "http://arxiv.org/pdf/2301.09416v1", "title": "Towards Robust Video Instance Segmentation with Temporal-Aware   Transformer", "abs": "Most existing transformer based video instance segmentation methods extract per frame features independently, hence it is challenging to solve the appearance deformation problem. In this paper, we observe the temporal information is important as well and we propose TAFormer to aggregate spatio-temporal features both in transformer encoder and decoder. Specifically, in transformer encoder, we propose a novel spatio-temporal joint multi-scale deformable attention module which dynamically integrates the spatial and temporal information to obtain enriched spatio-temporal features. In transformer decoder, we introduce a temporal self-attention module to enhance the frame level box queries with the temporal relation. Moreover, TAFormer adopts an instance level contrastive loss to increase the discriminability of instance query embeddings. Therefore the tracking error caused by visually similar instances can be decreased. Experimental results show that TAFormer effectively leverages the spatial and temporal information to obtain context-aware feature representation and outperforms state-of-the-art methods.", "authors": "Zhenghao Zhang, Fangtao Shao, Zuozhuo Dai, Siyu Zhu", "first_author": "Siyu Zhu", "first_end_author": "Zhenghao Zhang; Siyu Zhu", "publish_time": "2023-01-20 05:22:16+00:00", "update_time": "2023-01-20 05:22:16+00:00"}, {"id": "2301.08414", "url": "http://arxiv.org/abs/2301.08414v1", "pdf_url": "http://arxiv.org/pdf/2301.08414v1", "title": "FG-Depth: Flow-Guided Unsupervised Monocular Depth Estimation", "abs": "The great potential of unsupervised monocular depth estimation has been demonstrated by many works due to low annotation cost and impressive accuracy comparable to supervised methods. To further improve the performance, recent works mainly focus on designing more complex network structures and exploiting extra supervised information, e.g., semantic segmentation. These methods optimize the models by exploiting the reconstructed relationship between the target and reference images in varying degrees. However, previous methods prove that this image reconstruction optimization is prone to get trapped in local minima. In this paper, our core idea is to guide the optimization with prior knowledge from pretrained Flow-Net. And we show that the bottleneck of unsupervised monocular depth estimation can be broken with our simple but effective framework named FG-Depth. In particular, we propose (i) a flow distillation loss to replace the typical photometric loss that limits the capacity of the model and (ii) a prior flow based mask to remove invalid pixels that bring the noise in training loss. Extensive experiments demonstrate the effectiveness of each component, and our approach achieves state-of-the-art results on both KITTI and NYU-Depth-v2 datasets.", "authors": "Junyu Zhu, Lina Liu, Yong Liu, Wanlong Li, Feng Wen, Hongbo Zhang", "first_author": "Hongbo Zhang", "first_end_author": "Junyu Zhu; Hongbo Zhang", "publish_time": "2023-01-20 04:02:13+00:00", "update_time": "2023-01-20 04:02:13+00:00"}, {"id": "2301.08413", "url": "http://arxiv.org/abs/2301.08413v1", "pdf_url": "http://arxiv.org/pdf/2301.08413v1", "title": "When Source-Free Domain Adaptation Meets Label Propagation", "abs": "Source-free domain adaptation, where only a pre-trained source model is used to adapt to the target distribution, is a more general approach to achieving domain adaptation. However, it can be challenging to capture the inherent structure of the target features accurately due to the lack of supervised information on the target domain. To tackle this problem, we propose a novel approach called Adaptive Local Transfer (ALT) that tries to achieve efficient feature clustering from the perspective of label propagation. ALT divides the target data into inner and outlier samples based on the adaptive threshold of the learning state, and applies a customized learning strategy to best fits the data property. Specifically, inner samples are utilized for learning intra-class structure thanks to their relatively well-clustered properties. The low-density outlier samples are regularized by input consistency to achieve high accuracy with respect to the ground truth labels. In this way, local clustering can be prevented from forming spurious clusters while effectively propagating label information among subpopulations. Empirical evidence demonstrates that ALT outperforms the state of the arts on three public benchmarks: Office-31, Office-Home, and VisDA.", "authors": "Chunwei Wu, Guitao Cao, Yan Li, Xidong Xi, Wenming Cao, Hong Wang", "first_author": "Hong Wang", "first_end_author": "Chunwei Wu; Hong Wang", "publish_time": "2023-01-20 03:39:35+00:00", "update_time": "2023-01-20 03:39:35+00:00"}, {"id": "2301.09420", "url": "http://arxiv.org/abs/2301.09420v1", "pdf_url": "http://arxiv.org/pdf/2301.09420v1", "title": "On Multi-Agent Deep Deterministic Policy Gradients and their   Explainability for SMARTS Environment", "abs": "Multi-Agent RL or MARL is one of the complex problems in Autonomous Driving literature that hampers the release of fully-autonomous vehicles today. Several simulators have been in iteration after their inception to mitigate the problem of complex scenarios with multiple agents in Autonomous Driving. One such simulator--SMARTS, discusses the importance of cooperative multi-agent learning. For this problem, we discuss two approaches--MAPPO and MADDPG, which are based on-policy and off-policy RL approaches. We compare our results with the state-of-the-art results for this challenge and discuss the potential areas of improvement while discussing the explainability of these approaches in conjunction with waypoints in the SMARTS environment.", "authors": "Ansh Mittal, Aditya Malte", "first_author": "Aditya Malte", "first_end_author": "Ansh Mittal; Aditya Malte", "publish_time": "2023-01-20 03:17:16+00:00", "update_time": "2023-01-20 03:17:16+00:00"}, {"id": "2301.01201", "url": "http://arxiv.org/abs/2301.01201v2", "pdf_url": "http://arxiv.org/pdf/2301.01201v2", "title": "Uncertainty in Real-Time Semantic Segmentation on Embedded Systems", "abs": "Application for semantic segmentation models in areas such as autonomous vehicles and human computer interaction require real-time predictive capabilities. The challenges of addressing real-time application is amplified by the need to operate on resource constrained hardware. Whilst development of real-time methods for these platforms has increased, these models are unable to sufficiently reason about uncertainty present. This paper addresses this by combining deep feature extraction from pre-trained models with Bayesian regression and moment propagation for uncertainty aware predictions. We demonstrate how the proposed method can yield meaningful uncertainty on embedded hardware in real-time whilst maintaining predictive performance.", "authors": "Ethan Goan, Clinton Fookes", "first_author": "Clinton Fookes", "first_end_author": "Ethan Goan; Clinton Fookes", "publish_time": "2022-12-20 07:32:12+00:00", "update_time": "2023-01-20 03:13:47+00:00"}, {"id": "2301.08408", "url": "http://arxiv.org/abs/2301.08408v1", "pdf_url": "http://arxiv.org/pdf/2301.08408v1", "title": "Identity masking effectiveness and gesture recognition: Effects of eye   enhancement in seeing through the mask", "abs": "Face identity masking algorithms developed in recent years aim to protect the privacy of people in video recordings. These algorithms are designed to interfere with identification, while preserving information about facial actions. An important challenge is to preserve subtle actions in the eye region, while obscuring the salient identity cues from the eyes. We evaluated the effectiveness of identity-masking algorithms based on Canny filters, applied with and without eye enhancement, for interfering with identification and preserving facial actions. In Experiments 1 and 2, we tested human participants' ability to match the facial identity of a driver in a low resolution video to a high resolution facial image. Results showed that both masking methods impaired identification, and that eye enhancement did not alter the effectiveness of the Canny filter mask. In Experiment 3, we tested action preservation and found that neither method interfered significantly with driver action perception. We conclude that relatively simple, filter-based masking algorithms, which are suitable for application to low quality video, can be used in privacy protection without compromising action perception.", "authors": "Madeline Rachow, Thomas Karnowski, Alice J. O'Toole", "first_author": "Alice J. O'Toole", "first_end_author": "Madeline Rachow; Alice J. O'Toole", "publish_time": "2023-01-20 03:10:19+00:00", "update_time": "2023-01-20 03:10:19+00:00"}, {"id": "2209.08162", "url": "http://arxiv.org/abs/2209.08162v2", "pdf_url": "http://arxiv.org/pdf/2209.08162v2", "title": "Uncertainty Quantification of Collaborative Detection for Self-Driving", "abs": "Sharing information between connected and autonomous vehicles (CAVs) fundamentally improves the performance of collaborative object detection for self-driving. However, CAVs still have uncertainties on object detection due to practical challenges, which will affect the later modules in self-driving such as planning and control. Hence, uncertainty quantification is crucial for safety-critical systems such as CAVs. Our work is the first to estimate the uncertainty of collaborative object detection. We propose a novel uncertainty quantification method, called Double-M Quantification, which tailors a moving block bootstrap (MBB) algorithm with direct modeling of the multivariant Gaussian distribution of each corner of the bounding box. Our method captures both the epistemic uncertainty and aleatoric uncertainty with one inference pass based on the offline Double-M training process. And it can be used with different collaborative object detectors. Through experiments on the comprehensive collaborative perception dataset, we show that our Double-M method achieves more than 4X improvement on uncertainty score and more than 3% accuracy improvement, compared with the state-of-the-art uncertainty quantification methods. Our code is public on https://coperception.github.io/double-m-quantification.", "authors": "Sanbao Su, Yiming Li, Sihong He, Songyang Han, Chen Feng, Caiwen Ding, Fei Miao", "first_author": "Fei Miao", "first_end_author": "Sanbao Su; Fei Miao", "publish_time": "2022-09-16 20:30:45+00:00", "update_time": "2023-01-20 02:57:46+00:00"}, {"id": "2201.09169", "url": "http://arxiv.org/abs/2201.09169v2", "pdf_url": "http://arxiv.org/pdf/2201.09169v2", "title": "Rich Action-semantic Consistent Knowledge for Early Action Prediction", "abs": "Early action prediction (EAP) aims to recognize human actions from a part of action execution in ongoing videos, which is an important task for many practical applications. Most prior works treat partial or full videos as a whole, ignoring rich action knowledge hidden in videos, i.e., semantic consistencies among different partial videos. In contrast, we partition original partial or full videos to form a new series of partial videos and mine the Action Semantic Consistent Knowledge (ASCK) among these new partial videos evolving in arbitrary progress levels. Moreover, a novel Rich Action-semantic Consistent Knowledge network (RACK) under the teacher-student framework is proposed for EAP. Firstly, we use a two-stream pre-trained model to extract features of videos. Secondly, we treat the RGB or flow features of the partial videos as nodes and their action semantic consistencies as edges. Next, we build a bi-directional semantic graph for the teacher network and a single-directional semantic graph for the student network to model rich ASCK among partial videos. The MSE and MMD losses are incorporated as our distillation loss to enrich the ASCK of partial videos from the teacher to the student network. Finally, we obtain the final prediction by summering the logits of different sub-networks and applying a softmax layer. Extensive experiments and ablative studies have been conducted, demonstrating the effectiveness of modeling rich ASCK for EAP. With the proposed RACK, we have achieved state-of-the-art performance on three benchmarks. The code will be released if the paper is accepted.", "authors": "Xiaoli Liu, Jianqin Yin, Di Guo", "first_author": "Di Guo", "first_end_author": "Xiaoli Liu; Di Guo", "publish_time": "2022-01-23 03:39:31+00:00", "update_time": "2023-01-20 02:57:36+00:00"}, {"id": "2301.09422", "url": "http://arxiv.org/abs/2301.09422v1", "pdf_url": "http://arxiv.org/pdf/2301.09422v1", "title": "HALOC: Hardware-Aware Automatic Low-Rank Compression for Compact Neural   Networks", "abs": "Low-rank compression is an important model compression strategy for obtaining compact neural network models. In general, because the rank values directly determine the model complexity and model accuracy, proper selection of layer-wise rank is very critical and desired. To date, though many low-rank compression approaches, either selecting the ranks in a manual or automatic way, have been proposed, they suffer from costly manual trials or unsatisfied compression performance. In addition, all of the existing works are not designed in a hardware-aware way, limiting the practical performance of the compressed models on real-world hardware platforms.   To address these challenges, in this paper we propose HALOC, a hardware-aware automatic low-rank compression framework. By interpreting automatic rank selection from an architecture search perspective, we develop an end-to-end solution to determine the suitable layer-wise ranks in a differentiable and hardware-aware way. We further propose design principles and mitigation strategy to efficiently explore the rank space and reduce the potential interference problem.   Experimental results on different datasets and hardware platforms demonstrate the effectiveness of our proposed approach. On CIFAR-10 dataset, HALOC enables 0.07% and 0.38% accuracy increase over the uncompressed ResNet-20 and VGG-16 models with 72.20% and 86.44% fewer FLOPs, respectively. On ImageNet dataset, HALOC achieves 0.9% higher top-1 accuracy than the original ResNet-18 model with 66.16% fewer FLOPs. HALOC also shows 0.66% higher top-1 accuracy increase than the state-of-the-art automatic low-rank compression solution with fewer computational and memory costs. In addition, HALOC demonstrates the practical speedups on different hardware platforms, verified by the measurement results on desktop GPU, embedded GPU and ASIC accelerator.", "authors": "Jinqi Xiao, Chengming Zhang, Yu Gong, Miao Yin, Yang Sui, Lizhi Xiang, Dingwen Tao, Bo Yuan", "first_author": "Bo Yuan", "first_end_author": "Jinqi Xiao; Bo Yuan", "publish_time": "2023-01-20 01:57:34+00:00", "update_time": "2023-01-20 01:57:34+00:00"}, {"id": "2301.08390", "url": "http://arxiv.org/abs/2301.08390v1", "pdf_url": "http://arxiv.org/pdf/2301.08390v1", "title": "Open-Set Likelihood Maximization for Few-Shot Learning", "abs": "We tackle the Few-Shot Open-Set Recognition (FSOSR) problem, i.e. classifying instances among a set of classes for which we only have a few labeled samples, while simultaneously detecting instances that do not belong to any known class. We explore the popular transductive setting, which leverages the unlabelled query instances at inference. Motivated by the observation that existing transductive methods perform poorly in open-set scenarios, we propose a generalization of the maximum likelihood principle, in which latent scores down-weighing the influence of potential outliers are introduced alongside the usual parametric model. Our formulation embeds supervision constraints from the support set and additional penalties discouraging overconfident predictions on the query set. We proceed with a block-coordinate descent, with the latent scores and parametric model co-optimized alternately, thereby benefiting from each other. We call our resulting formulation \\textit{Open-Set Likelihood Optimization} (OSLO). OSLO is interpretable and fully modular; it can be applied on top of any pre-trained model seamlessly. Through extensive experiments, we show that our method surpasses existing inductive and transductive methods on both aspects of open-set recognition, namely inlier classification and outlier detection.", "authors": "Malik Boudiaf, Etienne Bennequin, Myriam Tami, Antoine Toubhans, Pablo Piantanida, C\u00e9line Hudelot, Ismail Ben Ayed", "first_author": "Ismail Ben Ayed", "first_end_author": "Malik Boudiaf; Ismail Ben Ayed", "publish_time": "2023-01-20 01:56:19+00:00", "update_time": "2023-01-20 01:56:19+00:00"}, {"id": "2301.08387", "url": "http://arxiv.org/abs/2301.08387v1", "pdf_url": "http://arxiv.org/pdf/2301.08387v1", "title": "Occlusion Reasoning for Skeleton Extraction of Self-Occluded Tree   Canopies", "abs": "In this work, we present a method to extract the skeleton of a self-occluded tree canopy by estimating the unobserved structures of the tree. A tree skeleton compactly describes the topological structure and contains useful information such as branch geometry, positions and hierarchy. This can be critical to planning contact interactions for agricultural manipulation, yet is difficult to gain due to occlusion by leaves, fruits and other branches. Our method uses an instance segmentation network to detect visible trunk, branches, and twigs. Then, based on the observed tree structures, we build a custom 3D likelihood map in the form of an occupancy grid to hypothesize on the presence of occluded skeletons through a series of minimum cost path searches. We show that our method outperforms baseline methods in highly occluded scenes, demonstrated through a set of experiments on a synthetic tree dataset. Qualitative results are also presented on a real tree dataset collected from the field.", "authors": "Chung Hee Kim, George Kantor", "first_author": "George Kantor", "first_end_author": "Chung Hee Kim; George Kantor", "publish_time": "2023-01-20 01:46:07+00:00", "update_time": "2023-01-20 01:46:07+00:00"}, {"id": "2212.07398", "url": "http://arxiv.org/abs/2212.07398v2", "pdf_url": "http://arxiv.org/pdf/2212.07398v2", "title": "Self-Play and Self-Describe: Policy Adaptation with Vision-Language   Foundation Models", "abs": "Recent progress on vision-language foundation models have brought significant advancement to building general-purpose robots. By using the pre-trained models to encode the scene and instructions as inputs for decision making, the instruction-conditioned policy can generalize across different objects and tasks. While this is encouraging, the policy still fails in most cases given an unseen task or environment. To adapt the policy to unseen tasks and environments, we explore a new paradigm on leveraging the pre-trained foundation models with Self-PLAY and Self-Describe (SPLAYD). When deploying the trained policy to a new task or a new environment, we first let the policy self-play with randomly generated instructions to record the demonstrations. While the execution could be wrong, we can use the pre-trained foundation models to accurately self-describe (i.e., re-label or classify) the demonstrations. This automatically provides new pairs of demonstration-instruction data for policy fine-tuning. We evaluate our method on a broad range of experiments with the focus on generalization on unseen objects, unseen tasks, unseen environments, and sim-to-real transfer. We show SPLAYD improves baselines by a large margin in all cases. Our project page is available at https://geyuying.github.io/SPLAYD/", "authors": "Yuying Ge, Annabella Macaluso, Li Erran Li, Ping Luo, Xiaolong Wang", "first_author": "Xiaolong Wang", "first_end_author": "Yuying Ge; Xiaolong Wang", "publish_time": "2022-12-14 18:31:47+00:00", "update_time": "2023-01-20 00:47:23+00:00"}, {"id": "2301.08365", "url": "http://arxiv.org/abs/2301.08365v1", "pdf_url": "http://arxiv.org/pdf/2301.08365v1", "title": "On Retrospective $k$-space Subsampling schemes For Deep MRI   Reconstruction", "abs": "$\\textbf{Purpose:}$ The MRI $k$-space acquisition is time consuming. Traditional techniques aim to acquire accelerated data, which in conjunction with recent DL methods, aid in producing high-fidelity images in truncated times. Conventionally, subsampling the $k$-space is performed by utilizing Cartesian-rectilinear trajectories, which even with the use of DL, provide imprecise reconstructions, though, a plethora of non-rectilinear or non-Cartesian trajectories can be implemented in modern MRI scanners. This work investigates the effect of the $k$-space subsampling scheme on the quality of reconstructed accelerated MRI measurements produced by trained DL models.   $\\textbf{Methods:}$ The RecurrentVarNet was used as the DL-based MRI-reconstruction architecture. Cartesian fully-sampled multi-coil $k$-space measurements from three datasets with different accelerations were retrospectively subsampled using eight distinct subsampling schemes (four Cartesian-rectilinear, two Cartesian non-rectilinear, two non-Cartesian). Experiments were conducted in two frameworks: Scheme-specific, where a distinct model was trained and evaluated for each dataset-subsampling scheme pair, and multi-scheme, where for each dataset a single model was trained on data randomly subsampled by any of the eight schemes and evaluated on data subsampled by all schemes.   $\\textbf{Results:}$ In the scheme-specific setting RecurrentVarNets trained and evaluated on non-rectilinearly subsampled data demonstrated superior performance especially for high accelerations, whilst in the multi-scheme setting, reconstruction performance on rectilinearly subsampled data improved when compared to the scheme-specific experiments.   $\\textbf{Conclusion:}$ Training DL-based MRI reconstruction algorithms on non-rectilinearly subsampled measurements can produce more faithful reconstructions.", "authors": "George Yiasemis, Clara I. S\u00e1nchez, Jan-Jakob Sonke, Jonas Teuwen", "first_author": "Jonas Teuwen", "first_end_author": "George Yiasemis; Jonas Teuwen", "publish_time": "2023-01-20 00:05:18+00:00", "update_time": "2023-01-20 00:05:18+00:00"}, {"id": "2110.05668", "url": "http://arxiv.org/abs/2110.05668v6", "pdf_url": "http://arxiv.org/pdf/2110.05668v6", "title": "NAS-Bench-360: Benchmarking Neural Architecture Search on Diverse Tasks", "abs": "Most existing neural architecture search (NAS) benchmarks and algorithms prioritize well-studied tasks, e.g. image classification on CIFAR or ImageNet. This makes the performance of NAS approaches in more diverse areas poorly understood. In this paper, we present NAS-Bench-360, a benchmark suite to evaluate methods on domains beyond those traditionally studied in architecture search, and use it to address the following question: do state-of-the-art NAS methods perform well on diverse tasks? To construct the benchmark, we curate ten tasks spanning a diverse array of application domains, dataset sizes, problem dimensionalities, and learning objectives. Each task is carefully chosen to interoperate with modern CNN-based search methods while possibly being far-afield from its original development domain. To speed up and reduce the cost of NAS research, for two of the tasks we release the precomputed performance of 15,625 architectures comprising a standard CNN search space. Experimentally, we show the need for more robust NAS evaluation of the kind NAS-Bench-360 enables by showing that several modern NAS procedures perform inconsistently across the ten tasks, with many catastrophically poor results. We also demonstrate how NAS-Bench-360 and its associated precomputed results will enable future scientific discoveries by testing whether several recent hypotheses promoted in the NAS literature hold on diverse tasks. NAS-Bench-360 is hosted at https://nb360.ml.cmu.edu.", "authors": "Renbo Tu, Nicholas Roberts, Mikhail Khodak, Junhong Shen, Frederic Sala, Ameet Talwalkar", "first_author": "Ameet Talwalkar", "first_end_author": "Renbo Tu; Ameet Talwalkar", "publish_time": "2021-10-12 01:13:18+00:00", "update_time": "2023-01-19 23:17:16+00:00"}, {"id": "2007.12140", "url": "http://arxiv.org/abs/2007.12140v5", "pdf_url": "http://arxiv.org/pdf/2007.12140v5", "title": "HITNet: Hierarchical Iterative Tile Refinement Network for Real-time   Stereo Matching", "abs": "This paper presents HITNet, a novel neural network architecture for real-time stereo matching. Contrary to many recent neural network approaches that operate on a full cost volume and rely on 3D convolutions, our approach does not explicitly build a volume and instead relies on a fast multi-resolution initialization step, differentiable 2D geometric propagation and warping mechanisms to infer disparity hypotheses. To achieve a high level of accuracy, our network not only geometrically reasons about disparities but also infers slanted plane hypotheses allowing to more accurately perform geometric warping and upsampling operations. Our architecture is inherently multi-resolution allowing the propagation of information across different levels. Multiple experiments prove the effectiveness of the proposed approach at a fraction of the computation required by state-of-the-art methods. At the time of writing, HITNet ranks 1st-3rd on all the metrics published on the ETH3D website for two view stereo, ranks 1st on most of the metrics among all the end-to-end learning approaches on Middlebury-v3, ranks 1st on the popular KITTI 2012 and 2015 benchmarks among the published methods faster than 100ms.", "authors": "Vladimir Tankovich, Christian H\u00e4ne, Yinda Zhang, Adarsh Kowdle, Sean Fanello, Sofien Bouaziz", "first_author": "Sofien Bouaziz", "first_end_author": "Vladimir Tankovich; Sofien Bouaziz", "publish_time": "2020-07-23 17:11:48+00:00", "update_time": "2023-01-19 23:15:04+00:00"}, {"id": "2301.03045", "url": "http://arxiv.org/abs/2301.03045v2", "pdf_url": "http://arxiv.org/pdf/2301.03045v2", "title": "Seamless Multimodal Biometrics for Continuous Personalised Wellbeing   Monitoring", "abs": "Artificially intelligent perception is increasingly present in the lives of every one of us. Vehicles are no exception, (...) In the near future, pattern recognition will have an even stronger role in vehicles, as self-driving cars will require automated ways to understand what is happening around (and within) them and act accordingly. (...) This doctoral work focused on advancing in-vehicle sensing through the research of novel computer vision and pattern recognition methodologies for both biometrics and wellbeing monitoring. The main focus has been on electrocardiogram (ECG) biometrics, a trait well-known for its potential for seamless driver monitoring. Major efforts were devoted to achieving improved performance in identification and identity verification in off-the-person scenarios, well-known for increased noise and variability. Here, end-to-end deep learning ECG biometric solutions were proposed and important topics were addressed such as cross-database and long-term performance, waveform relevance through explainability, and interlead conversion. Face biometrics, a natural complement to the ECG in seamless unconstrained scenarios, was also studied in this work. The open challenges of masked face recognition and interpretability in biometrics were tackled in an effort to evolve towards algorithms that are more transparent, trustworthy, and robust to significant occlusions. Within the topic of wellbeing monitoring, improved solutions to multimodal emotion recognition in groups of people and activity/violence recognition in in-vehicle scenarios were proposed. At last, we also proposed a novel way to learn template security within end-to-end models, dismissing additional separate encryption processes, and a self-supervised learning approach tailored to sequential data, in order to ensure data security and optimal performance. (...)", "authors": "Jo\u00e3o Ribeiro Pinto", "first_author": "Jo\u00e3o Ribeiro Pinto", "first_end_author": "Jo\u00e3o Ribeiro Pinto; Jo\u00e3o Ribeiro Pinto", "publish_time": "2023-01-08 14:07:01+00:00", "update_time": "2023-01-19 22:29:35+00:00"}, {"id": "2208.09198", "url": "http://arxiv.org/abs/2208.09198v2", "pdf_url": "http://arxiv.org/pdf/2208.09198v2", "title": "TTT-UCDR: Test-time Training for Universal Cross-Domain Retrieval", "abs": "Image retrieval under generalized test scenarios has gained significant momentum in literature, and the recently proposed protocol of Universal Cross-domain Retrieval is a pioneer in this direction. A common practice in any such generalized classification or retrieval algorithm is to exploit samples from multiple domains during training to learn a domain-invariant representation of data. Such criterion is often restrictive, and thus in this work, for the first time, we explore the challenges associated with generalized retrieval problems under a low-data regime, which is quite relevant in many real-world scenarios. We attempt to make any retrieval model trained on a small cross-domain dataset (containing just two training domains) more generalizable towards any unknown query domain or category by quickly adapting it to the test data during inference. This form of test-time training or adaptation of the retrieval model is explored by means of a number of self-supervision-based loss functions, for example, Rotnet, Jigsaw-puzzle, Barlow twins, etc., in this work. Extensive experiments on multiple large-scale datasets demonstrate the effectiveness of the proposed approach.", "authors": "Soumava Paul, Titir Dutta, Aheli Saha, Abhishek Samanta, Soma Biswas", "first_author": "Soma Biswas", "first_end_author": "Soumava Paul; Soma Biswas", "publish_time": "2022-08-19 07:50:04+00:00", "update_time": "2023-01-19 22:14:54+00:00"}, {"id": "2301.08330", "url": "http://arxiv.org/abs/2301.08330v1", "pdf_url": "http://arxiv.org/pdf/2301.08330v1", "title": "The role of noise in denoising models for anomaly detection in medical   images", "abs": "Pathological brain lesions exhibit diverse appearance in brain images, in terms of intensity, texture, shape, size, and location. Comprehensive sets of data and annotations are difficult to acquire. Therefore, unsupervised anomaly detection approaches have been proposed using only normal data for training, with the aim of detecting outlier anomalous voxels at test time. Denoising methods, for instance classical denoising autoencoders (DAEs) and more recently emerging diffusion models, are a promising approach, however naive application of pixelwise noise leads to poor anomaly detection performance. We show that optimization of the spatial resolution and magnitude of the noise improves the performance of different model training regimes, with similar noise parameter adjustments giving good performance for both DAEs and diffusion models. Visual inspection of the reconstructions suggests that the training noise influences the trade-off between the extent of the detail that is reconstructed and the extent of erasure of anomalies, both of which contribute to better anomaly detection performance. We validate our findings on two real-world datasets (tumor detection in brain MRI and hemorrhage/ischemia/tumor detection in brain CT), showing good detection on diverse anomaly appearances. Overall, we find that a DAE trained with coarse noise is a fast and simple method that gives state-of-the-art accuracy. Diffusion models applied to anomaly detection are as yet in their infancy and provide a promising avenue for further research.", "authors": "Antanas Kascenas, Pedro Sanchez, Patrick Schrempf, Chaoyang Wang, William Clackett, Shadia S. Mikhael, Jeremy P. Voisey, Keith Goatman, Alexander Weir, Nicolas Pugeault, Sotirios A. Tsaftaris, Alison Q. O'Neil", "first_author": "Alison Q. O'Neil", "first_end_author": "Antanas Kascenas; Alison Q. O'Neil", "publish_time": "2023-01-19 21:39:38+00:00", "update_time": "2023-01-19 21:39:38+00:00"}, {"id": "2301.08317", "url": "http://arxiv.org/abs/2301.08317v1", "pdf_url": "http://arxiv.org/pdf/2301.08317v1", "title": "Learning ultrasound plane pose regression: assessing generalized pose   coordinates in the fetal brain", "abs": "In obstetric ultrasound (US) scanning, the learner's ability to mentally build a three-dimensional (3D) map of the fetus from a two-dimensional (2D) US image represents a significant challenge in skill acquisition. We aim to build a US plane localization system for 3D visualization, training, and guidance without integrating additional sensors. This work builds on top of our previous work, which predicts the six-dimensional (6D) pose of arbitrarily-oriented US planes slicing the fetal brain with respect to a normalized reference frame using a convolutional neural network (CNN) regression network. Here, we analyze in detail the assumptions of the normalized fetal brain reference frame and quantify its accuracy with respect to the acquisition of transventricular (TV) standard plane (SP) for fetal biometry. We investigate the impact of registration quality in the training and testing data and its subsequent effect on trained models. Finally, we introduce data augmentations and larger training sets that improve the results of our previous work, achieving median errors of 3.53 mm and 6.42 degrees for translation and rotation, respectively.", "authors": "Chiara Di Vece, Maela Le Lous, Brian Dromey, Francisco Vasconcelos, Anna L David, Donald Peebles, Danail Stoyanov", "first_author": "Danail Stoyanov", "first_end_author": "Chiara Di Vece; Danail Stoyanov", "publish_time": "2023-01-19 21:16:36+00:00", "update_time": "2023-01-19 21:16:36+00:00"}, {"id": "2301.08247", "url": "http://arxiv.org/abs/2301.08247v1", "pdf_url": "http://arxiv.org/pdf/2301.08247v1", "title": "Multiview Compressive Coding for 3D Reconstruction", "abs": "A central goal of visual recognition is to understand objects and scenes from a single image. 2D recognition has witnessed tremendous progress thanks to large-scale learning and general-purpose representations. Comparatively, 3D poses new challenges stemming from occlusions not depicted in the image. Prior works try to overcome these by inferring from multiple views or rely on scarce CAD models and category-specific priors which hinder scaling to novel settings. In this work, we explore single-view 3D reconstruction by learning generalizable representations inspired by advances in self-supervised learning. We introduce a simple framework that operates on 3D points of single objects or whole scenes coupled with category-agnostic large-scale training from diverse RGB-D videos. Our model, Multiview Compressive Coding (MCC), learns to compress the input appearance and geometry to predict the 3D structure by querying a 3D-aware decoder. MCC's generality and efficiency allow it to learn from large-scale and diverse data sources with strong generalization to novel objects imagined by DALL$\\cdot$E 2 or captured in-the-wild with an iPhone.", "authors": "Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph Feichtenhofer, Georgia Gkioxari", "first_author": "Georgia Gkioxari", "first_end_author": "Chao-Yuan Wu; Georgia Gkioxari", "publish_time": "2023-01-19 18:59:52+00:00", "update_time": "2023-01-19 18:59:52+00:00"}, {"id": "2211.02578", "url": "http://arxiv.org/abs/2211.02578v2", "pdf_url": "http://arxiv.org/pdf/2211.02578v2", "title": "Data Models for Dataset Drift Controls in Machine Learning With Images", "abs": "Camera images are ubiquitous in machine learning research. They also play a central role in the delivery of important services spanning medicine and environmental surveying. However, the application of machine learning models in these domains has been limited because of robustness concerns. A primary failure mode are performance drops due to differences between the training and deployment data. While there are methods to prospectively validate the robustness of machine learning models to such dataset drifts, existing approaches do not account for explicit models of the primary object of interest: the data. This limits our ability to study and understand the relationship between data generation and downstream machine learning model performance in a physically accurate manner. In this study, we demonstrate how to overcome this limitation by pairing traditional machine learning with physical optics to obtain explicit and differentiable data models. We demonstrate how such data models can be constructed for image data and used to control downstream machine learning model performance related to dataset drift. The findings are distilled into three applications. First, drift synthesis enables the controlled generation of physically faithful drift test cases to power model selection and targeted generalization. Second, the gradient connection between machine learning task model and data model allows advanced, precise tolerancing of task model sensitivity to changes in the data generation. These drift forensics can be used to precisely specify the acceptable data environments in which a task model may be run. Third, drift optimization opens up the possibility to create drifts that can help the task model learn better faster, effectively optimizing the data generating process itself. A guide to access the open code and datasets is available at https://github.com/aiaudit-org/raw2logit.", "authors": "Luis Oala, Marco Aversa, Gabriel Nobis, Kurt Willis, Yoan Neuenschwander, Mich\u00e8le Buck, Christian Matek, Jerome Extermann, Enrico Pomarico, Wojciech Samek, Roderick Murray-Smith, Christoph Clausen, Bruno Sanguinetti", "first_author": "Bruno Sanguinetti", "first_end_author": "Luis Oala; Bruno Sanguinetti", "publish_time": "2022-11-04 16:50:10+00:00", "update_time": "2023-01-19 18:59:45+00:00"}, {"id": "2301.08245", "url": "http://arxiv.org/abs/2301.08245v1", "pdf_url": "http://arxiv.org/pdf/2301.08245v1", "title": "Booster: a Benchmark for Depth from Images of Specular and Transparent   Surfaces", "abs": "Estimating depth from images nowadays yields outstanding results, both in terms of in-domain accuracy and generalization. However, we identify two main challenges that remain open in this field: dealing with non-Lambertian materials and effectively processing high-resolution images. Purposely, we propose a novel dataset that includes accurate and dense ground-truth labels at high resolution, featuring scenes containing several specular and transparent surfaces. Our acquisition pipeline leverages a novel deep space-time stereo framework, enabling easy and accurate labeling with sub-pixel precision. The dataset is composed of 606 samples collected in 85 different scenes, each sample includes both a high-resolution pair (12 Mpx) as well as an unbalanced stereo pair (Left: 12 Mpx, Right: 1.1 Mpx). Additionally, we provide manually annotated material segmentation masks and 15K unlabeled samples. We divide the dataset into a training set, and two testing sets, the latter devoted to the evaluation of stereo and monocular depth estimation networks respectively to highlight the open challenges and future research directions in this field.", "authors": "Pierluigi Zama Ramirez, Alex Costanzino, Fabio Tosi, Matteo Poggi, Samuele Salti, Stefano Mattoccia, Luigi Di Stefano", "first_author": "Luigi Di Stefano", "first_end_author": "Pierluigi Zama Ramirez; Luigi Di Stefano", "publish_time": "2023-01-19 18:59:28+00:00", "update_time": "2023-01-19 18:59:28+00:00"}, {"id": "2301.08243", "url": "http://arxiv.org/abs/2301.08243v1", "pdf_url": "http://arxiv.org/pdf/2301.08243v1", "title": "Self-Supervised Learning from Images with a Joint-Embedding Predictive   Architecture", "abs": "This paper demonstrates an approach for learning highly semantic image representations without relying on hand-crafted data-augmentations. We introduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), a non-generative approach for self-supervised learning from images. The idea behind I-JEPA is simple: from a single context block, predict the representations of various target blocks in the same image. A core design choice to guide I-JEPA towards producing semantic representations is the masking strategy; specifically, it is crucial to (a) predict several target blocks in the image, (b) sample target blocks with sufficiently large scale (occupying 15%-20% of the image), and (c) use a sufficiently informative (spatially distributed) context block. Empirically, when combined with Vision Transformers, we find I-JEPA to be highly scalable. For instance, we train a ViT-Huge/16 on ImageNet using 32 A100 GPUs in under 38 hours to achieve strong downstream performance across a wide range of tasks requiring various levels of abstraction, from linear classification to object counting and depth prediction.", "authors": "Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, Nicolas Ballas", "first_author": "Nicolas Ballas", "first_end_author": "Mahmoud Assran; Nicolas Ballas", "publish_time": "2023-01-19 18:59:01+00:00", "update_time": "2023-01-19 18:59:01+00:00"}, {"id": "2301.08237", "url": "http://arxiv.org/abs/2301.08237v1", "pdf_url": "http://arxiv.org/pdf/2301.08237v1", "title": "LoCoNet: Long-Short Context Network for Active Speaker Detection", "abs": "Active Speaker Detection (ASD) aims to identify who is speaking in each frame of a video. ASD reasons from audio and visual information from two contexts: long-term intra-speaker context and short-term inter-speaker context. Long-term intra-speaker context models the temporal dependencies of the same speaker, while short-term inter-speaker context models the interactions of speakers in the same scene. These two contexts are complementary to each other and can help infer the active speaker. Motivated by these observations, we propose LoCoNet, a simple yet effective Long-Short Context Network that models the long-term intra-speaker context and short-term inter-speaker context. We use self-attention to model long-term intra-speaker context due to its effectiveness in modeling long-range dependencies, and convolutional blocks that capture local patterns to model short-term inter-speaker context. Extensive experiments show that LoCoNet achieves state-of-the-art performance on multiple datasets, achieving an mAP of 95.2%(+1.1%) on AVA-ActiveSpeaker, 68.1%(+22%) on Columbia dataset, 97.2%(+2.8%) on Talkies dataset and 59.7%(+8.0%) on Ego4D dataset. Moreover, in challenging cases where multiple speakers are present, or face of active speaker is much smaller than other faces in the same scene, LoCoNet outperforms previous state-of-the-art methods by 3.4% on the AVA-ActiveSpeaker dataset. The code will be released at https://github.com/SJTUwxz/LoCoNet_ASD.", "authors": "Xizi Wang, Feng Cheng, Gedas Bertasius, David Crandall", "first_author": "David Crandall", "first_end_author": "Xizi Wang; David Crandall", "publish_time": "2023-01-19 18:54:43+00:00", "update_time": "2023-01-19 18:54:43+00:00"}, {"id": "2301.08229", "url": "http://arxiv.org/abs/2301.08229v1", "pdf_url": "http://arxiv.org/pdf/2301.08229v1", "title": "Estimating Remaining Lifespan from the Face", "abs": "The face is a rich source of information that can be utilized to infer a person's biological age, sex, phenotype, genetic defects, and health status. All of these factors are relevant for predicting an individual's remaining lifespan. In this study, we collected a dataset of over 24,000 images (from Wikidata/Wikipedia) of individuals who died of natural causes, along with the number of years between when the image was taken and when the person passed away. We made this dataset publicly available. We fine-tuned multiple Convolutional Neural Network (CNN) models on this data, at best achieving a mean absolute error of 8.3 years in the validation data using VGGFace. However, the model's performance diminishes when the person was younger at the time of the image. To demonstrate the potential applications of our remaining lifespan model, we present examples of using it to estimate the average loss of life (in years) due to the COVID-19 pandemic and to predict the increase in life expectancy that might result from a health intervention such as weight loss. Additionally, we discuss the ethical considerations associated with such models.", "authors": "Amir Fekrazad", "first_author": "Amir Fekrazad", "first_end_author": "Amir Fekrazad; Amir Fekrazad", "publish_time": "2023-01-19 18:38:04+00:00", "update_time": "2023-01-19 18:38:04+00:00"}, {"id": "2301.08189", "url": "http://arxiv.org/abs/2301.08189v1", "pdf_url": "http://arxiv.org/pdf/2301.08189v1", "title": "Benchmarking YOLOv5 and YOLOv7 models with DeepSORT for droplet tracking   applications", "abs": "Tracking droplets in microfluidics is a challenging task. The difficulty arises in choosing a tool to analyze general microfluidic videos to infer physical quantities. The state-of-the-art object detector algorithm You Only Look Once (YOLO) and the object tracking algorithm Simple Online and Realtime Tracking with a Deep Association Metric (DeepSORT) are customizable for droplet identification and tracking. The customization includes training YOLO and DeepSORT networks to identify and track the objects of interest. We trained several YOLOv5 and YOLOv7 models and the DeepSORT network for droplet identification and tracking from microfluidic experimental videos. We compare the performance of the droplet tracking applications with YOLOv5 and YOLOv7 in terms of training time and time to analyze a given video across various hardware configurations. Despite the latest YOLOv7 being 10% faster, the real-time tracking is only achieved by lighter YOLO models on RTX 3070 Ti GPU machine due to additional significant droplet tracking costs arising from the DeepSORT algorithm. This work is a benchmark study for the YOLOv5 and YOLOv7 networks with DeepSORT in terms of the training time and inference time for a custom dataset of microfluidic droplets.", "authors": "Mihir Durve, Sibilla Orsini, Adriano Tiribocchi, Andrea Montessori, Jean-Michel Tucny, Marco Lauricella, Andrea Camposeo, Dario Pisignano, Sauro Succi", "first_author": "Sauro Succi", "first_end_author": "Mihir Durve; Sauro Succi", "publish_time": "2023-01-19 17:37:40+00:00", "update_time": "2023-01-19 17:37:40+00:00"}, {"id": "2301.08187", "url": "http://arxiv.org/abs/2301.08187v1", "pdf_url": "http://arxiv.org/pdf/2301.08187v1", "title": "A Multi-Resolution Framework for U-Nets with Applications to   Hierarchical VAEs", "abs": "U-Net architectures are ubiquitous in state-of-the-art deep learning, however their regularisation properties and relationship to wavelets are understudied. In this paper, we formulate a multi-resolution framework which identifies U-Nets as finite-dimensional truncations of models on an infinite-dimensional function space. We provide theoretical results which prove that average pooling corresponds to projection within the space of square-integrable functions and show that U-Nets with average pooling implicitly learn a Haar wavelet basis representation of the data. We then leverage our framework to identify state-of-the-art hierarchical VAEs (HVAEs), which have a U-Net architecture, as a type of two-step forward Euler discretisation of multi-resolution diffusion processes which flow from a point mass, introducing sampling instabilities. We also demonstrate that HVAEs learn a representation of time which allows for improved parameter efficiency through weight-sharing. We use this observation to achieve state-of-the-art HVAE performance with half the number of parameters of existing models, exploiting the properties of our continuous-time formulation.", "authors": "Fabian Falck, Christopher Williams, Dominic Danks, George Deligiannidis, Christopher Yau, Chris Holmes, Arnaud Doucet, Matthew Willetts", "first_author": "Matthew Willetts", "first_end_author": "Fabian Falck; Matthew Willetts", "publish_time": "2023-01-19 17:33:48+00:00", "update_time": "2023-01-19 17:33:48+00:00"}, {"id": "2301.08174", "url": "http://arxiv.org/abs/2301.08174v1", "pdf_url": "http://arxiv.org/pdf/2301.08174v1", "title": "Collaborative Robotic Ultrasound Tissue Scanning for Surgical Resection   Guidance in Neurosurgery", "abs": "The aim of this paper is to introduce a robotic platform for autonomous iUS tissue scanning to optimise intraoperative diagnosis and improve surgical resection during robot-assisted operations. To guide anatomy specific robotic scanning and generate a representation of the robot task space, fast and accurate techniques for the recovery of 3D morphological structures of the surgical cavity are developed. The prototypic DLR MIRO surgical robotic arm is used to control the applied force and the in-plane motion of the US transducer. A key application of the proposed platform is the scanning of brain tissue to guide tumour resection.", "authors": "Alistair Weld, Michael Dyck, Julian Klodmann, Giulio Anichini, Luke Dixon, Sophie Camp, Alin Albu-Sch\u00e4ffer, Stamatia Giannarou", "first_author": "Stamatia Giannarou", "first_end_author": "Alistair Weld; Stamatia Giannarou", "publish_time": "2023-01-19 17:05:07+00:00", "update_time": "2023-01-19 17:05:07+00:00"}, {"id": "2209.12684", "url": "http://arxiv.org/abs/2209.12684v2", "pdf_url": "http://arxiv.org/pdf/2209.12684v2", "title": "Soft-labeling Strategies for Rapid Sub-Typing", "abs": "The challenge of labeling large example datasets for computer vision continues to limit the availability and scope of image repositories. This research provides a new method for automated data collection, curation, labeling, and iterative training with minimal human intervention for the case of overhead satellite imagery and object detection. The new operational scale effectively scanned an entire city (68 square miles) in grid search and yielded a prediction of car color from space observations. A partially trained yolov5 model served as an initial inference seed to output further, more refined model predictions in iterative cycles. Soft labeling here refers to accepting label noise as a potentially valuable augmentation to reduce overfitting and enhance generalized predictions to previously unseen test data. The approach takes advantage of a real-world instance where a cropped image of a car can automatically receive sub-type information as white or colorful from pixel values alone, thus completing an end-to-end pipeline without overdependence on human labor.", "authors": "Grant Rosario, David Noever, Matt Ciolino", "first_author": "Matt Ciolino", "first_end_author": "Grant Rosario; Matt Ciolino", "publish_time": "2022-09-23 03:04:27+00:00", "update_time": "2023-01-19 16:41:19+00:00"}, {"id": "2301.08160", "url": "http://arxiv.org/abs/2301.08160v1", "pdf_url": "http://arxiv.org/pdf/2301.08160v1", "title": "FECANet: Boosting Few-Shot Semantic Segmentation with Feature-Enhanced   Context-Aware Network", "abs": "Few-shot semantic segmentation is the task of learning to locate each pixel of the novel class in the query image with only a few annotated support images. The current correlation-based methods construct pair-wise feature correlations to establish the many-to-many matching because the typical prototype-based approaches cannot learn fine-grained correspondence relations. However, the existing methods still suffer from the noise contained in naive correlations and the lack of context semantic information in correlations. To alleviate these problems mentioned above, we propose a Feature-Enhanced Context-Aware Network (FECANet). Specifically, a feature enhancement module is proposed to suppress the matching noise caused by inter-class local similarity and enhance the intra-class relevance in the naive correlation. In addition, we propose a novel correlation reconstruction module that encodes extra correspondence relations between foreground and background and multi-scale context semantic features, significantly boosting the encoder to capture a reliable matching pattern. Experiments on PASCAL-$5^i$ and COCO-$20^i$ datasets demonstrate that our proposed FECANet leads to remarkable improvement compared to previous state-of-the-arts, demonstrating its effectiveness.", "authors": "Huafeng Liu, Pai Peng, Tao Chen, Qiong Wang, Yazhou Yao, Xian-Sheng Hua", "first_author": "Xian-Sheng Hua", "first_end_author": "Huafeng Liu; Xian-Sheng Hua", "publish_time": "2023-01-19 16:31:13+00:00", "update_time": "2023-01-19 16:31:13+00:00"}, {"id": "2301.08157", "url": "http://arxiv.org/abs/2301.08157v1", "pdf_url": "http://arxiv.org/pdf/2301.08157v1", "title": "SoftEnNet: Symbiotic Monocular Depth Estimation and Lumen Segmentation   for Colonoscopy Endorobots", "abs": "Colorectal cancer is the third most common cause of cancer death worldwide. Optical colonoscopy is the gold standard for detecting colorectal cancer; however, about 25 percent of polyps are missed during the procedure. A vision-based autonomous endorobot can improve colonoscopy procedures significantly through systematic, complete screening of the colonic mucosa. The reliable robot navigation needed requires a three-dimensional understanding of the environment and lumen tracking to support autonomous tasks. We propose a novel multi-task model that simultaneously predicts dense depth and lumen segmentation with an ensemble of deep networks. The depth estimation sub-network is trained in a self-supervised fashion guided by view synthesis; the lumen segmentation sub-network is supervised. The two sub-networks are interconnected with pathways that enable information exchange and thereby mutual learning. As the lumen is in the image's deepest visual space, lumen segmentation helps with the depth estimation at the farthest location. In turn, the estimated depth guides the lumen segmentation network as the lumen location defines the farthest scene location. Unlike other environments, view synthesis often fails in the colon because of the deformable wall, textureless surface, specularities, and wide field of view image distortions, all challenges that our pipeline addresses. We conducted qualitative analysis on a synthetic dataset and quantitative analysis on a colon training model and real colonoscopy videos. The experiments show that our model predicts accurate scale-invariant depth maps and lumen segmentation from colonoscopy images in near real-time.", "authors": "Alwyn Mathew, Ludovic Magerand, Emanuele Trucco, Luigi Manfredi", "first_author": "Luigi Manfredi", "first_end_author": "Alwyn Mathew; Luigi Manfredi", "publish_time": "2023-01-19 16:22:17+00:00", "update_time": "2023-01-19 16:22:17+00:00"}, {"id": "2301.08153", "url": "http://arxiv.org/abs/2301.08153v1", "pdf_url": "http://arxiv.org/pdf/2301.08153v1", "title": "SwiftAvatar: Efficient Auto-Creation of Parameterized Stylized Character   on Arbitrary Avatar Engines", "abs": "The creation of a parameterized stylized character involves careful selection of numerous parameters, also known as the \"avatar vectors\" that can be interpreted by the avatar engine. Existing unsupervised avatar vector estimation methods that auto-create avatars for users, however, often fail to work because of the domain gap between realistic faces and stylized avatar images. To this end, we propose SwiftAvatar, a novel avatar auto-creation framework that is evidently superior to previous works. SwiftAvatar introduces dual-domain generators to create pairs of realistic faces and avatar images using shared latent codes. The latent codes can then be bridged with the avatar vectors as pairs, by performing GAN inversion on the avatar images rendered from the engine using avatar vectors. Through this way, we are able to synthesize paired data in high-quality as many as possible, consisting of avatar vectors and their corresponding realistic faces. We also propose semantic augmentation to improve the diversity of synthesis. Finally, a light-weight avatar vector estimator is trained on the synthetic pairs to implement efficient auto-creation. Our experiments demonstrate the effectiveness and efficiency of SwiftAvatar on two different avatar engines. The superiority and advantageous flexibility of SwiftAvatar are also verified in both subjective and objective evaluations.", "authors": "Shizun Wang, Weihong Zeng, Xu Wang, Hao Yang, Li Chen, Chuang Zhang, Ming Wu, Yi Yuan, Yunzhao Zeng, Min Zheng", "first_author": "Min Zheng", "first_end_author": "Shizun Wang; Min Zheng", "publish_time": "2023-01-19 16:14:28+00:00", "update_time": "2023-01-19 16:14:28+00:00"}, {"id": "2205.11606", "url": "http://arxiv.org/abs/2205.11606v3", "pdf_url": "http://arxiv.org/pdf/2205.11606v3", "title": "Discriminative Feature Learning through Feature Distance Loss", "abs": "Ensembles of Convolutional neural networks have shown remarkable results in learning discriminative semantic features for image classification tasks. Though, the models in the ensemble often concentrate on similar regions in images. This work proposes a novel method that forces a set of base models to learn different features for a classification task. These models are combined in an ensemble to make a collective classification. The key finding is that by forcing the models to concentrate on different features, the classification accuracy is increased. To learn different feature concepts, a so-called feature distance loss is implemented on the feature maps. The experiments on benchmark convolutional neural networks (VGG16, ResNet, AlexNet), popular datasets (Cifar10, Cifar100, miniImageNet, NEU, BSD, TEX), and different training samples (3, 5, 10, 20, 50, 100 per class) show the effectiveness of the proposed feature loss. The proposed method outperforms classical ensemble versions of the base models. The Class Activation Maps explicitly prove the ability to learn different feature concepts. The code is available at: https://github.com/2Obe/Feature-Distance-Loss.git", "authors": "Tobias Schlagenhauf, Yiwen Lin, Benjamin Noack", "first_author": "Benjamin Noack", "first_end_author": "Tobias Schlagenhauf; Benjamin Noack", "publish_time": "2022-05-23 20:01:32+00:00", "update_time": "2023-01-19 16:08:46+00:00"}, {"id": "2301.08147", "url": "http://arxiv.org/abs/2301.08147v1", "pdf_url": "http://arxiv.org/pdf/2301.08147v1", "title": "RGB-D-Based Categorical Object Pose and Shape Estimation: Methods,   Datasets, and Evaluation", "abs": "Recently, various methods for 6D pose and shape estimation of objects at a per-category level have been proposed. This work provides an overview of the field in terms of methods, datasets, and evaluation protocols. First, an overview of existing works and their commonalities and differences is provided. Second, we take a critical look at the predominant evaluation protocol, including metrics and datasets. Based on the findings, we propose a new set of metrics, contribute new annotations for the Redwood dataset, and evaluate state-of-the-art methods in a fair comparison. The results indicate that existing methods do not generalize well to unconstrained orientations and are actually heavily biased towards objects being upright. We provide an easy-to-use evaluation toolbox with well-defined metrics, methods, and dataset interfaces, which allows evaluation and comparison with various state-of-the-art approaches (https://github.com/roym899/pose_and_shape_evaluation).", "authors": "Leonard Bruns, Patric Jensfelt", "first_author": "Patric Jensfelt", "first_end_author": "Leonard Bruns; Patric Jensfelt", "publish_time": "2023-01-19 15:59:10+00:00", "update_time": "2023-01-19 15:59:10+00:00"}, {"id": "2301.08140", "url": "http://arxiv.org/abs/2301.08140v1", "pdf_url": "http://arxiv.org/pdf/2301.08140v1", "title": "Regularizing disparity estimation via multi task learning with   structured light reconstruction", "abs": "3D reconstruction is a useful tool for surgical planning and guidance. However, the lack of available medical data stunts research and development in this field, as supervised deep learning methods for accurate disparity estimation rely heavily on large datasets containing ground truth information. Alternative approaches to supervision have been explored, such as self-supervision, which can reduce or remove entirely the need for ground truth. However, no proposed alternatives have demonstrated performance capabilities close to what would be expected from a supervised setup. This work aims to alleviate this issue. In this paper, we investigate the learning of structured light projections to enhance the development of direct disparity estimation networks. We show for the first time that it is possible to accurately learn the projection of structured light on a scene, implicitly learning disparity. Secondly, we \\textcolor{black}{explore the use of a multi task learning (MTL) framework for the joint training of structured light and disparity. We present results which show that MTL with structured light improves disparity training; without increasing the number of model parameters. Our MTL setup outperformed the single task learning (STL) network in every validation test. Notably, in the medical generalisation test, the STL error was 1.4 times worse than that of the best MTL performance. The benefit of using MTL is emphasised when the training data is limited.} A dataset containing stereoscopic images, disparity maps and structured light projections on medical phantoms and ex vivo tissue was created for evaluation together with virtual scenes. This dataset will be made publicly available in the future.", "authors": "Alistair Weld, Joao Cartucho, Chi Xu, Joseph Davids, Stamatia Giannarou", "first_author": "Stamatia Giannarou", "first_end_author": "Alistair Weld; Stamatia Giannarou", "publish_time": "2023-01-19 15:54:52+00:00", "update_time": "2023-01-19 15:54:52+00:00"}, {"id": "2301.06496", "url": "http://arxiv.org/abs/2301.06496v2", "pdf_url": "http://arxiv.org/pdf/2301.06496v2", "title": "High-bandwidth Close-Range Information Transport through Light Pipes", "abs": "Image retrieval after propagation through multi-mode fibers is gaining attention due to their capacity to confine light and efficiently transport it over distances in a compact system. Here, we propose a generally applicable information-theoretic framework to transmit maximal-entropy (data) images and maximize the information transmission over sub-meter distances, a crucial capability that allows optical storage applications to scale and address different parts of storage media. To this end, we use millimeter-sized square optical waveguides to image a megapixel 8-bit spatial-light modulator. Data is thus represented as a 2D array of 8-bit values (symbols). Transmitting 100000s of symbols requires innovation beyond transmission matrix approaches. Deep neural networks have been recently utilized to retrieve images, but have been limited to small (thousands of symbols) and natural looking (low entropy) images. We maximize information transmission by combining a bandwidth-optimized homodyne detector with a differentiable hybrid neural-network consisting of a digital twin of the experiment setup and a U-Net. For the digital twin, we implement and compare a differentiable mode-based twin with a differentiable ray-based twin. Importantly, the latter can adapt to manufacturing-related setup imperfections during training which we show to be crucial. Our pipeline is trained end-to-end to recover digital input images while maximizing the achievable information page size based on a differentiable mutual-information estimator. We demonstrate retrieval of 66 kB at maximum with 1.7 bit per symbol on average with a range of 0.3 - 3.4 bit.", "authors": "Joowon Lim, Jannes Gladrow, Douglas Kelly, Greg O'Shea, Govert Verkes, Ioan Stefanovici, Sebastian Nowozin, Benn Thomsen", "first_author": "Benn Thomsen", "first_end_author": "Joowon Lim; Benn Thomsen", "publish_time": "2023-01-16 16:10:52+00:00", "update_time": "2023-01-19 15:44:27+00:00"}, {"id": "2301.08125", "url": "http://arxiv.org/abs/2301.08125v1", "pdf_url": "http://arxiv.org/pdf/2301.08125v1", "title": "Diagnose Like a Pathologist: Transformer-Enabled Hierarchical   Attention-Guided Multiple Instance Learning for Whole Slide Image   Classification", "abs": "Multiple Instance Learning (MIL) and transformers are increasingly popular in histopathology Whole Slide Image (WSI) classification. However, unlike human pathologists who selectively observe specific regions of histopathology tissues under different magnifications, most methods do not incorporate multiple resolutions of the WSIs, hierarchically and attentively, thereby leading to a loss of focus on the WSIs and information from other resolutions. To resolve this issue, we propose the Hierarchical Attention-Guided Multiple Instance Learning framework to fully exploit the WSIs, which can dynamically and attentively discover the discriminative regions across multiple resolutions of the WSIs. Within this framework, to further enhance the performance of the transformer and obtain a more holistic WSI (bag) representation, we propose an Integrated Attention Transformer, consisting of multiple Integrated Attention Modules, which is the combination of a transformer layer and an aggregation module that produces a bag representation based on every instance representation in that bag. The results of the experiments show that our method achieved state-of-the-art performances on multiple datasets, including Camelyon16, TCGA-RCC, TCGA-NSCLC, and our in-house IMGC dataset.", "authors": "Conghao Xiong, Hao Chen, Joseph Sung, Irwin King", "first_author": "Irwin King", "first_end_author": "Conghao Xiong; Irwin King", "publish_time": "2023-01-19 15:38:43+00:00", "update_time": "2023-01-19 15:38:43+00:00"}, {"id": "2301.06866", "url": "http://arxiv.org/abs/2301.06866v2", "pdf_url": "http://arxiv.org/pdf/2301.06866v2", "title": "Building Scalable Video Understanding Benchmarks through Sports", "abs": "Existing benchmarks for evaluating long video understanding falls short on multiple aspects, either lacking in scale or quality of annotations. These limitations arise from the difficulty in collecting dense annotations for long videos (e.g. actions, dialogues, etc.), which are often obtained by manually labeling many frames per second. In this work, we introduce an automated Annotation and Video Stream Alignment Pipeline (abbreviated ASAP). We demonstrate the generality of ASAP by aligning unlabeled videos of four different sports (Cricket, Football, Basketball, and American Football) with their corresponding dense annotations (i.e. commentary) freely available on the web. Our human studies indicate that ASAP can align videos and annotations with high fidelity, precision, and speed. We then leverage ASAP scalability to create LCric, a large-scale long video understanding benchmark, with over 1000 hours of densely annotated long Cricket videos (with an average sample length of 50 mins) collected at virtually zero annotation cost. We benchmark and analyze state-of-the-art video understanding models on LCric through a large set of compositional multi-choice and regression queries. We establish a human baseline that indicates significant room for new research to explore. The dataset along with the code for ASAP and baselines can be accessed here: https://asap-benchmark.github.io/.", "authors": "Aniket Agarwal, Alex Zhang, Karthik Narasimhan, Igor Gilitschenski, Vishvak Murahari, Yash Kant", "first_author": "Yash Kant", "first_end_author": "Aniket Agarwal; Yash Kant", "publish_time": "2023-01-17 13:20:21+00:00", "update_time": "2023-01-19 15:33:02+00:00"}, {"id": "2111.00770", "url": "http://arxiv.org/abs/2111.00770v3", "pdf_url": "http://arxiv.org/pdf/2111.00770v3", "title": "Dense Prediction with Attentive Feature Aggregation", "abs": "Aggregating information from features across different layers is an essential operation for dense prediction models. Despite its limited expressiveness, feature concatenation dominates the choice of aggregation operations. In this paper, we introduce Attentive Feature Aggregation (AFA) to fuse different network layers with more expressive non-linear operations. AFA exploits both spatial and channel attention to compute weighted average of the layer activations. Inspired by neural volume rendering, we extend AFA with Scale-Space Rendering (SSR) to perform late fusion of multi-scale predictions. AFA is applicable to a wide range of existing network designs. Our experiments show consistent and significant improvements on challenging semantic segmentation benchmarks, including Cityscapes, BDD100K, and Mapillary Vistas, at negligible computational and parameter overhead. In particular, AFA improves the performance of the Deep Layer Aggregation (DLA) model by nearly 6% mIoU on Cityscapes. Our experimental analyses show that AFA learns to progressively refine segmentation maps and to improve boundary details, leading to new state-of-the-art results on boundary detection benchmarks on BSDS500 and NYUDv2. Code and video resources are available at http://vis.xyz/pub/dla-afa.", "authors": "Yung-Hsu Yang, Thomas E. Huang, Min Sun, Samuel Rota Bul\u00f2, Peter Kontschieder, Fisher Yu", "first_author": "Fisher Yu", "first_end_author": "Yung-Hsu Yang; Fisher Yu", "publish_time": "2021-11-01 08:44:45+00:00", "update_time": "2023-01-19 15:25:52+00:00"}, {"id": "2301.08113", "url": "http://arxiv.org/abs/2301.08113v1", "pdf_url": "http://arxiv.org/pdf/2301.08113v1", "title": "Soft Thresholding for Visual Image Enhancement", "abs": "Thresholding converts a greyscale image into a binary image, and is thus often a necessary segmentation step in image processing. For a human viewer however, thresholding usually has a negative impact on the legibility of document images. This report describes a simple method for \"smearing out\" the threshold and transforming the greyscale image into a different greyscale image. The method is similar to fuzzy thresholding, but is discussed here in the simpler context of greyscale transformations and, unlike fuzzy thresholding, it is independent from the method for finding the threshold. A simple formula is presented for automatically determining the width of the threshold spread. The method can be used, e.g., for enhancing images for the presentation in online facsimile repositories.", "authors": "Christoph Dalitz", "first_author": "Christoph Dalitz", "first_end_author": "Christoph Dalitz; Christoph Dalitz", "publish_time": "2023-01-19 15:05:13+00:00", "update_time": "2023-01-19 15:05:13+00:00"}, {"id": "2301.08092", "url": "http://arxiv.org/abs/2301.08092v1", "pdf_url": "http://arxiv.org/pdf/2301.08092v1", "title": "RNAS-CL: Robust Neural Architecture Search by Cross-Layer Knowledge   Distillation", "abs": "Deep Neural Networks are vulnerable to adversarial attacks. Neural Architecture Search (NAS), one of the driving tools of deep neural networks, demonstrates superior performance in prediction accuracy in various machine learning applications. However, it is unclear how it performs against adversarial attacks. Given the presence of a robust teacher, it would be interesting to investigate if NAS would produce robust neural architecture by inheriting robustness from the teacher. In this paper, we propose Robust Neural Architecture Search by Cross-Layer Knowledge Distillation (RNAS-CL), a novel NAS algorithm that improves the robustness of NAS by learning from a robust teacher through cross-layer knowledge distillation. Unlike previous knowledge distillation methods that encourage close student/teacher output only in the last layer, RNAS-CL automatically searches for the best teacher layer to supervise each student layer. Experimental result evidences the effectiveness of RNAS-CL and shows that RNAS-CL produces small and robust neural architecture.", "authors": "Utkarsh Nath, Yancheng Wang, Yingzhen Yang", "first_author": "Yingzhen Yang", "first_end_author": "Utkarsh Nath; Yingzhen Yang", "publish_time": "2023-01-19 14:22:44+00:00", "update_time": "2023-01-19 14:22:44+00:00"}, {"id": "2210.11817", "url": "http://arxiv.org/abs/2210.11817v2", "pdf_url": "http://arxiv.org/pdf/2210.11817v2", "title": "Motion Matters: A Novel Motion Modeling For Cross-View Gait Feature   Learning", "abs": "As a unique biometric that can be perceived at a distance, gait has broad applications in person authentication, social security, and so on. Existing gait recognition methods suffer from changes in viewpoint and clothing and barely consider extracting diverse motion features, a fundamental characteristic in gaits, from gait sequences. This paper proposes a novel motion modeling method to extract the discriminative and robust representation. Specifically, we first extract the motion features from the encoded motion sequences in the shallow layer. Then we continuously enhance the motion feature in deep layers. This motion modeling approach is independent of mainstream work in building network architectures. As a result, one can apply this motion modeling method to any backbone to improve gait recognition performance. In this paper, we combine motion modeling with one commonly used backbone~(GaitGL) as GaitGL-M to illustrate motion modeling. Extensive experimental results on two commonly-used cross-view gait datasets demonstrate the superior performance of GaitGL-M over existing state-of-the-art methods.", "authors": "Jingqi Li, Jiaqi Gao, Yuzhen Zhang, Hongming Shan, Junping Zhang", "first_author": "Junping Zhang", "first_end_author": "Jingqi Li; Junping Zhang", "publish_time": "2022-10-21 08:42:00+00:00", "update_time": "2023-01-19 14:11:52+00:00"}, {"id": "2301.08072", "url": "http://arxiv.org/abs/2301.08072v1", "pdf_url": "http://arxiv.org/pdf/2301.08072v1", "title": "Dif-Fusion: Towards High Color Fidelity in Infrared and Visible Image   Fusion with Diffusion Models", "abs": "Color plays an important role in human visual perception, reflecting the spectrum of objects. However, the existing infrared and visible image fusion methods rarely explore how to handle multi-spectral/channel data directly and achieve high color fidelity. This paper addresses the above issue by proposing a novel method with diffusion models, termed as Dif-Fusion, to generate the distribution of the multi-channel input data, which increases the ability of multi-source information aggregation and the fidelity of colors. In specific, instead of converting multi-channel images into single-channel data in existing fusion methods, we create the multi-channel data distribution with a denoising network in a latent space with forward and reverse diffusion process. Then, we use the the denoising network to extract the multi-channel diffusion features with both visible and infrared information. Finally, we feed the multi-channel diffusion features to the multi-channel fusion module to directly generate the three-channel fused image. To retain the texture and intensity information, we propose multi-channel gradient loss and intensity loss. Along with the current evaluation metrics for measuring texture and intensity fidelity, we introduce a new evaluation metric to quantify color fidelity. Extensive experiments indicate that our method is more effective than other state-of-the-art image fusion methods, especially in color fidelity.", "authors": "Jun Yue, Leyuan Fang, Shaobo Xia, Yue Deng, Jiayi Ma", "first_author": "Jiayi Ma", "first_end_author": "Jun Yue; Jiayi Ma", "publish_time": "2023-01-19 13:37:19+00:00", "update_time": "2023-01-19 13:37:19+00:00"}, {"id": "2301.08067", "url": "http://arxiv.org/abs/2301.08067v1", "pdf_url": "http://arxiv.org/pdf/2301.08067v1", "title": "Interpreting CNN Predictions using Conditional Generative Adversarial   Networks", "abs": "We propose a novel method that trains a conditional Generative Adversarial Network (GAN) to generate visual interpretations of a Convolutional Neural Network (CNN). To comprehend a CNN, the GAN is trained with information on how the CNN processes an image when making predictions. Supplying that information has two main challenges: how to represent this information in a form that is feedable to the GANs and how to effectively feed the representation to the GAN. To address these issues, we developed a suitable representation of CNN architectures by cumulatively averaging intermediate interpretation maps. We also propose two alternative approaches to feed the representations to the GAN and to choose an effective training strategy. Our approach learned the general aspects of CNNs and was agnostic to datasets and CNN architectures. The study includes both qualitative and quantitative evaluations and compares the proposed GANs with state-of-the-art approaches. We found that the initial layers of CNNs and final layers are equally crucial for interpreting CNNs upon interpreting the proposed GAN. We believe training a GAN to interpret CNNs would open doors for improved interpretations by leveraging fast-paced deep learning advancements. The code used for experimentation is publicly available at https://github.com/Akash-guna/Explain-CNN-With-GANS", "authors": "Akash Guna R T, Raul Benitez, Sikha O K", "first_author": "Sikha O K", "first_end_author": "Akash Guna R T; Sikha O K", "publish_time": "2023-01-19 13:26:12+00:00", "update_time": "2023-01-19 13:26:12+00:00"}, {"id": "2301.08064", "url": "http://arxiv.org/abs/2301.08064v1", "pdf_url": "http://arxiv.org/pdf/2301.08064v1", "title": "Position Regression for Unsupervised Anomaly Detection", "abs": "In recent years, anomaly detection has become an essential field in medical image analysis. Most current anomaly detection methods for medical images are based on image reconstruction. In this work, we propose a novel anomaly detection approach based on coordinate regression. Our method estimates the position of patches within a volume, and is trained only on data of healthy subjects. During inference, we can detect and localize anomalies by considering the error of the position estimate of a given patch. We apply our method to 3D CT volumes and evaluate it on patients with intracranial haemorrhages and cranial fractures. The results show that our method performs well in detecting these anomalies. Furthermore, we show that our method requires less memory than comparable approaches that involve image reconstruction. This is highly relevant for processing large 3D volumes, for instance, CT or MRI scans.", "authors": "Florentin Bieder, Julia Wolleb, Robin Sandk\u00fchler, Philippe C. Cattin", "first_author": "Philippe C. Cattin", "first_end_author": "Florentin Bieder; Philippe C. Cattin", "publish_time": "2023-01-19 13:22:11+00:00", "update_time": "2023-01-19 13:22:11+00:00"}, {"id": "2301.08044", "url": "http://arxiv.org/abs/2301.08044v1", "pdf_url": "http://arxiv.org/pdf/2301.08044v1", "title": "Reference Guided Image Inpainting using Facial Attributes", "abs": "Image inpainting is a technique of completing missing pixels such as occluded region restoration, distracting objects removal, and facial completion. Among these inpainting tasks, facial completion algorithm performs face inpainting according to the user direction. Existing approaches require delicate and well controlled input by the user, thus it is difficult for an average user to provide the guidance sufficiently accurate for the algorithm to generate desired results. To overcome this limitation, we propose an alternative user-guided inpainting architecture that manipulates facial attributes using a single reference image as the guide. Our end-to-end model consists of attribute extractors for accurate reference image attribute transfer and an inpainting model to map the attributes realistically and accurately to generated images. We customize MS-SSIM loss and learnable bidirectional attention maps in which importance structures remain intact even with irregular shaped masks. Based on our evaluation using the publicly available dataset CelebA-HQ, we demonstrate that the proposed method delivers superior performance compared to some state-of-the-art methods specialized in inpainting tasks.", "authors": "Dongsik Yoon, Jeonggi Kwak, Yuanming Li, David Han, Youngsaeng Jin, Hanseok Ko", "first_author": "Hanseok Ko", "first_end_author": "Dongsik Yoon; Hanseok Ko", "publish_time": "2023-01-19 12:39:08+00:00", "update_time": "2023-01-19 12:39:08+00:00"}, {"id": "2108.06230", "url": "http://arxiv.org/abs/2108.06230v5", "pdf_url": "http://arxiv.org/pdf/2108.06230v5", "title": "Generative Zero-Shot Learning for Semantic Segmentation of 3D Point   Clouds", "abs": "While there has been a number of studies on Zero-Shot Learning (ZSL) for 2D images, its application to 3D data is still recent and scarce, with just a few methods limited to classification. We present the first generative approach for both ZSL and Generalized ZSL (GZSL) on 3D data, that can handle both classification and, for the first time, semantic segmentation. We show that it reaches or outperforms the state of the art on ModelNet40 classification for both inductive ZSL and inductive GZSL. For semantic segmentation, we created three benchmarks for evaluating this new ZSL task, using S3DIS, ScanNet and SemanticKITTI. Our experiments show that our method outperforms strong baselines, which we additionally propose for this task.", "authors": "Bj\u00f6rn Michele, Alexandre Boulch, Gilles Puy, Maxime Bucher, Renaud Marlet", "first_author": "Renaud Marlet", "first_end_author": "Bj\u00f6rn Michele; Renaud Marlet", "publish_time": "2021-08-13 13:29:27+00:00", "update_time": "2023-01-19 11:58:47+00:00"}, {"id": "2301.08252", "url": "http://arxiv.org/abs/2301.08252v1", "pdf_url": "http://arxiv.org/pdf/2301.08252v1", "title": "Evaluation of the potential of Near Infrared Hyperspectral Imaging for   monitoring the invasive brown marmorated stink bug", "abs": "The brown marmorated stink bug (BMSB), Halyomorpha halys, is an invasive insect pest of global importance that damages several crops, compromising agri-food production. Field monitoring procedures are fundamental to perform risk assessment operations, in order to promptly face crop infestations and avoid economical losses. To improve pest management, spectral cameras mounted on Unmanned Aerial Vehicles (UAVs) and other Internet of Things (IoT) devices, such as smart traps or unmanned ground vehicles, could be used as an innovative technology allowing fast, efficient and real-time monitoring of insect infestations. The present study consists in a preliminary evaluation at the laboratory level of Near Infrared Hyperspectral Imaging (NIR-HSI) as a possible technology to detect BMSB specimens on different vegetal backgrounds, overcoming the problem of BMSB mimicry. Hyperspectral images of BMSB were acquired in the 980-1660 nm range, considering different vegetal backgrounds selected to mimic a real field application scene. Classification models were obtained following two different chemometric approaches. The first approach was focused on modelling spectral information and selecting relevant spectral regions for discrimination by means of sparse-based variable selection coupled with Soft Partial Least Squares Discriminant Analysis (s-Soft PLS-DA) classification algorithm. The second approach was based on modelling spatial and spectral features contained in the hyperspectral images using Convolutional Neural Networks (CNN). Finally, to further improve BMSB detection ability, the two strategies were merged, considering only the spectral regions selected by s-Soft PLS-DA for CNN modelling.", "authors": "Veronica Ferrari, Rosalba Calvini, Bas Boom, Camilla Menozzi, Aravind Krishnaswamy Rangarajan, Lara Maistrello, Peter Offermans, Alessandro Ulrici", "first_author": "Alessandro Ulrici", "first_end_author": "Veronica Ferrari; Alessandro Ulrici", "publish_time": "2023-01-19 11:37:20+00:00", "update_time": "2023-01-19 11:37:20+00:00"}, {"id": "2301.08555", "url": "http://arxiv.org/abs/2301.08555v1", "pdf_url": "http://arxiv.org/pdf/2301.08555v1", "title": "Hybrid Open-set Segmentation with Synthetic Negative Data", "abs": "Open-set segmentation is often conceived by complementing closed-set classification with anomaly detection. Existing dense anomaly detectors operate either through generative modelling of regular training data or by discriminating with respect to negative training data. These two approaches optimize different objectives and therefore exhibit different failure modes. Consequently, we propose the first dense hybrid anomaly score that fuses generative and discriminative cues. The proposed score can be efficiently implemented by upgrading any semantic segmentation model with translation-equivariant estimates of data likelihood and dataset posterior. Our design is a remarkably good fit for efficient inference on large images due to negligible computational overhead over the closed-set baseline. The resulting dense hybrid open-set models require negative training images that can be sampled either from an auxiliary negative dataset or from a jointly trained generative model. We evaluate our contributions on benchmarks for dense anomaly detection and open-set segmentation of traffic scenes. The experiments reveal strong open-set performance in spite of negligible computational overhead.", "authors": "Matej Grci\u0107, Sini\u0161a \u0160egvi\u0107", "first_author": "Sini\u0161a \u0160egvi\u0107", "first_end_author": "Matej Grci\u0107; Sini\u0161a \u0160egvi\u0107", "publish_time": "2023-01-19 11:02:44+00:00", "update_time": "2023-01-19 11:02:44+00:00"}, {"id": "2301.00986", "url": "http://arxiv.org/abs/2301.00986v2", "pdf_url": "http://arxiv.org/pdf/2301.00986v2", "title": "Look, Listen, and Attack: Backdoor Attacks Against Video Action   Recognition", "abs": "Deep neural networks (DNNs) are vulnerable to a class of attacks called \"backdoor attacks\", which create an association between a backdoor trigger and a target label the attacker is interested in exploiting. A backdoored DNN performs well on clean test images, yet persistently predicts an attacker-defined label for any sample in the presence of the backdoor trigger. Although backdoor attacks have been extensively studied in the image domain, there are very few works that explore such attacks in the video domain, and they tend to conclude that image backdoor attacks are less effective in the video domain. In this work, we revisit the traditional backdoor threat model and incorporate additional video-related aspects to that model. We show that poisoned-label image backdoor attacks could be extended temporally in two ways, statically and dynamically, leading to highly effective attacks in the video domain. In addition, we explore natural video backdoors to highlight the seriousness of this vulnerability in the video domain. And, for the first time, we study multi-modal (audiovisual) backdoor attacks against video action recognition models, where we show that attacking a single modality is enough for achieving a high attack success rate.", "authors": "Hasan Abed Al Kader Hammoud, Shuming Liu, Mohammed Alkhrashi, Fahad AlBalawi, Bernard Ghanem", "first_author": "Bernard Ghanem", "first_end_author": "Hasan Abed Al Kader Hammoud; Bernard Ghanem", "publish_time": "2023-01-03 07:40:28+00:00", "update_time": "2023-01-19 11:02:33+00:00"}, {"id": "2212.10957", "url": "http://arxiv.org/abs/2212.10957v2", "pdf_url": "http://arxiv.org/pdf/2212.10957v2", "title": "TruFor: Leveraging all-round clues for trustworthy image forgery   detection and localization", "abs": "In this paper we present TruFor, a forensic framework that can be applied to a large variety of image manipulation methods, from classic cheapfakes to more recent manipulations based on deep learning. We rely on the extraction of both high-level and low-level traces through a transformer-based fusion architecture that combines the RGB image and a learned noise-sensitive fingerprint. The latter learns to embed the artifacts related to the camera internal and external processing by training only on real data in a self-supervised manner. Forgeries are detected as deviations from the expected regular pattern that characterizes each pristine image. Looking for anomalies makes the approach able to robustly detect a variety of local manipulations, ensuring generalization. In addition to a pixel-level localization map and a whole-image integrity score, our approach outputs a reliability map that highlights areas where localization predictions may be error-prone. This is particularly important in forensic applications in order to reduce false alarms and allow for a large scale analysis. Extensive experiments on several datasets show that our method is able to reliably detect and localize both cheapfakes and deepfakes manipulations outperforming state-of-the-art works. Code will be publicly available at https://grip-unina.github.io/TruFor/", "authors": "Fabrizio Guillaro, Davide Cozzolino, Avneesh Sud, Nicholas Dufour, Luisa Verdoliva", "first_author": "Luisa Verdoliva", "first_end_author": "Fabrizio Guillaro; Luisa Verdoliva", "publish_time": "2022-12-21 11:49:43+00:00", "update_time": "2023-01-19 10:26:38+00:00"}, {"id": "2301.07969", "url": "http://arxiv.org/abs/2301.07969v1", "pdf_url": "http://arxiv.org/pdf/2301.07969v1", "title": "Fast Inference in Denoising Diffusion Models via MMD Finetuning", "abs": "Denoising Diffusion Models (DDMs) have become a popular tool for generating high-quality samples from complex data distributions. These models are able to capture sophisticated patterns and structures in the data, and can generate samples that are highly diverse and representative of the underlying distribution. However, one of the main limitations of diffusion models is the complexity of sample generation, since a large number of inference timesteps is required to faithfully capture the data distribution. In this paper, we present MMD-DDM, a novel method for fast sampling of diffusion models. Our approach is based on the idea of using the Maximum Mean Discrepancy (MMD) to finetune the learned distribution with a given budget of timesteps. This allows the finetuned model to significantly improve the speed-quality trade-off, by substantially increasing fidelity in inference regimes with few steps or, equivalently, by reducing the required number of steps to reach a target fidelity, thus paving the way for a more practical adoption of diffusion models in a wide range of applications. We evaluate our approach on unconditional image generation with extensive experiments across the CIFAR-10, CelebA, ImageNet and LSUN-Church datasets. Our findings show that the proposed method is able to produce high-quality samples in a fraction of the time required by widely-used diffusion models, and outperforms state-of-the-art techniques for accelerated sampling. Code is available at: https://github.com/diegovalsesia/MMD-DDM.", "authors": "Emanuele Aiello, Diego Valsesia, Enrico Magli", "first_author": "Enrico Magli", "first_end_author": "Emanuele Aiello; Enrico Magli", "publish_time": "2023-01-19 09:48:07+00:00", "update_time": "2023-01-19 09:48:07+00:00"}]}]